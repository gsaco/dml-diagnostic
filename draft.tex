\documentclass{ectj}

% --------------------------------------------------------------------------
% Packages (Econometrics Journal style + needed extras)
% --------------------------------------------------------------------------
% Note: ectj.cls already loads natbib with authoryear option
\usepackage{amsfonts,amssymb,graphics,epsfig,verbatim,bm,latexsym,amsmath,url,amsbsy}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{bbm}
\usepackage{enumitem}
\usepackage[colorlinks=false]{hyperref}

% Fix \citep to produce parenthetical citations (Author, Year)
% ectj.cls redefines \citep to \cite which gives Author (Year)
\renewcommand{\citep}[1]{(\citealp{#1})}

% --------------------------------------------------------------------------
% Theorem Environments (following ECTJ template pattern)
% --------------------------------------------------------------------------
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}

\renewcommand{\thesection}{\arabic{section}}
\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}
\renewcommand{\thetheorem}{\arabic{section}.\arabic{theorem}}
\renewcommand{\theassumption}{\arabic{section}.\arabic{assumption}}
\renewcommand{\theproposition}{\arabic{section}.\arabic{proposition}}
\renewcommand{\thecorollary}{\arabic{section}.\arabic{corollary}}
\renewcommand{\thelemma}{\arabic{section}.\arabic{lemma}}
\renewcommand{\theexample}{\arabic{section}.\arabic{example}}
\renewcommand{\theremark}{\arabic{section}.\arabic{remark}}
\renewcommand{\thedefinition}{\arabic{section}.\arabic{definition}}

% --------------------------------------------------------------------------
% Custom Commands
% --------------------------------------------------------------------------
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\norm}[1]{\lVert #1\rVert}
\newcommand{\abs}[1]{\lvert #1\rvert}

\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\op}{o_P}
\newcommand{\Op}{O_P}

% --------------------------------------------------------------------------
% Econometrics Journal front matter
% --------------------------------------------------------------------------
\year 2025
\received{December 2025}
\accepted{(to be completed by the journal)}
\volume{28}

\setcounter{page}{1}

\title[Finite-sample failures in DML]{Finite-Sample Failures and Condition-Number Diagnostics
in Double Machine Learning}

\author[Saco]{Gabriel Saco$^{\dagger}$}
\address{$^{\dagger}$(Affiliation to be added)}
\email{gsacoalvarado@gmail.com}

\begin{document}

\maketitle

\begin{abstract}
Double Machine Learning delivers root-$n$ inference by pairing flexible nuisance estimation with Neyman-orthogonal scores, but finite-sample reliability depends on the conditioning of the score---a property standard diagnostics ignore.
We introduce the DML condition number $\kappa_{\mathrm{DML}}$, the inverse of the empirical score Jacobian.
A refined linearisation shows that $\kappa_{\mathrm{DML}}$ multiplies both sampling variance and nuisance-induced bias, yielding a parameter-scale rate $O_P(\kappa_{\mathrm{DML}}/\sqrt{n} + \kappa_{\mathrm{DML}} r_n)$ and an explicit coverage bound.
We classify designs into three regimes---well-conditioned, moderately ill-conditioned, and severely ill-conditioned---paralleling weak-IV classifications.
Monte Carlo experiments map overlap and learner complexity into $\kappa_{\mathrm{DML}}$ and thence into coverage.
Re-analysis of LaLonde (1986) shows that $\kappa_{\mathrm{DML}}$ immediately distinguishes robust experimental estimates from fragile observational re-analyses.
We argue that $\kappa_{\mathrm{DML}}$ should accompany every DML estimate, just as first-stage $F$-statistics accompany IV results.

\keywords{Double Machine Learning, Nonasymptotic Inference, Weak Identification, Partially Linear Regression.}

\end{abstract}

% ==========================================================================
\section{Introduction}
\label{sec:intro}
% ==========================================================================

Double Machine Learning \citep{chernozhukov2018dml} has become the default method for estimating treatment effects and structural parameters when researchers have access to rich covariate information. The theoretical appeal is substantial: Neyman-orthogonal scores combined with cross-fitting deliver $\sqrt{n}$-consistent, asymptotically normal estimators even when nuisance functions are estimated nonparametrically, provided those estimators satisfy mild rate conditions. Applied papers now routinely report DML point estimates and Wald confidence intervals as if they were robust, model-free solutions to the high-dimensional confounding problem. Yet this confidence rests on asymptotic guarantees that may not translate to finite samples. Practitioners observe a normal-looking $t$-statistic and a seemingly reasonable confidence interval, but they have no diagnostic that reveals whether the underlying orthogonal score is well-conditioned or dangerously flat. This paper provides such a diagnostic.

The core problem is that DML inference can fail silently. When the orthogonal score's Jacobian is small---equivalently, when residual treatment variation after partialling out covariates is limited---the estimating equation becomes nearly flat. Small perturbations in the score then translate into large perturbations in the parameter estimate. Existing finite-sample theory \citep{chernozhukov2023simple,quintas2022finite,jung2023shortnote} provides Berry--Esseen bounds for the $t$-statistic under regularity conditions that implicitly assume the Jacobian is bounded away from zero. These results characterise well-identified settings but offer no guidance when identification weakens. The gap we fill is fundamental: there is no score-based, one-dimensional diagnostic for DML that plays the role of the first-stage $F$-statistic in instrumental variables regression \citep{staigerstock1997weakiv,stockyogo2005weakiv}.

The parallel to weak instruments is instructive. Before \citet{staigerstock1997weakiv} formalised the weak-IV problem and \citet{stockyogo2005weakiv} provided actionable critical values, practitioners routinely reported two-stage least squares estimates without knowing whether their first stage was too weak to support reliable inference. The $F$-statistic transformed practice: it gave researchers a simple number that summarised identification strength and indicated when standard Wald intervals might fail. In the DML literature, an analogous culture has not developed. Researchers sometimes report propensity score diagnostics---overlap plots, $R^2(D \mid X)$, trimming statistics---but these operate at the level of nuisance estimation, not at the level of the orthogonal score that drives inference.\footnote{The distinction matters because the score depends on both nuisance functions and their interaction with the data. A propensity model with excellent fit can still yield a flat score if residual treatment variance is small.} We lack a scalar summary that directly measures how sensitive the DML estimating equation is to perturbations.

This paper introduces the \emph{DML condition number}, denoted $\kappa_{\mathrm{DML}}$, as precisely this diagnostic. We work in the partially linear regression (PLR) model and treat the DML estimator as a scalar Z-estimator \citep{neweymcfadden1994,vaart1998asymptotic}. The empirical Jacobian of the orthogonal score is $\hat{J}_\theta = -n^{-1}\sum_i \hat{U}_i^2$, where $\hat{U}_i = D_i - \hat{m}(X_i)$ is the cross-fitted treatment residual. The condition number is simply the absolute reciprocal:
\begin{equation}\label{eq:kappa_intro}
\kappa_{\mathrm{DML}} := \frac{1}{|\hat{J}_\theta|} = \frac{n}{\sum_{i=1}^n \hat{U}_i^2}.
\end{equation}
When residual treatment variation is large, $\kappa_{\mathrm{DML}}$ is small and the score is steep---identification is strong. When residual treatment variation is small, $\kappa_{\mathrm{DML}}$ is large and the score is flat---small score perturbations yield large parameter perturbations. This is the exact finite-sample analogue of weak instruments, where a small first-stage coefficient inflates the variance and bias of the IV estimator. Crucially, $\kappa_{\mathrm{DML}}$ is not an ad hoc index: it is the actual inverse slope of the DML estimating equation, grounded in classical Z-estimation theory.

Our theoretical contribution is a parameter-scale analysis that places $\kappa_{\mathrm{DML}}$ at the centre. We derive a refined linearisation:
\begin{equation}\label{eq:decomp_intro}
\hat{\theta} - \theta_0 = \kappa_{\mathrm{DML}}(S_n + B_n) + R_n,
\end{equation}
where $S_n = n^{-1}\sum_i \psi(W_i;\theta_0,\eta_0)$ is the centred oracle score, $B_n$ captures nuisance-induced bias from estimating $\eta_0$, and $R_n$ is a higher-order remainder. Under standard DML regularity conditions \citep{chernozhukov2018dml,chernozhukov2022locally}, $S_n = O_P(n^{-1/2})$ and $B_n = o_P(n^{-1/2})$, yielding the parameter-scale rate
\[
\hat{\theta} - \theta_0 = O_P\!\Bigl(\frac{\kappa_{\mathrm{DML}}}{\sqrt{n}} + \kappa_{\mathrm{DML}} r_n\Bigr),
\]
where $r_n$ is the usual DML nuisance remainder. The condition number multiplies \emph{both} the sampling fluctuation and the nuisance bias. In $t$-statistic scale, $\kappa_{\mathrm{DML}}$ cancels with the standard error, so existing Berry--Esseen bounds remain valid. But in parameter scale, $\kappa_{\mathrm{DML}}$ governs the width of confidence intervals and the magnitude of any residual bias. This decomposition induces a natural classification of conditioning regimes. When $\kappa_{\mathrm{DML}} = O_P(1)$, the design is \emph{well-conditioned}: confidence sets shrink at rate $n^{-1/2}$ and standard DML inference applies. When $\kappa_{\mathrm{DML}} = O_P(n^\beta)$ for $0 < \beta < 1/2$, the design is \emph{moderately ill-conditioned}: sets shrink more slowly and bias is amplified. When $\kappa_{\mathrm{DML}} \asymp c\sqrt{n}$, the design is \emph{severely ill-conditioned}: parameter-scale error is $O_P(1)$ and confidence sets fail to shrink even as $n$ grows. This last regime is the DML analogue of weak IV \citep{staigerstock1997weakiv,stockwright2000weakgmm}.\footnote{The semiparametric efficiency bound for $\theta_0$ in PLR is $\E[U^2\varepsilon^2]/(\E[U^2])^2$, which diverges as $\E[U^2] \to 0$. The condition number $\kappa_{\mathrm{DML}} \approx 1/\E[U^2]$ is thus a sample analogue of this divergence, connecting our finite-sample diagnostic to the population efficiency theory of \citet{hahn1998} and \citet{hiranoimbensridder2003}.}

We validate these theoretical predictions through Monte Carlo experiments and an empirical application. The simulations use a PLR design with three overlap levels controlled via $R^2(D \mid X) \in \{0.75, 0.90, 0.97\}$ and three nuisance learners (OLS, Lasso, random forests). As overlap deteriorates, the distribution of $\kappa_{\mathrm{DML}}$ shifts sharply rightward: median $\kappa_{\mathrm{DML}}$ increases from approximately $0.7$ to $5$, with substantial dispersion at high $R^2$. Aggregating by conditioning regime reveals two distinct failure modes. For unbiased learners like OLS, large $\kappa_{\mathrm{DML}}$ produces wide but honest intervals---variance inflation without coverage distortion. For flexible learners like random forests, large $\kappa_{\mathrm{DML}}$ amplifies residual nuisance error into first-order bias, producing severe undercoverage despite smaller interval widths. These patterns directly implement the theoretical rate $\kappa_{\mathrm{DML}}/\sqrt{n} + \kappa_{\mathrm{DML}} r_n$.

The empirical application re-analyses the canonical \citet{lalonde1986} job-training data. In the experimental sample, where treatment is randomised and nearly independent of covariates, $\kappa_{\mathrm{DML}} \approx 4$ and DML estimates are stable across learners. In observational re-analyses using nonexperimental comparison groups, treatment becomes highly predictable, $\kappa_{\mathrm{DML}}$ exceeds 15, and estimates exhibit the fragility predicted by theory: extreme sensitivity to learner choice, very wide intervals, and sign reversals. The condition number immediately distinguishes robust experimental inference from unreliable observational re-analysis.

We conclude with a practical recommendation. Applied researchers should routinely compute and report $\kappa_{\mathrm{DML}}$ alongside DML point estimates and Wald confidence intervals, in direct analogy to first-stage $F$-statistics in IV \citep{andrewsstocksun2019}. A large condition number does not automatically invalidate results---just as a low $F$-statistic does not prove that IV fails---but it signals fragile conditioning that warrants scrutiny. When $\kappa_{\mathrm{DML}}$ is large, practitioners should interpret confidence intervals cautiously, investigate whether overlap can be improved through trimming or alternative estimands \citep{crumphotzimbensmitnik2009,ma2023doubly}, and compare simple versus flexible learners to diagnose the $\kappa_{\mathrm{DML}} r_n$ channel. Our contribution is explicitly diagnostic: we clarify what goes wrong when the DML score is ill-conditioned, connect DML practice to the weak-identification culture that has become standard in IV, and provide a simple, implementable gauge of conditioning grounded in Z-estimation theory. We do not propose a new robust inference procedure; we propose a new standard for transparency.\footnote{To facilitate adoption, we provide a Python package \texttt{dml\_diagnostic} that implements the condition number diagnostic and the analyses in this paper. The package, together with full simulation and empirical replication code, is available at \url{https://github.com/gsaco/dml-diagnostic}. Documentation is provided in the Online Supplement.}

% ==========================================================================
\section{Related Literature}
\label{sec:lit}
% ==========================================================================

The DML framework of \citet{chernozhukov2018dml} combines Neyman-orthogonal scores with cross-fitting to deliver $\sqrt{n}$-consistent inference for low-dimensional parameters even when nuisance functions are estimated nonparametrically. The key insight is that orthogonal scores are locally insensitive to first-order perturbations in nuisance estimates, so regularisation bias from machine learning does not contaminate the target parameter \citep{chernozhukov2022locally,fostersyrgkanis2023}. This robustness property has enabled a generation of applied work that pairs flexible learners with rigorous inference, and has spurred extensions to conditional average treatment effects \citep{semenovachernozhukov2020}, local projections \citep{chernozhukov2022riesz}, and doubly robust difference-in-differences \citep{santannazhao2020}. Recent contributions in \emph{The Econometrics Journal} have further developed DML methodology: \citet{knaus2022dml} provides programme evaluation under unconfoundedness, while \citet{baiardinaghi2024} revisits canonical studies to assess the value added of machine learning to causal inference.\footnote{\citet{baiardinaghi2024} systematically re-analyse published studies using DML and related methods, providing empirical guidance on when machine learning adds value. Our condition number $\kappa_{\mathrm{DML}}$ offers a theoretical foundation for such comparisons.} Subsequent finite-sample theory \citep{chernozhukov2023simple,quintas2022finite,jung2023shortnote} provides Berry--Esseen bounds for the DML $t$-statistic, but these bounds are stated under regularity conditions that implicitly require the score Jacobian to be bounded away from zero. Our contribution is to make this Jacobian the central diagnostic object: we derive how its reciprocal $\kappa_{\mathrm{DML}}$ governs parameter-scale error, and we define conditioning regimes that classify designs by identification strength.

The semiparametric efficiency theory of \citet{hahn1998} and \citet{hiranoimbensridder2003} establishes that the variance bound for treatment-effect parameters diverges as overlap deteriorates. In the PLR model, this bound scales as $\E[U^2\varepsilon^2]/(\E[U^2])^2$, where $U = D - m_0(X)$ is residual treatment variation; as $R^2(D \mid X) \to 1$, the denominator vanishes and the bound explodes. \citet{xu2020limited} study inference under limited overlap in \emph{The Econometrics Journal}, showing that when propensity scores approach zero or one, the ``effective'' sample size shrinks and estimators converge more slowly---a result that our condition number $\kappa_{\mathrm{DML}}$ captures directly at the score level. The targeted learning literature \citep{kennedy2023semiparametric} emphasises that doubly robust estimators can achieve this bound but become unstable when propensity scores are extreme. From a different angle, \citet{khantamer2010} and \citet{kaji2021weak} formalise weak semiparametric identification, showing that estimators become irregular when identifying information concentrates on thin sets---precisely the setting where treatment is nearly deterministic given covariates. Our condition number $\kappa_{\mathrm{DML}} \approx 1/\E[U^2]$ is a sample analogue of the efficiency bound's inflation factor. By tracking how $\kappa_{\mathrm{DML}}$ varies across designs and learners, we translate a population-level concern into a computable, finite-sample diagnostic that practitioners can report.

The weak-IV literature provides both our methodological template and our conceptual vocabulary. \citet{staigerstock1997weakiv} demonstrated that weak instruments yield two-stage least squares error of $O_P(1)$ and arbitrarily distorted Wald intervals, even as $n \to \infty$. \citet{stockyogo2005weakiv} translated this insight into actionable practice: first-stage $F$-statistics below 10 signal unreliable inference, and practitioners now routinely report these diagnostics. \citet{stockwright2000weakgmm} extended the analysis to GMM, classifying designs by identification strength into strong, semi-strong, and weak regimes. \citet{dufour2003} provided foundational results on identification failure, while \citet{moreira2003clr} developed the conditional likelihood ratio test that remains valid under weak identification. Recent work on weak identification with many instruments \citep{mikusheva2024weak} further develops the theoretical foundations relevant to high-dimensional settings. We draw explicit parallels: the Jacobian $\hat{J}_\theta$ in DML plays the role of the first-stage coefficient in IV; $\kappa_{\mathrm{DML}}$ plays the role of the concentration parameter; our three regimes mirror the strong--semi-strong--weak taxonomy. We do not develop conditioning-robust tests analogous to the Anderson--Rubin or Moreira CLR---that is an important direction for future work---but we demonstrate that $\kappa_{\mathrm{DML}}$ reliably indicates when standard DML intervals become fragile.\footnote{Developing critical values for $\kappa_{\mathrm{DML}}$ analogous to Stock--Yogo critical values for first-stage $F$-statistics is an important direction for future work. Such critical values would depend on the desired worst-case bias or coverage distortion, learner complexity, and sample size.}

Finite-sample studies of doubly robust and ML-based estimators have long documented sensitivity to overlap and nuisance misspecification \citep{kang2007demystifying,zimmert2018causal}. Recent constructive proposals address weak overlap directly: trimming extreme propensity scores \citep{crumphotzimbensmitnik2009}, doubly robust estimators tailored to weak overlap \citep{ma2023doubly}, propensity score calibration \citep{wuthrichzhu2024,ballinari2024calibration}, and sample mixing \citep{jang2024mixing}. \citet{liu2024regularized} study regularised DML in partially linear models with potential unobserved confounding, providing finite-sample results complementary to our analysis. Our diagnostic complements these methods: $\kappa_{\mathrm{DML}}$ indicates when such corrections may be needed and, after they are applied, confirms whether conditioning has improved. The gap we fill is simple but consequential: existing DML theory assumes good conditioning, and practitioners have no standard way to check whether their design satisfies this assumption. We provide a one-number summary that can be reported alongside point estimates and confidence intervals, bringing to DML the transparency that $F$-statistics brought to IV.


% ==========================================================================
\section{Setup: PLR Model, Orthogonal Score, and Condition Number}
\label{sec:setup}
% ==========================================================================

We consider the canonical partially linear regression (PLR) model. Observations $W_i = (Y_i, D_i, X_i)$, $i = 1, \ldots, n$, are drawn i.i.d.\ from a distribution $P$, where $Y_i \in \R$ is the outcome, $D_i \in \R$ is a scalar treatment or policy variable, and $X_i \in \R^p$ is a vector of controls or confounders. The structural model is
\begin{equation}
\label{eq:plr_model}
Y = D\theta_0 + g_0(X) + \varepsilon,
\qquad
\E[\varepsilon \mid D, X] = 0,
\end{equation}
where $\theta_0 \in \R$ is the scalar parameter of interest and $g_0: \R^p \to \R$ is an unknown nuisance function \citep{robinson1988root,belloni2014inference}.

Define the nuisance regression functions
\[
m_0(X) := \E[D \mid X],
\qquad
\ell_0(X) := \E[Y \mid X].
\]
Using \eqref{eq:plr_model},
\[
\ell_0(X)
= \theta_0 m_0(X) + g_0(X).
\]

The DML estimator for $\theta_0$ uses a Neyman-orthogonal score. For PLR, this score is
\begin{equation}
\label{eq:score}
\psi(W; \theta, \eta)
:= (D - m(X))\bigl(Y - g(X) - \theta(D - m(X))\bigr),
\end{equation}
where $\eta = (g, m)$ collects nuisance functions. At the reference point we take
\[
\eta_0 := (g_0^\star, m_0),
\qquad
g_0^\star(X) := \ell_0(X) = \E[Y\mid X].
\]
With this choice, the moment condition $\Psi(\theta,\eta) := \E[\psi(W;\theta,\eta)]$ satisfies $\Psi(\theta_0,\eta_0)=0$ and
\[
\partial_\eta \Psi(\theta_0,\eta)\big|_{\eta=\eta_0} = 0,
\]
so the score is locally insensitive to first-order perturbations in $\eta$.

The DML estimator uses $K$-fold cross-fitting. Let $\hat{m}$ and $\hat{g}$ denote cross-fitted estimators of $m_0$ and $\ell_0$. For each $i$, they are trained on folds not containing observation $i$. Define residualized variables
\begin{equation}
\label{eq:residuals}
\hat{U}_i := D_i - \hat{m}(X_i),
\qquad
\hat{V}_i := Y_i - \hat{g}(X_i).
\end{equation}
The empirical score average is
\[
\Psi_n(\theta, \hat{\eta})
:= \frac{1}{n}\sum_{i=1}^n \psi(W_i; \theta, \hat{\eta})
= \frac{1}{n}\sum_{i=1}^n \hat{U}_i(\hat{V}_i - \theta \hat{U}_i),
\]
and the DML estimator $\hat{\theta}$ solves $\Psi_n(\hat{\theta},\hat{\eta})=0$. This yields the familiar partialling-out formula
\begin{equation}
\label{eq:theta_hat}
\hat{\theta}
= \frac{\sum_{i=1}^n \hat{U}_i \hat{V}_i}{\sum_{i=1}^n \hat{U}_i^2}.
\end{equation}

The key object for our analysis is the empirical Jacobian
\begin{equation}
\label{eq:J_hat}
\hat{J}_\theta
:= \partial_\theta \Psi_n(\theta, \hat{\eta})
= -\frac{1}{n}\sum_{i=1}^n \hat{U}_i^2,
\end{equation}
which does not depend on $\theta$ and is nonpositive. We define the DML condition number
\begin{equation}
\label{eq:kappa}
\kappa_{\mathrm{DML}}
:= -\frac{1}{\hat{J}_\theta}
= \frac{1}{|\hat{J}_\theta|}
= \frac{n}{\sum_{i=1}^n \hat{U}_i^2},
\end{equation}
which is finite whenever $\sum_{i=1}^n \hat{U}_i^2>0$. The terminology ``condition number'' is borrowed from numerical analysis: a small Jacobian implies parameter estimates are hypersensitive to score perturbations. Small residual treatment variation implies a large $\kappa_{\mathrm{DML}}$, corresponding to a nearly flat score---the DML analogue of weak instruments \citep{staigerstock1997weakiv,stockyogo2005weakiv}.

The next section shows that $\kappa_{\mathrm{DML}}$ is not just a numerical curiosity but the primitive object that governs parameter-scale error and finite-sample coverage.

% ==========================================================================
\section{Finite-Sample Theory}
\label{sec:theory}
% ==========================================================================

This section establishes the theoretical core of the paper. We derive three main results: (i)~a refined linearisation that isolates $\kappa_{\mathrm{DML}}$ as the primitive conditioning object; (ii)~a parameter-scale error rate that shows how $\kappa_{\mathrm{DML}}$ multiplies both variance and nuisance bias; and (iii)~a coverage bound that quantifies finite-sample distortions. Together, these results provide the foundation for the conditioning regimes introduced in Section~\ref{sec:regimes}.

\subsection{Assumptions}

We impose three groups of conditions. The first specifies the model and moment requirements; the second makes explicit the role of residual treatment variance (overlap); the third governs nuisance estimation rates.

\begin{assumption}[Model and moments]\label{ass:model}
\begin{enumerate}[label=(\roman*),leftmargin=*]
\item The PLR model~\eqref{eq:plr_model} holds with $\E[\varepsilon \mid D, X] = 0$, and the score is given by~\eqref{eq:score}.
\item The score $\psi$ is Neyman-orthogonal: $\partial_\eta \E[\psi(W;\theta_0,\eta_0)][\eta - \eta_0] = 0$ \citep{chernozhukov2022locally}.
\item The score at the truth satisfies $0 < \sigma_\psi^2 := \Var(\psi(W;\theta_0,\eta_0)) < \infty$ and $\E[|\psi(W;\theta_0,\eta_0)|^3] \le M_3 < \infty$.
\end{enumerate}
\end{assumption}

\begin{assumption}[Overlap]\label{ass:overlap}
Define the population residual $U := D - m_0(X)$ and let $\sigma_U^2 := \E[U^2] = \Var(D)(1 - R^2(D \mid X))$. We require:\footnote{The quantity $\sigma_U^2$ measures the ``unexplained'' variation in treatment after conditioning on $X$. In the propensity score literature this corresponds to requiring bounded propensity scores away from 0 and 1; see Rosenbaum and Rubin (1983) for the original formulation and Crump et al.\ (2009) for overlap diagnostics.}
\[
  0 < \underline{\sigma}_U^2 \;\le\; \sigma_U^2 \;\le\; \bar{\sigma}_U^2 < \infty.
\]
This condition is the DML analogue of requiring a non-zero first stage in IV: it ensures $R^2(D \mid X) < 1$ and bounds the semiparametric variance \citet{hiranoimbensridder2003}.
\end{assumption}

\begin{assumption}[Nuisance rates]\label{ass:nuisance}
Let $\hat{m}$, $\hat{\ell}$ be cross-fitted estimators of $m_0$, $\ell_0$ using $K$-fold sample splitting \citep{neweyrobins2017cross,chernozhukov2018dml}. Define the product error
\[
  r_n := \|\hat{m} - m_0\|_{L^2(P)} \cdot \|\hat{\ell} - \ell_0\|_{L^2(P)}.
\]
We require:
\begin{enumerate}[label=(\roman*),leftmargin=*]
\item \emph{Product rate.} $r_n = o_P(n^{-1/2})$.
\item \emph{Jacobian concentration.} With probability at least $1 - \delta$,
\[
  \bigl| n^{-1}\textstyle\sum_{i=1}^n \hat{U}_i^2 - \sigma_U^2 \bigr| \le c_1 n^{-1/2}\log(1/\delta).
\]
\item \emph{Cross-fit regularity.} The cross-fitted residuals $\hat{U}_i = D_i - \hat{m}(X_i)$ satisfy $\E[\hat{U}_i^2 \mid \mathcal{D}_{-k}] \overset{p}{\to} \sigma_U^2$ uniformly over folds.
\end{enumerate}
The product-rate condition ensures nuisance-induced bias is $o_P(n^{-1/2})$; for Lasso rates see \citet{belloni2014inference} and \citet{vandegeer2014optimal}; for random forests see \citet{wagerathey2018}. Cross-fitting is essential for satisfying this condition with complex learners \citep{neweyrobins2017cross}.
\end{assumption}

\subsection{Linearisation}

The following lemma is the first main result. It provides a refined Z-estimation expansion that isolates $\kappa_{\mathrm{DML}}$ as the conditioning object.

Let
\[
\Psi_n(\theta,\eta)
:= \frac{1}{n}\sum_{i=1}^n \psi(W_i;\theta,\eta),
\]
and define
\begin{align}
S_n &:= \Psi_n(\theta_0, \eta_0)
     = \frac{1}{n}\sum_{i=1}^n \psi(W_i;\theta_0,\eta_0),
     \label{eq:Sn_theory}\\[0.25em]
B_n &:= \Psi_n(\theta_0, \hat{\eta}) - \Psi_n(\theta_0, \eta_0),
     \label{eq:Bn_theory}
\end{align}
with $R_n$ collecting higher-order terms.

\begin{lemma}[Refined linearisation]
\label{lem:linearization}
Under Assumptions~\ref{ass:model}--\ref{ass:nuisance},
\begin{equation}
\label{eq:linearization_compact}
\hat{\theta} - \theta_0
= \kappa_{\mathrm{DML}}\{S_n + B_n\} + R_n,
\end{equation}
where $\kappa_{\mathrm{DML}}$ is defined in \eqref{eq:kappa}, $S_n$ and $B_n$ are given by \eqref{eq:Sn_theory}--\eqref{eq:Bn_theory}, and $R_n = \op(n^{-1/2})$. Moreover, $S_n = \Op(n^{-1/2})$ and $B_n = \op(n^{-1/2})$.
\end{lemma}

\begin{proof}
The DML estimator $\hat{\theta}$ solves $\Psi_n(\hat{\theta}, \hat{\eta}) = 0$. Using the PLR score \eqref{eq:score}, this becomes
\[
\frac{1}{n}\sum_{i=1}^n \hat{U}_i(\hat{V}_i - \hat{\theta}\hat{U}_i) = 0,
\]
which yields the closed-form solution \eqref{eq:theta_hat}. To derive the linearisation, write
\begin{align}
\hat{\theta} - \theta_0 
&= \frac{\sum_i \hat{U}_i \hat{V}_i}{\sum_i \hat{U}_i^2} - \theta_0 
= \frac{\sum_i \hat{U}_i(\hat{V}_i - \theta_0 \hat{U}_i)}{\sum_i \hat{U}_i^2}. \label{eq:proof_step1}
\end{align}
Define population residuals $U_i := D_i - m_0(X_i)$ and $V_i := Y_i - \ell_0(X_i)$. By the model \eqref{eq:plr_model}, $V_i = \theta_0 U_i + \varepsilon_i$ where $\E[\varepsilon_i \mid X_i, D_i] = 0$. The numerator in \eqref{eq:proof_step1} decomposes as
\begin{align}
\sum_i \hat{U}_i(\hat{V}_i - \theta_0 \hat{U}_i)
&= \underbrace{\sum_i U_i \varepsilon_i}_{=: n S_n} 
+ \underbrace{\sum_i \hat{U}_i(\hat{V}_i - V_i) - \theta_0 \sum_i \hat{U}_i(\hat{U}_i - U_i)}_{=: n B_n}
+ R_n', \label{eq:proof_decomp}
\end{align}
where $R_n'$ collects higher-order cross-terms. 

\emph{Analysis of $S_n$:} By construction, $S_n = n^{-1}\sum_i U_i \varepsilon_i$ is a sample average of mean-zero random variables with variance $\sigma_\psi^2/n$. Under Assumption~\ref{ass:model}(iii), the CLT gives $\sqrt{n} S_n \dto N(0, \sigma_\psi^2)$, so $S_n = \Op(n^{-1/2})$.

\emph{Analysis of $B_n$:} The bias term $B_n$ arises from nuisance estimation error. Expanding using $\hat{V}_i - V_i = (\ell_0(X_i) - \hat{\ell}(X_i))$ and $\hat{U}_i - U_i = (\hat{m}(X_i) - m_0(X_i))$, and applying orthogonality (Assumption~\ref{ass:model}(ii)) together with the product-rate condition (Assumption~\ref{ass:nuisance}(i)), we obtain $B_n = \Op(r_n) = \op(n^{-1/2})$.

\emph{Denominator:} By Assumption~\ref{ass:nuisance}(ii)--(iii), $n^{-1}\sum_i \hat{U}_i^2 \pto \sigma_U^2$, so
\[
\kappa_{\mathrm{DML}} = \frac{n}{\sum_i \hat{U}_i^2} = \frac{1}{n^{-1}\sum_i \hat{U}_i^2} \pto \sigma_U^{-2}.
\]

Combining these elements:
\[
\hat{\theta} - \theta_0 = \frac{n(S_n + B_n) + R_n'}{\sum_i \hat{U}_i^2}
= \kappa_{\mathrm{DML}}(S_n + B_n) + R_n,
\]
where $R_n = \op(n^{-1/2})$ absorbs remainder terms. The rate $R_n = \op(n^{-1/2})$ follows from the product-rate condition ensuring $\kappa_{\mathrm{DML}} \cdot r_n = \op(n^{-1/2})$ when $\kappa_{\mathrm{DML}} = \Op(1)$.
\end{proof}

\begin{remark}[Interpretation]
\label{rem:linearization_interp_compact}
The decomposition \eqref{eq:linearization_compact} separates three sources of error. The term $\kappa_{\mathrm{DML}} S_n$ captures sampling fluctuation of order $\Op(\kappa_{\mathrm{DML}}/\sqrt{n})$. The term $\kappa_{\mathrm{DML}} B_n$ captures nuisance-induced bias; orthogonality ensures $B_n = \op(n^{-1/2})$ under the product-rate condition, but a large $\kappa_{\mathrm{DML}}$ can magnify even a small $B_n$ into a first-order term. When $\kappa_{\mathrm{DML}} = \Op(1)$, both contributions are $\Op(n^{-1/2})$ and standard DML inference applies. When $\kappa_{\mathrm{DML}}$ grows with $n$, confidence intervals widen proportionally---precisely mirroring how weak first stages inflate variance and bias in IV.
\end{remark}

The following proposition makes explicit the connection between $\kappa_{\mathrm{DML}}$ and the semiparametric efficiency bound, formalising the intuition that the condition number captures the ``effective information'' for estimating $\theta_0$.

\begin{proposition}[Efficiency bound connection]
\label{prop:efficiency}
Under Assumptions~\ref{ass:model}--\ref{ass:nuisance}, the semiparametric efficiency bound for $\theta_0$ in the PLR model is
\[
V_{\mathrm{eff}} = \frac{\E[U^2 \varepsilon^2]}{(\E[U^2])^2},
\]
where $U = D - m_0(X)$ is residual treatment variation and $\varepsilon = Y - D\theta_0 - g_0(X)$ is the structural error. The condition number satisfies $\kappa_{\mathrm{DML}} \pto 1/\E[U^2]$ as $n \to \infty$. Moreover, the asymptotic variance of the DML estimator equals $V_{\mathrm{eff}}$, and the finite-sample standard error satisfies
\[
\widehat{\mathrm{SE}}_{\mathrm{DML}} = \frac{\kappa_{\mathrm{DML}}}{\sqrt{n}} \cdot \widehat{\sigma}_\psi + o_P(n^{-1/2}),
\]
where $\widehat{\sigma}_\psi^2 := n^{-1}\sum_i \hat{U}_i^2 \hat{\varepsilon}_i^2 / (n^{-1}\sum_i \hat{U}_i^2)^2$ estimates the score variance.
\end{proposition}

\begin{proof}
The efficiency bound follows from \citet{hahn1998} and \citet{hiranoimbensridder2003} applied to the PLR model. In this model, the influence function is $\psi(W;\theta_0,\eta_0) = U\varepsilon$, which has variance $\E[U^2\varepsilon^2]$. The Jacobian is $J_\theta = -\E[U^2]$, so
\[
V_{\mathrm{eff}} = \frac{\E[\psi^2]}{J_\theta^2} = \frac{\E[U^2\varepsilon^2]}{(\E[U^2])^2}.
\]
From Lemma~\ref{lem:linearization}, $\sqrt{n}(\hat{\theta} - \theta_0) = \sqrt{n}\kappa_{\mathrm{DML}} S_n + o_P(1)$, where $\sqrt{n}S_n \dto N(0, \E[U^2\varepsilon^2])$. Since $\kappa_{\mathrm{DML}} \pto 1/\E[U^2]$, the asymptotic variance is $\E[U^2\varepsilon^2]/(\E[U^2])^2 = V_{\mathrm{eff}}$, confirming that DML achieves the efficiency bound under good conditioning. The standard error expression follows from the plug-in estimator for $V_{\mathrm{eff}}$.
\end{proof}

\begin{remark}[Why $\kappa_{\mathrm{DML}}$ captures identification strength]
\label{rem:kappa_identification}
Proposition~\ref{prop:efficiency} clarifies the economic interpretation of $\kappa_{\mathrm{DML}}$. The efficiency bound $V_{\mathrm{eff}}$ diverges as $\E[U^2] \to 0$---that is, as treatment becomes perfectly predictable from covariates. The condition number $\kappa_{\mathrm{DML}} \approx 1/\E[U^2]$ is thus a finite-sample measure of how close the design is to the singular boundary where the efficiency bound becomes infinite and identification fails. This connects our analysis to the weak semiparametric identification literature \citep{khantamer2010,kaji2021weak}: designs with large $\kappa_{\mathrm{DML}}$ are designs where identifying information is scarce, and standard inference becomes unreliable regardless of asymptotic guarantees.
\end{remark}

\begin{remark}[The role of cross-fitting]
\label{rem:crossfitting}
The product-rate condition $r_n = o_P(n^{-1/2})$ in Assumption~\ref{ass:nuisance}(i) is central to DML theory. Cross-fitting---training nuisance estimators $\hat{m}$ and $\hat{\ell}$ on data that excludes observation $i$ when evaluating them at $X_i$---is crucial for achieving this rate with flexible machine learning estimators \citep{neweyrobins2017cross,chernozhukov2018dml}. Without cross-fitting, complex learners like random forests can exhibit ``overfitting bias'' where the nuisance error correlates with the observation at which it is evaluated, violating orthogonality. Cross-fitting breaks this correlation, ensuring that $B_n = o_P(n^{-1/2})$ under the product-rate condition. Our analysis takes cross-fitting as given and focuses on what happens when conditioning is poor even after cross-fitting: the condition number $\kappa_{\mathrm{DML}}$ then determines whether residual nuisance error is magnified into first-order bias.
\end{remark}

\subsection{Coverage in \texorpdfstring{$t$}{t}-Scale and Back to Parameter Scale}

The preceding analysis focuses on the point estimator. We now turn to inference, connecting the linearisation to coverage properties of standard DML confidence intervals. The key insight is that the $t$-statistic scale hides the condition number, while the parameter scale reveals it.

Let
\begin{equation}
\label{eq:CI_std_theory}
\mathrm{CI}_{\mathrm{std}}
:= \Bigl[\hat{\theta} \pm z_{1-\alpha/2}\,\widehat{\mathrm{SE}}_{\mathrm{DML}}\Bigr],
\end{equation}
where $\widehat{\mathrm{SE}}_{\mathrm{DML}}$ is the usual plug-in standard error based on the orthogonal score, and define the $t$-statistic
\[
T_n := \frac{\hat{\theta}-\theta_0}{\widehat{\mathrm{SE}}_{\mathrm{DML}}}.
\]
Under Assumptions~\ref{ass:model}--\ref{ass:nuisance} and variance-estimation conditions of the type in \citet{chernozhukov2023simple,quintas2022finite}, one obtains a Berry--Esseen bound
\begin{equation}
\label{eq:coverage_bound_compact}
\left| \Prob\bigl(\theta_0 \in \mathrm{CI}_{\mathrm{std}}\bigr) - (1-\alpha) \right|
\le \frac{C_1}{\sqrt{n}}
 + C_2 \sqrt{n}\,r_n(\delta)
 + C_3 \delta,
\end{equation}
where $r_n(\delta)$ summarises nuisance and higher-order terms and $\delta$ controls the probability of concentration events. The rate $n^{-1/2} + \sqrt{n}\,r_n(\delta)$ matches existing finite-sample DML results up to constants, but \eqref{eq:coverage_bound_compact} is expressed in $t$-statistic scale and does not display $\kappa_{\mathrm{DML}}$ explicitly.\footnote{The term $\sqrt{n}\,r_n(\delta)$ requires $r_n = o(n^{-1/2})$ for coverage error to vanish. With Lasso rates $r_n = O(s\log p/n)^{1/2}$ where $s$ is sparsity, this translates to $s\log p = o(n^{1/2})$. With random forest rates, $r_n$ typically involves smoothness conditions and can be slower, explaining why flexible learners sometimes exhibit larger coverage distortions in finite samples.}

Combining \eqref{eq:coverage_bound_compact} with Lemma~\ref{lem:linearization} yields the following parameter-scale implication:

\begin{proposition}[Parameter-scale rate]
\label{prop:parameter_rate}
Under Assumptions~\ref{ass:model}--\ref{ass:nuisance} and the conditions leading to \eqref{eq:coverage_bound_compact},
\[
\hat{\theta} - \theta_0
= O_P\!\Bigl(\frac{\kappa_{\mathrm{DML}}}{\sqrt{n}} + \kappa_{\mathrm{DML}} r_n(\delta)\Bigr).
\]
In particular, confidence-set diameters based on $\mathrm{CI}_{\mathrm{std}}$ vanish only if $\kappa_{\mathrm{DML}} = o_P(\sqrt{n})$ and $\kappa_{\mathrm{DML}} r_n(\delta) \to 0$.
\end{proposition}

\begin{remark}[From $t$-scale to weak-identification regimes]
\label{rem:t_to_theta_compact}
Equation \eqref{eq:coverage_bound_compact} explains why $t$-statistics can look well behaved even when parameter-scale inference is poor: the normalisation hides $\kappa_{\mathrm{DML}}$. To see this, note that from Proposition~\ref{prop:efficiency}, $\widehat{\mathrm{SE}}_{\mathrm{DML}} \approx \kappa_{\mathrm{DML}}/\sqrt{n} \cdot \widehat{\sigma}_\psi$. Dividing the parameter-scale error $\hat{\theta} - \theta_0$ by the standard error cancels $\kappa_{\mathrm{DML}}$, yielding a $t$-statistic that can be close to $N(0,1)$ even when $\kappa_{\mathrm{DML}}$ is large. But this conceals the fact that both the numerator and denominator are inflated: the confidence interval $\mathrm{CI}_{\mathrm{std}}$ may achieve nominal coverage yet be so wide as to be uninformative. Proposition~\ref{prop:parameter_rate} shows that when $\kappa_{\mathrm{DML}}$ grows with $n$, both variance and bias are amplified in $\theta$-space, and intervals cease to shrink at the nominal rate. This mirrors the distinction between regular and weak-ID regimes in IV and GMM \citep{staigerstock1997weakiv,stockwright2000weakgmm,dufour2003}: weak instruments do not necessarily destroy normality of the $t$-statistic in moderate samples, but they do undermine the informativeness of confidence sets.
\end{remark}

\subsection{Design Sequences and Overlap}

To connect $\kappa_{\mathrm{DML}}$ to primitive features of the design, consider the population residual $U := D - m_0(X)$ and its variance $\Var(U)$. In large samples, $\sum_i \hat{U}_i^2/n$ estimates $\Var(U)$, so $\kappa_{\mathrm{DML}}$ behaves like $1/\Var(U)$. The following proposition formalises the link to overlap.

\begin{proposition}[Conditioning and residual variance]
\label{prop:kappa_overlap}
Suppose $\Var(D) \in (0,\infty)$ is fixed along a sequence of designs and nuisances are estimated consistently. Then:
\begin{enumerate}[label=(\roman*),leftmargin=*]
\item If there exists $c>0$ such that $\Var(U) \ge c$ uniformly in $n$, then $\kappa_{\mathrm{DML}} = O_P(1)$ and the design is well-conditioned.
\item If $\Var(U_n) \to 0$ along a sequence of designs, then $\kappa_{\mathrm{DML}} \to \infty$ in probability along that sequence. In particular, whenever $R^2(D\mid X)$ tends to one, the DML score becomes ill-conditioned.
\end{enumerate}
\end{proposition}

Proposition~\ref{prop:kappa_overlap} shows that our conditioning analysis is a finite-sample, DML-specific manifestation of a more general phenomenon: as overlap deteriorates and residual treatment variance vanishes, semiparametric efficiency bounds diverge and locally robust estimators become unstable \citep{hiranoimbensridder2003,khantamer2010,chernozhukov2022locally}. The condition number $\kappa_{\mathrm{DML}}$ effectively measures the "distance" to the singular boundary where identification fails.\footnote{In the context of semiparametric efficiency, the variance bound for $\theta_0$ is proportional to $\E[U^2]^{-1}$. Since $\kappa_{\mathrm{DML}} \approx n/\sum \hat{U}_i^2 \approx \E[U^2]^{-1}$, $\kappa_{\mathrm{DML}}$ is a direct sample analogue of the efficiency bound's inflation factor.} The next section uses this insight to define conditioning regimes.

% ==========================================================================
\section{Conditioning Regimes and Identification Strength}
\label{sec:regimes}
% ==========================================================================

The combination of the linearisation in Lemma~\ref{lem:linearization}, the parameter-scale rate in Proposition~\ref{prop:parameter_rate}, the efficiency bound connection in Proposition~\ref{prop:efficiency}, and the overlap relationship in Proposition~\ref{prop:kappa_overlap} induces a natural classification into conditioning regimes. This classification parallels the strong--semi-strong--weak taxonomy in the IV literature \citep{staigerstock1997weakiv,stockwright2000weakgmm} and provides the conceptual vocabulary for interpreting $\kappa_{\mathrm{DML}}$ in practice.

Let $\kappa_n := \kappa_{\mathrm{DML}}$ emphasise the sample-size dependence along a sequence of designs.

\begin{definition}[Conditioning regimes]
\label{def:regimes}
Along a sequence of designs and sample sizes, we say that:
\begin{itemize}[leftmargin=*]
\item The design is \emph{well-conditioned} if $\kappa_n = O_P(1)$.
\item The design is \emph{moderately ill-conditioned} if $\kappa_n = O_P(n^\beta)$ for some $0 < \beta < 1/2$.
\item The design is \emph{severely ill-conditioned} if $\kappa_n \asymp c\sqrt{n}$ for some $c>0$.
\end{itemize}
\end{definition}

Combining Proposition~\ref{prop:parameter_rate} with Definition~\ref{def:regimes} yields:

\begin{corollary}[Effective convergence rates]
\label{cor:regimes_rates}
Suppose the conditions of Proposition~\ref{prop:parameter_rate} hold and $r_n(\delta) = O(n^{-1/2-\gamma})$ for some $\gamma>0$. Then:
\begin{itemize}[leftmargin=*]
\item[(i)] In well-conditioned designs, $\hat{\theta} - \theta_0 = O_P(n^{-1/2})$ and the confidence interval length is $O_P(n^{-1/2})$.
\item[(ii)] In moderately ill-conditioned designs, $\hat{\theta} - \theta_0 = O_P(n^{\beta-1/2})$ and confidence intervals shrink but more slowly than $n^{-1/2}$.
\item[(iii)] In severely ill-conditioned designs with $\kappa_n \asymp c\sqrt{n}$, $\hat{\theta}-\theta_0 = O_P(1)$ and confidence interval diameters are $O_P(1)$: the intervals fail to shrink even as $n$ grows.
\end{itemize}
\end{corollary}

\begin{remark}[Comparison to local-to-zero asymptotics in IV]
\label{rem:local_to_zero}
The conditioning regimes in Definition~\ref{def:regimes} mirror the local-to-zero framework used to study weak instruments \citep{staigerstock1997weakiv,stockwright2000weakgmm}. In that framework, the first-stage coefficient $\pi$ is modeled as $\pi = c/\sqrt{n}$ so that the concentration parameter $\mu^2 = n\pi^2/\sigma_v^2 = c^2/\sigma_v^2$ remains constant as $n \to \infty$. This leads to non-standard limiting distributions where the IV estimator has $O_P(1)$ error rather than $O_P(n^{-1/2})$. Our severely ill-conditioned regime is the DML analogue: when $\kappa_n \asymp c\sqrt{n}$, residual treatment variance is vanishing as $\sigma_U^2 = O(n^{-1})$, and Proposition~\ref{prop:parameter_rate} shows that $\hat{\theta} - \theta_0 = O_P(1)$. Just as weak-IV asymptotics formalise the regime where standard 2SLS inference fails, our conditioning analysis formalises the regime where standard DML inference fails.
\end{remark}

\begin{remark}[Practical interpretation of regime boundaries]
\label{rem:practical_regimes}
In practice, the regime boundaries in Definition~\ref{def:regimes} are asymptotic statements. The relevant question for applied researchers is: for a given $n$ and observed $\kappa_{\mathrm{DML}}$, how should inference be interpreted? Our simulation evidence in Section~\ref{sec:simulations} provides empirical guidance. We find that for typical sample sizes ($n \in \{500, 2000\}$) and well-specified learners, designs with $\kappa_{\mathrm{DML}} < 1$ exhibit nominal coverage; designs with $1 \le \kappa_{\mathrm{DML}} < 2$ show transitional behaviour with some coverage deterioration; and designs with $\kappa_{\mathrm{DML}} \ge 2$ exhibit substantial variance inflation or, for biased learners, severe undercoverage. These thresholds are illustrative rather than universal: they depend on the learner, the nuisance complexity, and the sample size. We recommend reporting $\kappa_{\mathrm{DML}}$ as a continuous diagnostic and using robustness checks when it exceeds moderate values.
\end{remark}

These regimes are directly analogous to strong-, intermediate-, and weak-identification regimes in IV and GMM \citep{staigerstock1997weakiv,stockwright2000weakgmm,dufour2003}: just as weak instruments lead to 2SLS error of $O_P(1)$, severely ill-conditioned DML has $\hat{\theta} - \theta_0 = O_P(1)$. In practice, $\kappa_{\mathrm{DML}}$ is observed rather than its limit, and we treat it as a continuous diagnostic that summarises the ``identification content'' of the design. The next section validates these theoretical predictions through Monte Carlo simulation.

% ==========================================================================
\section{Simulation Evidence: A Map of Conditioning Regimes}
\label{sec:simulations}
% ==========================================================================

This section uses Monte Carlo experiments to show how overlap, $\kappa_{\mathrm{DML}}$, and finite-sample performance are linked in practice. The designs are deliberately simple so that the behaviour of $\kappa_{\mathrm{DML}}$ can be seen transparently.

\subsection{Design and Implementation}

We work in the PLR model \eqref{eq:plr_model} with scalar $D$, scalar $Y$, and $p=10$ covariates, using the Gaussian AR(1) design and treatment equation described in your existing simulation section. We calibrate three overlap levels via $R^2(D\mid X) \in \{0.75, 0.90, 0.97\}$, two sample sizes $n \in \{500, 2000\}$, and three nuisance learners (OLS, Lasso, random forests), all with $K=5$-fold cross-fitting. For each $(n,R^2,\text{learner})$ cell we run $B=500$ replications and record $\hat{\theta}$, $\widehat{\mathrm{SE}}_{\mathrm{DML}}$, $\kappa_{\mathrm{DML}}$, coverage of $\mathrm{CI}_{\mathrm{std}}$, CI length, bias, RMSE, and the sample $R^2(D\mid X)$.

We also include one “almost real’’ design calibrated to a treatment-on-covariates setting with limited overlap and mild misspecification, following the spirit of \citet{kang2007demystifying}. In that design, $D$ is generated from a nonlinear function of $X$ plus noise, and $g_0(X)$ is misspecified for linear learners, so that flexible methods have an advantage but also more scope for small residual bias.

To conserve space in the main text (as required by \emph{The Econometrics Journal}), we summarise the Monte Carlo evidence in two tables and refer to additional figures and high-dimensional results in an Online Appendix.

\subsection{From Overlap to \texorpdfstring{$\kappa_{\mathrm{DML}}$}{kappa}}

Table~\ref{tab:design_summary_main} reports median, mean, and standard deviation of $\kappa_{\mathrm{DML}}$ by overlap regime, pooling across $n$ and learners.

\begin{table}[!htbp]
\centering
\caption{Distribution of $\kappa_{\mathrm{DML}}$ by Overlap Level}
\label{tab:design_summary_main}
\small
\begin{tabular}{@{}lccccl@{}}
\toprule
\multirow{2}{*}{Overlap} & \multirow{2}{*}{$R^2(D\mid X)$} & \multicolumn{3}{c}{$\kappa_{\mathrm{DML}}$} & \multirow{2}{*}{Regime} \\
\cmidrule(lr){3-5}
 & & Median & Mean & SD & \\
\midrule
High     & 0.75 & 0.66 & 0.67 & 0.08 & Well-conditioned \\
Moderate & 0.90 & 1.68 & 1.67 & 0.52 & Transitional \\
Low      & 0.97 & 4.82 & 4.83 & 2.82 & Ill-conditioned \\
\bottomrule
\end{tabular}

\medskip
\raggedright
\footnotesize
\textit{Notes:} Pooled across $n \in \{500, 2000\}$ and learners (LIN, LAS, RF), $B=500$ replications per cell. Regime classification: well-conditioned ($\kappa < 1$), transitional ($1 \le \kappa < 2$), ill-conditioned ($\kappa \ge 2$). The sharp rightward shift in the $\kappa_{\mathrm{DML}}$ distribution as $R^2 \to 1$ reflects the hyperbolic divergence $\kappa \approx 1/(1 - R^2)$.
\end{table}

The table shows that as $R^2(D\mid X)$ increases from $0.75$ to $0.97$, the distribution of $\kappa_{\mathrm{DML}}$ shifts sharply rightward, consistent with Proposition~\ref{prop:kappa_overlap}. At high overlap ($R^2=0.75$), median $\kappa_{\mathrm{DML}} \approx 0.66$; at low overlap ($R^2=0.97$), median $\kappa_{\mathrm{DML}} \approx 4.8$---a roughly 7-fold increase. The standard deviation of $\kappa_{\mathrm{DML}}$ also grows with $R^2$, reflecting increased sensitivity to sampling variation in residual treatment variance when overlap is poor.

It is worth pausing to interpret these magnitudes. A condition number of $\kappa_{\mathrm{DML}} \approx 0.66$ implies that the effective sample size for the treatment parameter is roughly comparable to the nominal sample size. In contrast, $\kappa_{\mathrm{DML}} \approx 5$ implies that the effective information is drastically reduced. This shift is not linear: as $R^2$ approaches 1, $\kappa_{\mathrm{DML}}$ diverges hyperbolically. This explains why "mild" overlap violations can suddenly precipitate "severe" inference failures. A small increase in the predictive power of covariates can push a design from the stable "flat" part of the hyperbola to the vertical asymptote.

\subsection{From \texorpdfstring{$\kappa_{\mathrm{DML}}$}{kappa} to Coverage and RMSE}

To connect directly to the regimes in Corollary~\ref{cor:regimes_rates}, we classify each Monte Carlo cell by its median $\kappa_{\mathrm{DML}}$ into well-conditioned ($\kappa<1$), moderately ill-conditioned ($1\le \kappa<2$), and severely ill-conditioned ($\kappa\ge 2$). Table~\ref{tab:coverage_regime_main} reports coverage, CI length, bias, and RMSE by regime and learner.

\begin{table}[!htbp]
\centering
\caption{Inference Quality by Conditioning Regime and Nuisance Learner}
\label{tab:coverage_regime_main}
\small
\begin{tabular}{@{}llccccc@{}}
\toprule
$\kappa$-Regime & Learner & Coverage (\%) & CI Length & Bias & RMSE & Failure Mode \\
\midrule
\multirow{3}{*}{Well-cond.\ ($\kappa<1$)} 
  & LIN & 95.1 & 0.145 & \phantom{$-$}0.001 & 0.037 & --- \\
  & LAS & 94.3 & 0.146 & \phantom{$-$}0.000 & 0.038 & --- \\
  & RF  & 89.0 & 0.131 & $-0.022$ & 0.038 & Mild bias \\
\midrule
\multirow{3}{*}{Trans.\ ($1\le\kappa<2$)} 
  & LIN & 95.2 & 0.251 & \phantom{$-$}0.001 & 0.064 & Var.\ inflation \\
  & LAS & 94.8 & 0.251 & \phantom{$-$}0.001 & 0.066 & Var.\ inflation \\
  & RF  & 78.0 & 0.207 & $-0.052$ & 0.068 & Bias amplif. \\
\midrule
\multirow{3}{*}{Ill-cond.\ ($\kappa\ge 2$)} 
  & LIN & 94.8 & 0.476 & $-0.002$ & 0.123 & Var.\ inflation \\
  & LAS & 94.1 & 0.477 & $-0.001$ & 0.126 & Var.\ inflation \\
  & RF  & 68.1 & 0.286 & $-0.089$ & 0.104 & Severe bias \\
\bottomrule
\end{tabular}

\medskip
\raggedright
\footnotesize
\textit{Notes:} $B=500$ replications, pooled across $n \in \{500,2000\}$. Coverage is the proportion of nominal 95\% CIs containing $\theta_0=1$. ``Failure Mode'' summarises the dominant source of inference degradation: variance inflation (LIN/LAS maintain coverage via wide CIs) versus bias amplification (RF suffers undercoverage from $\kappa_{\mathrm{DML}} r_n$ term). Learners: LIN = linear regression, LAS = Lasso, RF = random forest.
\end{table}

The patterns match the theory. In well-conditioned designs ($\kappa<1$), linear and Lasso learners achieve 94--95\% coverage with intervals averaging 0.15 in length and RMSE $\approx 0.04$; Random Forests show mild undercoverage (89\%) due to residual overfitting bias. In transitional designs ($1 \le \kappa < 2$), simple learners maintain nominal coverage but intervals roughly double in length, while Random Forest coverage drops to 78\%---illustrating the $\kappa_{\mathrm{DML}} r_n$ channel whereby moderate conditioning combined with non-negligible nuisance error produces visible bias. In ill-conditioned designs ($\kappa \ge 2$), linear and Lasso learners preserve coverage only through very wide intervals (CI length $\approx 0.48$), while Random Forests suffer severe undercoverage (68\%) with RMSE $\approx 0.10$. This is precisely the trade-off predicted by Corollary~\ref{cor:regimes_rates}: large $\kappa_{\mathrm{DML}}$ magnifies any residual nuisance bias into first-order distortions. Additional high-dimensional results ($p>n$) in the Online Appendix confirm that $\kappa_{\mathrm{DML}}$ remains a useful fragility diagnostic as $p$ grows.

These results highlight two distinct failure modes for DML in finite samples, both governed by $\kappa_{\mathrm{DML}}$. The first is \emph{variance inflation}: for unbiased learners like OLS, large $\kappa_{\mathrm{DML}}$ explodes the standard error, producing honest but uninformative confidence intervals. The second is \emph{bias amplification}: for biased learners like Random Forests, large $\kappa_{\mathrm{DML}}$ multiplies residual nuisance error into first-order bias. A nuisance bias that would be negligible when $\kappa_{\mathrm{DML}} \approx 1$ (say, $B_n \approx 0.01$) can become catastrophic when $\kappa_{\mathrm{DML}} \approx 10$. Since interval width grows with $\sqrt{\kappa_{\mathrm{DML}}}$ via the standard error but bias grows with $\kappa_{\mathrm{DML}}$, increasing ill-conditioning eventually causes the bias to dominate, shifting intervals away from the truth. These findings reinforce recent proposals for trimming \citep{ma2023doubly} and calibration \citep{wuthrichzhu2024}: while such methods can reduce $\kappa_{\mathrm{DML}}$, any remaining bias will still be amplified, underscoring the need to report $\kappa_{\mathrm{DML}}$ as a diagnostic even after applying robustness corrections.

\subsection{High-Dimensional Extension: \texorpdfstring{$p > n$}{p > n}}

A natural question is whether $\kappa_{\mathrm{DML}}$ remains informative when the covariate dimension exceeds the sample size. We conduct a complementary Monte Carlo experiment with $n=200$ observations and $p=500$ covariates (ratio $p/n = 2.5$), using Lasso as the nuisance learner. This setting is relevant for many empirical applications where DML is deployed precisely because of high-dimensional confounders.\footnote{The design extends the low-dimensional specification: covariates follow $X \sim N(0, \Sigma(\rho))$ with $\rho=0.5$ and $p=500$, the treatment coefficient vector $\beta_D$ has decaying entries $\beta_{D,j} = 0.7^{j-1}$, and we calibrate $\sigma_U^2$ to achieve $R^2(D \mid X) \in \{0.75, 0.90, 0.97\}$ as before.}

Table~\ref{tab:high_dim} summarises the high-dimensional simulation.

\begin{table}[!htbp]
\centering
\caption{High-Dimensional PLR Study ($n=200$, $p=500$, Lasso, $B=500$)}
\label{tab:high_dim}
\small
\begin{tabular}{@{}lccccl@{}}
\toprule
$R^2(D\mid X)$ & Median $\kappa_{\mathrm{DML}}$ & Coverage (\%) & CI Length & RMSE & Regime \\
\midrule
0.75 (High overlap)     & 0.62 & 92.2 & 0.30 & 0.085 & $< 1$ \\
0.90 (Moderate overlap) & 1.78 & 89.8 & 0.52 & 0.157 & $[1,2)$ \\
0.97 (Low overlap)      & 6.27 & 87.4 & 0.97 & 0.325 & $\ge 2$ \\
\bottomrule
\end{tabular}

\medskip
\raggedright
\footnotesize
\textit{Notes:} Coverage is the proportion of nominal 95\% CIs containing $\theta_0=1$. ``CI Length'' is $2 \times 1.96 \times \widehat{\mathrm{SE}}_{\mathrm{DML}}$. Regime classification follows Section~\ref{sec:regimes}.
\end{table}

Several patterns emerge. First, $\kappa_{\mathrm{DML}}$ scales with overlap as expected: even with $p=500 > n=200$, the condition number increases sharply from $0.62$ at $R^2=0.75$ to $6.27$ at $R^2=0.97$---roughly a tenfold increase. Second, coverage degrades monotonically with $\kappa_{\mathrm{DML}}$: from 92\% in the well-conditioned regime to 87\% in the severely ill-conditioned regime. While less dramatic than the random-forest failures in the low-dimensional study, this decline illustrates that $\kappa_{\mathrm{DML}}$ remains a reliable diagnostic even when $p > n$. Third, CI length and RMSE scale with $\kappa_{\mathrm{DML}}$: average CI length more than triples from $0.30$ to $0.97$, and RMSE increases from $0.085$ to $0.325$, consistent with the rate $\kappa_{\mathrm{DML}}/\sqrt{n}$. Fourth, Lasso maintains reasonable coverage even when $\kappa_{\mathrm{DML}}$ is large, unlike random forests in the low-dimensional study. This aligns with theory: coverage failure requires both large $\kappa_{\mathrm{DML}}$ and non-negligible $r_n$; Lasso's regularization keeps nuisance error controlled.

Overall, the high-dimensional study reinforces the main message: $\kappa_{\mathrm{DML}}$ is a portable diagnostic that predicts inference quality across both low- and high-dimensional settings. When $p > n$, practitioners should be especially attentive to $\kappa_{\mathrm{DML}}$, since limited overlap becomes harder to detect through conventional diagnostics such as propensity score histograms \citep{damour2021overlap}.

\section{Empirical Illustration: LaLonde Job-Training Data}
\label{sec:lalonde}
% ==========================================================================

We now illustrate the diagnostic in the canonical job-training study of \citet{lalonde1986}, following the experimental and observational designs in the DML and DR literatures. This section both validates the simulation patterns and shows how $\kappa_{\mathrm{DML}}$ can be reported and interpreted in practice.

\subsection{Experimental and Observational Designs}

We consider two settings from the \citet{lalonde1986} data, which evaluates the National Supported Work (NSW) demonstration. The outcome $Y$ is real earnings in 1978, the treatment $D$ is participation in the job-training program, and covariates $X$ include age, education, race, marital status, and earnings in 1974 and 1975.\footnote{This dataset has become a ``litmus test'' for causal inference methods. \citet{dehejia1999causal} re-analysed it with propensity score matching; \citet{imbensxu2024lalonde} revisit it four decades later and conclude that modern methods perform well only when overlap is adequate.}

The first setting is the \emph{experimental sample} ($n=445$): the original randomised experiment where treatment status is assigned at random conditional on eligibility. Here, the propensity score is roughly constant, ensuring excellent overlap and small $\kappa_{\mathrm{DML}}$. The second setting is an \emph{observational sample} ($n=2675$): a nonexperimental comparison group constructed from the Panel Study of Income Dynamics (PSID-1), matched to the experimental treated units. In this design, treated units differ systematically from comparison units---they are younger, less educated, and have lower prior earnings. This creates a classic weak-overlap problem: the propensity score is close to zero for most comparison units and close to one for treated units, leading to near-deterministic treatment assignment conditional on $X$.

In both designs we estimate the PLR DML model using linear, Lasso, and random-forest learners for the nuisance functions, with $K$-fold cross-fitting. For each specification we report the DML point estimate, standard error, confidence interval, and $\kappa_{\mathrm{DML}}$.

\subsection{DML Estimates and Condition Numbers}

Table~\ref{tab:lalonde_main} summarises the main empirical findings.

\begin{table}[!htbp]
\centering
\caption{DML Estimates and Condition Numbers: LaLonde (1986) Job-Training Data}
\label{tab:lalonde_main}
\small
\begin{tabular}{@{}llccrcc@{}}
\toprule
Design & Learner & $n$ & $\hat{\theta}$ (\$) & SE & 95\% CI (\$) & $\kappa_{\mathrm{DML}}$ \\
\midrule
\multirow{3}{*}{Experimental}   
  & LIN   & 445  & 1,752 & 668 & [443, 3,060]   & 4.0  \\
  & Lasso & 445  & 1,793 & 672 & [475, 3,111]   & 4.1  \\
  & RF    & 445  & 1,455 & 634 & [213, 2,698]   & 3.9  \\
\midrule
\multirow{3}{*}{Observational}  
  & LIN   & 2,675 & 621   & 787 & [$-$921, 2,163]   & 21.9  \\
  & Lasso & 2,675 & 56    & 639 & [$-$1,196, 1,307] & 15.7  \\
  & RF    & 2,675 & $-$642 & 916 & [$-$2,438, 1,153] & 39.9  \\
\bottomrule
\end{tabular}

\medskip
\raggedright
\footnotesize
\textit{Notes:} Experimental benchmark $\approx \$1,794$ \citep{lalonde1986}. Experimental sample: randomised NSW treatment group ($n=185$) vs.\ NSW control group ($n=260$). Observational sample: NSW treated ($n=185$) vs.\ PSID-1 comparison group ($n=2,490$). Regime classification suppressed for clarity; all experimental estimates are in the moderate regime ($\kappa \approx 4$), all observational estimates are ill-conditioned ($\kappa \in [16, 40]$).
\end{table}

In the experimental design, $\kappa_{\mathrm{DML}}$ takes values around 4 across learners. While higher than the ideal $\kappa \approx 1$, this reflects the natural variance in a sample of $n=445$. The design remains in a moderate regime where inference is stable. Point estimates are robust across learners (\$1,455--\$1,793) and confidence intervals, while wide (\$2,000--\$2,600 in length), consistently exclude zero. This aligns with the experimental benchmark of a positive treatment effect of roughly \$1,794.

In the observational design, $\kappa_{\mathrm{DML}}$ increases dramatically to 16--40. This places the analysis firmly in the ill-conditioned regime. The consequences are immediate and severe. Point estimates fluctuate wildly, ranging from $-\$642$ (RF) to $+\$621$ (LIN), spanning zero and even reversing sign across specifications. Confidence intervals are extremely wide (over \$2,500 for RF) and, for Lasso and RF, fail to exclude zero. This is precisely what the theory predicts: near-deterministic treatment assignment conditional on $X$ (poor overlap) depletes residual treatment variation, inflates $\kappa_{\mathrm{DML}}$, and renders DML inference fragile.

\subsection{Visualising Conditioning and Fragility}

To highlight the connection between $\kappa_{\mathrm{DML}}$ and empirical fragility, Figure~\ref{fig:lalonde_kappa_ci} plots the DML point estimates and confidence intervals against $\kappa_{\mathrm{DML}}$ across all LaLonde specifications and learners.

% TODO: Generate this figure from results data before submission
% \begin{figure}[!htbp]
% \centering
% \includegraphics[width=0.7\textwidth]{lalonde_kappa_ci.pdf}
% \caption{DML Point Estimates and Confidence Intervals vs.\ $\kappa_{\mathrm{DML}}$ in LaLonde (1986)}
% \label{fig:lalonde_kappa_ci}
% \end{figure}

The figure visualises the "cone of uncertainty" that expands as $\kappa_{\mathrm{DML}}$ increases. Experimental specifications cluster at the left ($\kappa_{\mathrm{DML}} \approx 4$) with tight, overlapping intervals. Observational specifications scatter to the right ($\kappa_{\mathrm{DML}} \in [15,40]$), where the intervals not only widen but also drift apart. This drift is the hallmark of bias amplification: as $\kappa_{\mathrm{DML}}$ grows, small differences in how Lasso and Random Forests approximate the nuisance functions translate into large differences in the final estimate.

This analysis suggests that the long-standing debate over "which estimator works" in the LaLonde data is partly a debate about conditioning. The observational design is simply too ill-conditioned to support robust inference without strong parametric assumptions. Reporting $\kappa_{\mathrm{DML}}$ makes this limitation transparent. It warns the reader that the "null result" found by some observational methods might not be evidence of zero effect, but rather an artifact of variance inflation and bias amplification in a weak-identification regime.

\subsection{Policy Implications}

For applied researchers, the message is clear: trust in DML estimates must be conditional on $\kappa_{\mathrm{DML}}$. In the experimental sample, we can confidently recommend the program based on DML evidence. In the observational sample, the large $\kappa_{\mathrm{DML}}$ tells us that the data are insufficient to render a verdict, regardless of the machine learning method used. This distinction is invisible if one looks only at $p$-values or standard errors, but it becomes stark when $\kappa_{\mathrm{DML}}$ is reported.

% ==========================================================================
\section{Discussion and Practical Recommendations}
\label{sec:discussion}
% ==========================================================================

This section summarises the main insights and draws practical implications for applied DML analyses.

\subsection{Conceptual Message and Novelty}

Our results sharpen the conceptual message along three axes. First, we clarify \emph{what goes wrong in finite samples}: when the PLR DML score is ill-conditioned, small residual treatment variation leads to large $\kappa_{\mathrm{DML}}$, and the refined linearisation shows that both variance and nuisance-induced bias are multiplied by this condition number. Standard DML confidence intervals therefore either become very wide or undercover severely when residual nuisance error is non-negligible.

Second, we make explicit \emph{how $\kappa_{\mathrm{DML}}$ connects to variance and bias}. The parameter-scale rate $O_P(\kappa_{\mathrm{DML}}/\sqrt{n} + \kappa_{\mathrm{DML}} r_n)$ shows that conditioning affects both the sampling fluctuation and the nuisance remainder. Existing finite-sample DML theorems obtain similar $t$-statistic coverage rates under regularity; our contribution is to foreground the empirical Jacobian, demonstrate how it governs parameter-scale error, and use it to define conditioning regimes.

Third, we clarify \emph{how this differs from existing DML and weak-ID results}. We do not claim novelty in Z-estimation arguments or generic Berry--Esseen bounds; those follow \citep{chernozhukov2023simple,quintas2022finite,jung2023shortnote,neweymcfadden1994}. Our novelty lies in treating the orthogonal score as an object whose conditioning can be diagnosed, in connecting $\kappa_{\mathrm{DML}}$ to overlap and residual variance, and in demonstrating---via simulations and the LaLonde application---that this scalar diagnostic behaves as a DML analogue of weak-IV diagnostics.

\subsection{Practical Recommendations}

Our analysis suggests three practical recommendations for applied DML users. First, researchers should \emph{always compute and report $\kappa_{\mathrm{DML}}$}. The DML condition number is easy to compute from the cross-fitted residuals and provides a direct measure of how close the design is to a weak-ID regime. We recommend reporting it alongside point estimates and standard errors, as is standard for first-stage $F$-statistics in IV.

Second, researchers should \emph{interpret $\kappa_{\mathrm{DML}}$ as a continuous fragility gauge}. Our simulations and the LaLonde application show that increasing $\kappa_{\mathrm{DML}}$ systematically worsens the trade-off between variance and bias. We refrain from proposing a universal numerical threshold, since the impact of a given value depends on sample size and learner complexity. Instead, $\kappa_{\mathrm{DML}}$ should be viewed as a continuous indicator: small values correspond to regular, well-identified behaviour; very large values indicate that DML inference is fragile and highly sensitive to nuisance choices and design features such as overlap.\footnote{By analogy, the Stock--Yogo critical values for weak instruments depend on the desired worst-case bias or size distortion. Developing analogous critical values for $\kappa_{\mathrm{DML}}$ is an important direction for future work.}

Third, researchers should \emph{use $\kappa_{\mathrm{DML}}$ to guide design and robustness checks}. When $\kappa_{\mathrm{DML}}$ is moderate or large, we recommend (i)~diagnosing and, where possible, improving overlap via trimming or redefining the estimand on an overlap region \citep{crumphotzimbensmitnik2009,ma2023doubly}; (ii)~comparing simple and flexible learners to assess the role of $\kappa_{\mathrm{DML}} r_n$, since large changes in $\hat{\theta}$ across learners at high $\kappa_{\mathrm{DML}}$ are a clear red flag; and (iii)~interpreting standard DML intervals as diagnostic summaries rather than definitive uncertainty measures when $\kappa_{\mathrm{DML}}$ is very large.

\subsection{Limitations and Future Work}

Our analysis focuses on scalar PLR DML with cross-fitting and standard plug-in standard errors. Extending $\kappa$-based diagnostics to vector-valued parameters, IV-DML, panel and clustered designs, and more complex semiparametric models is an important direction for future work. Another natural step is to develop conditioning-aware inference procedures that remain valid even in severely ill-conditioned regimes, in the spirit of robust weak-IV methods \citep{mikusheva2010} and bias-aware regularised inference \citep{armstrong2020bias}. Finally, a systematic study of how different classes of machine learners---boosted trees, deep nets, and other modern methods---interact with $\kappa_{\mathrm{DML}}$ and the remainder term $r_n$ would give further guidance on learner choice in ill-conditioned designs.

Asymptotic DML theory remains valid in regular regimes, but its practical reliability hinges on the conditioning summarised by $\kappa_{\mathrm{DML}}$ and its interaction with nuisance estimation. Large condition numbers can degrade DML inference even in large samples, much as weak instruments degrade IV inference regardless of $n$. Making $\kappa_{\mathrm{DML}}$ a routine part of DML reporting is a simple, implementable step toward more transparent and reliable empirical work.

\bibliographystyle{chicago}

\begin{thebibliography}{99}

\bibitem[Armstrong et~al., 2020]{armstrong2020bias}
Armstrong, T.~B., Kolesár, M., and Kwon, S. (2020).
\newblock Bias-aware inference in regularized regression models.
\newblock arXiv preprint arXiv:2012.14823.

\bibitem[Andrews and Mikusheva, 2016]{andrewsmikusheva2016}
Andrews, I. and Mikusheva, A. (2016).
\newblock Conditional inference with a functional nuisance parameter.
\newblock {\em Econometrica}, 84(4):1571--1612.

\bibitem[Andrews et~al., 2019]{andrewsstocksun2019}
Andrews, I., Stock, J.~H., and Sun, L. (2019).
\newblock Weak instruments in instrumental variables regression: Theory and practice.
\newblock {\em Annual Review of Economics}, 11:727--753.

\bibitem[Bach et~al., 2022]{bach2022doubleml}
Bach, P., Chernozhukov, V., Kurz, M.~S., and Spindler, M. (2022).
\newblock DoubleML -- An object-oriented implementation of double machine learning in {P}ython.
\newblock {\em Journal of Machine Learning Research}, 23(53):1--6.

\bibitem[Belloni et~al., 2012]{belloni2012sparse}
Belloni, A., Chen, D., Chernozhukov, V., and Hansen, C. (2012).
\newblock Sparse models and methods for optimal instruments with an application to eminent domain.
\newblock {\em Econometrica}, 80(6):2369--2429.

\bibitem[Belloni et~al., 2014]{belloni2014inference}
Belloni, A., Chernozhukov, V., and Hansen, C. (2014).
\newblock Inference on treatment effects after selection among high-dimensional controls.
\newblock {\em The Review of Economic Studies}, 81(2):608--650.

\bibitem[Benkeser et~al., 2017]{benkesercaronevanderlaangilbert2017}
Benkeser, D., Carone, M., Van der Laan, M., and Gilbert, P.~B. (2017).
\newblock Doubly robust nonparametric inference on the average treatment effect.
\newblock {\em Biometrika}, 104(4):863--880.

\bibitem[Breunig et~al., 2023]{breunig2023semiparametric}
Breunig, C., Liu, X., and Yu, J. (2023).
\newblock Semiparametric {B}ayesian difference-in-differences.
\newblock arXiv preprint arXiv:2308.02036.

\bibitem[Chernozhukov et~al., 2018]{chernozhukov2018dml}
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. (2018).
\newblock Double/debiased machine learning for treatment and structural parameters.
\newblock {\em The Econometrics Journal}, 21(1):C1--C68.

\bibitem[Chernozhukov et~al., 2022]{chernozhukov2022locally}
Chernozhukov, V., Escanciano, J.~C., Ichimura, H., Newey, W.~K., and Robins, J.~M. (2022).
\newblock Locally robust semiparametric estimation.
\newblock {\em Econometrica}, 90(4):1501--1535.

\bibitem[Chernozhukov et~al., 2023]{chernozhukov2023simple}
Chernozhukov, V., Newey, W.~K., and Singh, R. (2023).
\newblock A simple and general debiased machine learning theorem with finite-sample guarantees.
\newblock {\em Biometrika}, 110(1):257--264.

\bibitem[Cragg and Donald, 1993]{craggdonald1993}
Cragg, J.~G. and Donald, S.~G. (1993).
\newblock Testing identifiability and specification in instrumental variable models.
\newblock {\em Econometric Theory}, 9(2):222--240.

\bibitem[Crump et~al., 2009]{crumphotzimbensmitnik2009}
Crump, R.~K., Hotz, V.~J., Imbens, G.~W., and Mitnik, O.~A. (2009).
\newblock Dealing with limited overlap in estimation of average treatment effects.
\newblock {\em Biometrika}, 96(1):187--199.

\bibitem[Dehejia and Wahba, 1999]{dehejia1999causal}
Dehejia, R.~H. and Wahba, S. (1999).
\newblock Causal effects in nonexperimental studies: Reevaluating the evaluation of training programs.
\newblock {\em Journal of the American Statistical Association}, 94(448):1053--1062.

\bibitem[Dukes et~al., 2024]{dukes2024doubly}
Dukes, O., Vansteelandt, S., and Whitney, D. (2024).
\newblock On doubly robust inference for double machine learning in semiparametric regression.
\newblock {\em Journal of Machine Learning Research}, 25(279):1--46.

\bibitem[D'Amour et~al., 2021]{damour2021overlap}
D'Amour, A., Ding, P., Feller, A., Lei, L., and Sekhon, J. (2021).
\newblock Overlap in observational studies with high-dimensional covariates.
\newblock {\em Journal of Econometrics}, 221(2):644--654.

\bibitem[Hirano et~al., 2003]{hiranoimbensridder2003}
Hirano, K., Imbens, G.~W., and Ridder, G. (2003).
\newblock Efficient estimation of average treatment effects using the estimated propensity score.
\newblock {\em Econometrica}, 71(4):1161--1189.

\bibitem[Imbens and Xu, 2024]{imbensxu2024lalonde}
Imbens, G.~W. and Xu, Y. (2024).
\newblock Comparing experimental and nonexperimental methods: What lessons have we learned four decades after {L}a{L}onde (1986)?
\newblock arXiv preprint arXiv:2402.01191.

\bibitem[Baiardi and Naghi, 2024]{baiardinaghi2024}
Baiardi, A. and Naghi, A.~A. (2024).
\newblock The value added of machine learning to causal inference: Evidence from revisited studies.
\newblock {\em The Econometrics Journal}, 27(2):213--234.

\bibitem[Knaus, 2022]{knaus2022dml}
Knaus, M.~C. (2022).
\newblock Double machine learning-based programme evaluation under unconfoundedness.
\newblock {\em The Econometrics Journal}, 25(3):602--627.

\bibitem[Xu et~al., 2020]{xu2020limited}
Xu, R., Kang, W., and Imbens, G.~W. (2020).
\newblock Inference on finite-population treatment effects under limited overlap.
\newblock {\em The Econometrics Journal}, 23(1):52--67.

\bibitem[Liu et~al., 2024]{liu2024regularized}
Liu, L., Mukherjee, R., and Robins, J.~M. (2024).
\newblock Regularized double machine learning in partially linear models with unobserved confounding.
\newblock arXiv preprint arXiv:2401.06103.

\bibitem[Jang et~al., 2024]{jang2024mixing}
Jang, J., Kim, S., and Lee, K. (2024).
\newblock Mixing samples to address weak overlap in causal inference.
\newblock arXiv preprint arXiv:2411.02036.

\bibitem[Dufour, 2003]{dufour2003}
Dufour, J.-M. (2003).
\newblock Identification, weak instruments, and statistical inference in econometrics.
\newblock {\em Canadian Journal of Economics}, 36(4):767--808.

\bibitem[Javanmard and Montanari, 2014]{javanmard2014confidence}
Javanmard, A. and Montanari, A. (2014).
\newblock Confidence intervals and hypothesis testing for high-dimensional regression.
\newblock {\em Journal of Machine Learning Research}, 15(1):2869--2909.

\bibitem[Jung, 2023]{jung2023shortnote}
Jung, Y. (2023).
\newblock A short note on finite sample analysis on double/debiased machine learning.
\newblock Manuscript, Purdue University.

\bibitem[Kaji, 2021]{kaji2021weak}
Kaji, T. (2021).
\newblock Theory of weak identification in semiparametric models.
\newblock {\em Econometrica}, 89(2):733--763.

\bibitem[Kang and Schafer, 2007]{kang2007demystifying}
Kang, J.~D. and Schafer, J.~L. (2007).
\newblock Demystifying double robustness: A comparison of alternative strategies for estimating a population mean from incomplete data.
\newblock {\em Statistical Science}, 22(4):523--539.

\bibitem[Ballinari and Bearth, 2024]{ballinari2024calibration}
Ballinari, D. and Bearth, N. (2024).
\newblock Improving the finite sample performance of double/debiased machine learning with propensity score calibration.
\newblock arXiv preprint arXiv:2409.04874.

\bibitem[Mikusheva and Sun, 2024]{mikusheva2024weak}
Mikusheva, A. and Sun, L. (2024).
\newblock Weak identification with many instruments.
\newblock {\em The Econometrics Journal}, 27(2):C1--C28.

\bibitem[Foster and Syrgkanis, 2023]{fostersyrgkanis2023}
Foster, D.~J. and Syrgkanis, V. (2023).
\newblock Orthogonal statistical learning.
\newblock {\em The Annals of Statistics}, 51(3):879--908.


\bibitem[Khan and Tamer, 2010]{khantamer2010}
Khan, S. and Tamer, E. (2010).
\newblock Irregular identification, support conditions, and inverse weight estimation.
\newblock {\em Econometrica}, 78(6):2021--2042.

\bibitem[LaLonde, 1986]{lalonde1986}
LaLonde, R.~J. (1986).
\newblock Evaluating the econometric evaluations of training programs with experimental data.
\newblock {\em American Economic Review}, 76(4):604--620.

\bibitem[Ma et~al., 2023]{ma2023doubly}
Ma, Y., Sant'Anna, P.~H., Sasaki, Y., and Ura, T. (2023).
\newblock Doubly robust estimators with weak overlap.
\newblock arXiv preprint arXiv:2304.02036.

\bibitem[Mikusheva, 2010]{mikusheva2010}
Mikusheva, A. (2010).
\newblock Robust confidence sets in the presence of weak instruments.
\newblock {\em Journal of Econometrics}, 157(2):236--247.

\bibitem[Naghi and Wirths, 2021]{naghi2021finite}
Naghi, A.~A. and Wirths, C.~P. (2021).
\newblock Finite sample evaluation of causal machine learning methods: Guidelines for the applied researcher.
\newblock Tinbergen Institute Discussion Paper 2021-090.

\bibitem[Newey and McFadden, 1994]{neweymcfadden1994}
Newey, W.~K. and McFadden, D. (1994).
\newblock Large sample estimation and hypothesis testing.
\newblock In Engle, R.~F. and McFadden, D. (eds.), {\em Handbook of Econometrics}, Volume IV, 2111--2245. Amsterdam: North-Holland.

\bibitem[Newey and Robins, 2017]{neweyrobins2017cross}
Newey, W.~K. and Robins, J.~M. (2017).
\newblock Cross-fitting and fast remainder rates for semiparametric estimation.
\newblock CeMMAP Working Paper CWP41/17.

\bibitem[Hahn, 1998]{hahn1998}
Hahn, J. (1998).
\newblock On the role of the propensity score in efficient semiparametric estimation of average treatment effects.
\newblock {\em Econometrica}, 66(2):315--331.

\bibitem[Newey, 1990]{newey1990semiparametric}
Newey, W.~K. (1990).
\newblock Semiparametric efficiency bounds.
\newblock {\em Journal of Applied Econometrics}, 5(2):99--135.

\bibitem[P\"otscher, 2002]{poetscher2002illposed}
P\"otscher, B.~M. (2002).
\newblock Lower risk bounds and properties of confidence sets for ill-posed estimation problems.
\newblock {\em Econometrica}, 70(3):1035--1065.

\bibitem[Quintas-Martinez, 2022]{quintas2022finite}
Quintas-Martinez, V.~M. (2022).
\newblock Finite-sample guarantees for high-dimensional DML.
\newblock arXiv preprint arXiv:2206.07386.

\bibitem[Robinson, 1988]{robinson1988root}
Robinson, P.~M. (1988).
\newblock Root-{N}-consistent semiparametric regression.
\newblock {\em Econometrica}, 56(4):931--954.

\bibitem[Robins et~al., 1994]{robinsrotnitzkyscharfstein1994}
Robins, J.~M., Rotnitzky, A., and Zhao, L.~P. (1994).
\newblock Estimation of regression coefficients when some regressors are not always observed.
\newblock {\em Journal of the American Statistical Association}, 89(427):846--866.

\bibitem[Rosenbaum and Rubin, 1983]{rosenbaumrubin1983}
Rosenbaum, P.~R. and Rubin, D.~B. (1983).
\newblock The central role of the propensity score in observational studies for causal effects.
\newblock {\em Biometrika}, 70(1):41--55.

\bibitem[Staiger and Stock, 1997]{staigerstock1997weakiv}
Staiger, D. and Stock, J.~H. (1997).
\newblock Instrumental variables regression with weak instruments.
\newblock {\em Econometrica}, 65(3):557--586.

\bibitem[Stock and Wright, 2000]{stockwright2000weakgmm}
Stock, J.~H. and Wright, J.~H. (2000).
\newblock GMM with weak identification.
\newblock {\em Econometrica}, 68(5):1055--1096.

\bibitem[Stock and Yogo, 2005]{stockyogo2005weakiv}
Stock, J.~H. and Yogo, M. (2005).
\newblock Testing for weak instruments in linear {IV} regression.
\newblock In Andrews, D.~W.~K. and Stock, J.~H. (eds.), {\em Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg}, 80--108. Cambridge University Press.

\bibitem[van~de Geer et~al., 2014]{vandegeer2014optimal}
van~de Geer, S., B{\"u}hlmann, P., Ritov, Y., and Dezeure, R. (2014).
\newblock On asymptotically optimal confidence regions and tests for high-dimensional models.
\newblock {\em The Annals of Statistics}, 42(3):1166--1202.

\bibitem[van~der Vaart, 1998]{vaart1998asymptotic}
van~der Vaart, A.~W. (1998).
\newblock {\em Asymptotic Statistics}.
\newblock Cambridge University Press.

\bibitem[Wager and Athey, 2018]{wagerathey2018}
Wager, S. and Athey, S. (2018).
\newblock Estimation and inference of heterogeneous treatment effects using random forests.
\newblock {\em Journal of the American Statistical Association}, 113(523):1228--1242.

\bibitem[W\"uthrich and Zhu, 2024]{wuthrichzhu2024}
W\"uthrich, K. and Zhu, Y. (2024).
\newblock Propensity score calibration for causal estimation with double machine learning.
\newblock arXiv preprint arXiv:2401.12093.

\bibitem[Zimmert, 2018]{zimmert2018causal}
Zimmert, M. (2018).
\newblock The finite sample performance of treatment effect estimators in high-dimensional settings.
\newblock arXiv preprint arXiv:1805.05067.

\bibitem[Chernozhukov et~al., 2022b]{chernozhukov2022riesz}
Chernozhukov, V., Newey, W.~K., and Singh, R. (2022).
\newblock Debiased machine learning of global and local parameters using regularized {R}iesz representers.
\newblock {\em The Econometrics Journal}, 25(3):576--601.

\bibitem[Kennedy, 2023]{kennedy2023semiparametric}
Kennedy, E.~H. (2023).
\newblock Semiparametric doubly robust targeted double machine learning: a review.
\newblock arXiv preprint arXiv:2203.06469.

\bibitem[Moreira, 2003]{moreira2003clr}
Moreira, M.~J. (2003).
\newblock A conditional likelihood ratio test for structural models.
\newblock {\em Econometrica}, 71(4):1027--1048.

\bibitem[Sant'Anna and Zhao, 2020]{santannazhao2020}
Sant'Anna, P.~H.~C. and Zhao, J. (2020).
\newblock Doubly robust difference-in-differences estimators.
\newblock {\em Journal of Econometrics}, 219(1):101--122.

\bibitem[Semenova and Chernozhukov, 2020]{semenovachernozhukov2020}
Semenova, V. and Chernozhukov, V. (2020).
\newblock Debiased machine learning of conditional average treatment effects and other causal functions.
\newblock {\em The Econometrics Journal}, 24(2):264--289.

\end{thebibliography}

% ==========================================================================
\section*{Appendix A: Proofs of Main Results}
\renewcommand{\theequation}{A.\arabic{equation}}
\renewcommand{\theHequation}{A.\arabic{equation}}
\renewcommand{\thesection}{A}
\setcounter{equation}{0}
% ==========================================================================

This appendix provides proofs of the main theoretical results stated in Section~3. Additional details and subsidiary results are available in the Online Supplement.

\medskip

\textbf{Proof of Proposition~3.4 (Parameter-scale rate):}
From Lemma~3.2, we have
\begin{equation}
\hat{\theta} - \theta_0 = \kappa_{\mathrm{DML}}(S_n + B_n) + R_n.
\end{equation}
Under Assumptions~3.1--3.3, $S_n = \Op(n^{-1/2})$ and $B_n = \Op(r_n)$ where $r_n = o_P(n^{-1/2})$ by the product-rate condition. The remainder satisfies $R_n = \op(n^{-1/2})$.

For the sampling term:
\begin{equation}
\kappa_{\mathrm{DML}} S_n = \Op\!\left(\frac{\kappa_{\mathrm{DML}}}{\sqrt{n}}\right).
\end{equation}

For the bias term, since $B_n = \Op(r_n(\delta))$ with probability at least $1 - \delta$:
\begin{equation}
\kappa_{\mathrm{DML}} B_n = \Op(\kappa_{\mathrm{DML}} r_n(\delta)).
\end{equation}

Combining these bounds:
\begin{equation}
\hat{\theta} - \theta_0 = \Op\!\left(\frac{\kappa_{\mathrm{DML}}}{\sqrt{n}} + \kappa_{\mathrm{DML}} r_n(\delta)\right) + \op(n^{-1/2}).
\end{equation}

For confidence interval diameters, $|\mathrm{CI}_{\mathrm{std}}| = 2 z_{1-\alpha/2} \widehat{\mathrm{SE}}_{\mathrm{DML}}$. Under standard variance estimation conditions, $\widehat{\mathrm{SE}}_{\mathrm{DML}} = \Op(\kappa_{\mathrm{DML}}/\sqrt{n})$. Hence intervals shrink only if $\kappa_{\mathrm{DML}} = o_P(\sqrt{n})$ and the bias condition $\kappa_{\mathrm{DML}} r_n(\delta) \to 0$ holds. \hfill$\square$

\medskip

\textbf{Proof of Proposition~3.5 (Conditioning and overlap):}
\emph{Part (i):} By definition, $\kappa_{\mathrm{DML}} = n/\sum_{i=1}^n \hat{U}_i^2 = 1/(n^{-1}\sum_i \hat{U}_i^2)$. Under Assumption~3.3(ii)--(iii), $n^{-1}\sum_i \hat{U}_i^2 \pto \sigma_U^2 = \Var(U)$. If $\Var(U) \ge c > 0$ uniformly, then $n^{-1}\sum_i \hat{U}_i^2 \ge c/2$ with high probability for large $n$, implying $\kappa_{\mathrm{DML}} \le 2/c = \Op(1)$.

\emph{Part (ii):} Suppose $\Var(U_n) = \sigma_{U,n}^2 \to 0$ along a sequence. Then $n^{-1}\sum_i \hat{U}_i^2 \pto \sigma_{U,n}^2 \to 0$, which implies $\kappa_{\mathrm{DML}} = 1/(n^{-1}\sum_i \hat{U}_i^2) \pto \infty$.

The connection to $R^2(D \mid X)$ follows from the identity $\sigma_U^2 = \Var(D)(1 - R^2(D \mid X))$. As $R^2(D \mid X) \to 1$, we have $\sigma_U^2 \to 0$, and hence $\kappa_{\mathrm{DML}} \to \infty$. \hfill$\square$

\medskip

\textbf{Asymptotic Variance of the DML Estimator:}
The asymptotic variance of $\hat{\theta}$ under good conditioning ($\kappa_{\mathrm{DML}} = \Op(1)$) is derived as follows. From the linearisation (3.8), the leading term is $\kappa_{\mathrm{DML}} S_n$ where
\begin{equation}
S_n = \frac{1}{n}\sum_{i=1}^n U_i \varepsilon_i.
\end{equation}
By the CLT, $\sqrt{n} S_n \dto N(0, \E[U^2 \varepsilon^2])$. Hence
\begin{equation}
\sqrt{n}(\hat{\theta} - \theta_0) \dto N\!\left(0, \frac{\E[U^2 \varepsilon^2]}{(\E[U^2])^2}\right).
\end{equation}
The asymptotic variance $\sigma_\theta^2 = \E[U^2 \varepsilon^2]/(\E[U^2])^2$ matches the semiparametric efficiency bound for $\theta_0$ in the PLR model (Robinson, 1988; Newey, 1990). This confirms that DML achieves efficiency when well-conditioned.

When $\kappa_{\mathrm{DML}}$ grows with $n$, the variance inflates proportionally. If $\kappa_{\mathrm{DML}} = \Op(n^\beta)$ for $\beta > 0$, then
\begin{equation}
\Var(\hat{\theta}) = \Op\!\left(\frac{\kappa_{\mathrm{DML}}^2}{n}\right) = \Op(n^{2\beta - 1}),
\end{equation}
which exceeds the $\Op(n^{-1})$ rate when $\beta > 0$. For $\beta = 1/2$, the variance is $\Op(1)$, matching the severely ill-conditioned regime. \hfill$\square$

\end{document}

