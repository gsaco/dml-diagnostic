{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2145abb9",
   "metadata": {},
   "source": [
    "# Empirical Application: The DML Condition Number Diagnostic\n",
    "\n",
    "**Paper Reference**: Saco (2025), \"Finite-Sample Failures and Condition-Number Diagnostics in Double Machine Learning\"\n",
    "\n",
    "---\n",
    "\n",
    "This notebook demonstrates the practical relevance of the DML condition number $\\kappa_{\\text{DML}}$ using the canonical LaLonde (1986) job training dataset. We show that:\n",
    "\n",
    "1. **Well-conditioned designs** (experimental sample): $\\kappa_{\\text{DML}} \\approx 1$‚Äì4, delivering reliable inference\n",
    "2. **Ill-conditioned designs** (observational sample): $\\kappa_{\\text{DML}} \\gg 10$, leading to inflated standard errors and unreliable CIs\n",
    "\n",
    "The condition number is defined as:\n",
    "$$\\kappa_{\\text{DML}} := \\frac{1}{|\\hat{J}_\\theta|} = \\frac{n}{\\sum_{i=1}^n \\hat{U}_i^2}$$\n",
    "where $\\hat{U}_i = D_i - \\hat{m}(X_i)$ are the cross-fitted treatment residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc230dc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31merror: externally-managed-environment\n",
      "\u001b[1;31m\n",
      "\u001b[1;31m√ó This environment is externally managed\n",
      "\u001b[1;31m‚ï∞‚îÄ> To install Python packages system-wide, try brew install\n",
      "\u001b[1;31m    xyz, where xyz is the package you are trying to\n",
      "\u001b[1;31m    install.\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    If you wish to install a Python library that isn't in Homebrew,\n",
      "\u001b[1;31m    use a virtual environment:\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    python3 -m venv path/to/venv\n",
      "\u001b[1;31m    source path/to/venv/bin/activate\n",
      "\u001b[1;31m    python3 -m pip install xyz\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    If you wish to install a Python application that isn't in Homebrew,\n",
      "\u001b[1;31m    it may be easiest to use 'pipx install xyz', which will manage a\n",
      "\u001b[1;31m    virtual environment for you. You can install pipx with\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    brew install pipx\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    You may restore the old behavior of pip by passing\n",
      "\u001b[1;31m    the '--break-system-packages' flag to pip, or by adding\n",
      "\u001b[1;31m    'break-system-packages = true' to your pip.conf file. The latter\n",
      "\u001b[1;31m    will permanently disable this error.\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    If you disable this error, we STRONGLY recommend that you additionally\n",
      "\u001b[1;31m    pass the '--user' flag to pip, or set 'user = true' in your pip.conf\n",
      "\u001b[1;31m    file. Failure to do this can result in a broken Homebrew installation.\n",
      "\u001b[1;31m    \n",
      "\u001b[1;31m    Read more about this behavior here: <https://peps.python.org/pep-0668/>\n",
      "\u001b[1;31m\n",
      "\u001b[1;31mnote: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;31mhint: See PEP 668 for the detailed specification. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Empirical Application: LaLonde/NSW Data Analysis\n",
    "# =============================================================================\n",
    "# This notebook demonstrates the DML condition number diagnostic (Œ∫_DML)\n",
    "# using the canonical LaLonde (1986) job training dataset.\n",
    "#\n",
    "# Reference: Saco (2025), \"Finite-Sample Failures and Condition-Number \n",
    "# Diagnostics in Double Machine Learning\", The Econometrics Journal.\n",
    "# =============================================================================\n",
    "\n",
    "# Standard library\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scientific computing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "# =============================================================================\n",
    "# Publication-Quality Figure Settings (Econometrics Journal Style)\n",
    "# =============================================================================\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "\n",
    "# LaTeX-style fonts\n",
    "plt.rcParams.update({\n",
    "    # Figure size and DPI\n",
    "    'figure.figsize': (6.5, 4.5),\n",
    "    'figure.dpi': 150,\n",
    "    'savefig.dpi': 300,\n",
    "    'savefig.bbox': 'tight',\n",
    "    'savefig.pad_inches': 0.05,\n",
    "    \n",
    "    # Font settings (matching LaTeX)\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Computer Modern Roman', 'Times New Roman', 'DejaVu Serif'],\n",
    "    'font.size': 10,\n",
    "    'axes.titlesize': 11,\n",
    "    'axes.labelsize': 10,\n",
    "    'xtick.labelsize': 9,\n",
    "    'ytick.labelsize': 9,\n",
    "    'legend.fontsize': 9,\n",
    "    'legend.title_fontsize': 9,\n",
    "    \n",
    "    # Use LaTeX for text rendering (if available)\n",
    "    'text.usetex': False,  # Set True if LaTeX is installed\n",
    "    'mathtext.fontset': 'cm',\n",
    "    \n",
    "    # Lines and markers\n",
    "    'lines.linewidth': 1.5,\n",
    "    'lines.markersize': 6,\n",
    "    \n",
    "    # Axes\n",
    "    'axes.linewidth': 0.8,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'grid.linewidth': 0.5,\n",
    "    \n",
    "    # Legend\n",
    "    'legend.frameon': True,\n",
    "    'legend.framealpha': 0.9,\n",
    "    'legend.edgecolor': '0.8',\n",
    "    \n",
    "    # Tight layout\n",
    "    'figure.constrained_layout.use': True,\n",
    "})\n",
    "\n",
    "# Color palette for academic figures\n",
    "COLORS = {\n",
    "    'experimental': '#2C7BB6',  # Blue\n",
    "    'observational': '#D7191C',  # Red\n",
    "    'LIN': '#1B9E77',           # Teal\n",
    "    'LASSO': '#D95F02',         # Orange\n",
    "    'RF': '#7570B3',            # Purple\n",
    "    'benchmark': '#66A61E',     # Green\n",
    "    'threshold': '#E7298A',     # Magenta\n",
    "    'neutral': '#666666',       # Gray\n",
    "}\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Add src to path if needed\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"DML Condition Number Diagnostic: Empirical Application\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Matplotlib version: {mpl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f65056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our custom modules\n",
    "from empirical.data_lalonde import (\n",
    "    load_lalonde_data, \n",
    "    get_experimental_sample,\n",
    "    get_observational_sample,\n",
    "    get_covariate_matrix,\n",
    "    summary_statistics\n",
    ")\n",
    "\n",
    "from empirical.dml_kappa import (\n",
    "    PLRDoubleMLKappa,\n",
    "    get_learner,\n",
    "    estimate_propensity_score,\n",
    "    compute_overlap_diagnostics,\n",
    "    run_dml_analysis\n",
    ")\n",
    "\n",
    "from empirical.utils_tables import (\n",
    "    results_to_dataframe,\n",
    "    combine_results_tables,\n",
    "    format_results_table,\n",
    "    results_to_latex,\n",
    "    plot_propensity_histogram,\n",
    "    plot_overlap_comparison,\n",
    "    plot_kappa_by_design,\n",
    "    plot_ci_length_vs_kappa\n",
    ")\n",
    "\n",
    "print(\"Custom modules loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c925b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Loading and Sample Construction\n",
    "\n",
    "We load the LaLonde/NSW data and construct two samples:\n",
    "\n",
    "1. **Experimental sample**: NSW participants + NSW control group (randomised)\n",
    "2. **Observational sample**: NSW participants + CPS/PSID comparison group (non-experimental)\n",
    "\n",
    "The experimental sample provides a benchmark where treatment is randomised, while the observational sample illustrates the challenges of conditioning on covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd39f564",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LaLonde/NSW data\n",
    "\n",
    "df = load_lalonde_data(include_psid=True, include_cps=False, verbose=False)\n",
    "\n",
    "\n",
    "print(\"\\n=== Dataset Summary ===\")\n",
    "\n",
    "print(f\"Full dataset: {len(df)} observations\")\n",
    "\n",
    "print(f\"Treated: {int(df['D'].sum())}, Controls: {int((df['D'] == 0).sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719058c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct experimental and observational samples\n",
    "\n",
    "df_exp = get_experimental_sample(df)\n",
    "\n",
    "df_obs = get_observational_sample(df)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Experimental sample: n = {len(df_exp)}, n_treated = {int(df_exp['D'].sum())}\")\n",
    "\n",
    "print(f\"Observational sample: n = {len(df_obs)}, n_treated = {int(df_obs['D'].sum())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a77408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for key variables\n",
    "covariates = ['age', 'education', 'married', 'nodegree', 're74', 're75']\n",
    "\n",
    "print(\"\\n=== Experimental Sample ===\")\n",
    "display(summary_statistics(df_exp, covariates + ['re78']))\n",
    "\n",
    "print(\"\\n=== Observational Sample ===\")\n",
    "display(summary_statistics(df_obs, covariates + ['re78']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8ae2cd",
   "metadata": {},
   "source": [
    "### Covariate Balance\n",
    "\n",
    "Compare covariate distributions between treated and control groups in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariate_balance_table(df, covariates, treatment_col='treat'):\n",
    "    \"\"\"Compute standardised mean differences for covariate balance.\"\"\"\n",
    "    treated = df[df[treatment_col] == 1]\n",
    "    control = df[df[treatment_col] == 0]\n",
    "    \n",
    "    balance = []\n",
    "    for cov in covariates:\n",
    "        mean_t = treated[cov].mean()\n",
    "        mean_c = control[cov].mean()\n",
    "        std_t = treated[cov].std()\n",
    "        std_c = control[cov].std()\n",
    "        pooled_std = np.sqrt((std_t**2 + std_c**2) / 2)\n",
    "        smd = (mean_t - mean_c) / pooled_std if pooled_std > 0 else 0\n",
    "        balance.append({\n",
    "            'Covariate': cov,\n",
    "            'Mean (Treated)': mean_t,\n",
    "            'Mean (Control)': mean_c,\n",
    "            'SMD': smd\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(balance)\n",
    "\n",
    "print(\"=== Covariate Balance: Experimental ===\")\n",
    "balance_exp = covariate_balance_table(df_exp, covariates)\n",
    "display(balance_exp.style.format({'Mean (Treated)': '{:.2f}', 'Mean (Control)': '{:.2f}', 'SMD': '{:.3f}'}))\n",
    "\n",
    "print(\"\\n=== Covariate Balance: Observational ===\")\n",
    "balance_obs = covariate_balance_table(df_obs, covariates)\n",
    "display(balance_obs.style.format({'Mean (Treated)': '{:.2f}', 'Mean (Control)': '{:.2f}', 'SMD': '{:.3f}'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882309b7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Propensity Score Overlap Diagnostics\n",
    "\n",
    "Before running DML, we examine propensity score overlap. Poor overlap is a key driver of large $\\kappa_{\\text{DML}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate propensity scores using logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_exp = get_covariate_matrix(df_exp, covariates)\n",
    "D_exp = df_exp['treat'].values\n",
    "\n",
    "X_obs = get_covariate_matrix(df_obs, covariates)\n",
    "D_obs = df_obs['treat'].values\n",
    "\n",
    "# Fit logistic regression\n",
    "ps_model_exp = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "ps_model_exp.fit(X_exp, D_exp)\n",
    "ps_exp = ps_model_exp.predict_proba(X_exp)[:, 1]\n",
    "\n",
    "ps_model_obs = LogisticRegression(max_iter=1000, solver='lbfgs')\n",
    "ps_model_obs.fit(X_obs, D_obs)\n",
    "ps_obs = ps_model_obs.predict_proba(X_obs)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f2330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlap diagnostics\n",
    "\n",
    "overlap_exp = compute_overlap_diagnostics(D_exp, X_exp)\n",
    "\n",
    "overlap_obs = compute_overlap_diagnostics(D_obs, X_obs)\n",
    "\n",
    "\n",
    "\n",
    "print(\"=== Overlap Diagnostics ===\")\n",
    "\n",
    "print(f\"\\nExperimental sample:\")\n",
    "\n",
    "for k, v in overlap_exp.items():\n",
    "\n",
    "    # Avoid formatting arrays\n",
    "\n",
    "    if isinstance(v, (np.ndarray, list)):\n",
    "\n",
    "        print(f\"  {k}: array with shape {np.asarray(v).shape}\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nObservational sample:\")\n",
    "\n",
    "for k, v in overlap_obs.items():\n",
    "\n",
    "    if isinstance(v, (np.ndarray, list)):\n",
    "\n",
    "        print(f\"  {k}: array with shape {np.asarray(v).shape}\")\n",
    "\n",
    "    else:\n",
    "\n",
    "        print(f\"  {k}: {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e19eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Figure 1: Propensity Score Overlap (Publication Quality)\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(7, 3.5))\n",
    "\n",
    "# Panel (a): Experimental Sample\n",
    "ax = axes[0]\n",
    "ax.hist(ps_exp[D_exp == 0], bins=25, alpha=0.7, label='Control', \n",
    "        density=True, color=COLORS['experimental'], edgecolor='white', linewidth=0.5)\n",
    "ax.hist(ps_exp[D_exp == 1], bins=25, alpha=0.7, label='Treated', \n",
    "        density=True, color=COLORS['observational'], edgecolor='white', linewidth=0.5)\n",
    "ax.axvline(0.1, color=COLORS['neutral'], linestyle='--', linewidth=1, alpha=0.8)\n",
    "ax.axvline(0.9, color=COLORS['neutral'], linestyle='--', linewidth=1, alpha=0.8)\n",
    "ax.axvline(0.5, color='black', linestyle='-', linewidth=0.8, alpha=0.5)\n",
    "ax.set_xlabel(r'Propensity Score $\\hat{e}(X)$')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('(a) Experimental Sample', fontweight='bold')\n",
    "ax.legend(loc='upper right', framealpha=0.9)\n",
    "ax.set_xlim([-0.02, 1.02])\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "# Panel (b): Observational Sample  \n",
    "ax = axes[1]\n",
    "ax.hist(ps_obs[D_obs == 0], bins=25, alpha=0.7, label='Control (PSID)', \n",
    "        density=True, color=COLORS['experimental'], edgecolor='white', linewidth=0.5)\n",
    "ax.hist(ps_obs[D_obs == 1], bins=25, alpha=0.7, label='Treated (NSW)', \n",
    "        density=True, color=COLORS['observational'], edgecolor='white', linewidth=0.5)\n",
    "ax.axvline(0.1, color=COLORS['neutral'], linestyle='--', linewidth=1, alpha=0.8, label='Trimming bounds')\n",
    "ax.axvline(0.9, color=COLORS['neutral'], linestyle='--', linewidth=1, alpha=0.8)\n",
    "ax.set_xlabel(r'Propensity Score $\\hat{e}(X)$')\n",
    "ax.set_ylabel('Density')\n",
    "ax.set_title('(b) Observational Sample', fontweight='bold')\n",
    "ax.legend(loc='upper right', framealpha=0.9)\n",
    "ax.set_xlim([-0.02, 1.02])\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../results/figure1_propensity_overlap.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig('../../results/figure1_propensity_overlap.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure 1 saved: results/figure1_propensity_overlap.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2ff90a",
   "metadata": {},
   "source": [
    "**Interpretation**: \n",
    "- The experimental sample shows good overlap with propensity scores clustered around 0.5 (reflecting randomisation).\n",
    "- The observational sample shows poor overlap with the control group concentrated near 0 and the treated group spread across the range."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50f5bc1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. DML Estimation with $\\kappa_{\\text{DML}}$ Diagnostic\n",
    "\n",
    "We estimate the average treatment effect (ATE) of the NSW job training program using DML with three nuisance learners of varying flexibility:\n",
    "\n",
    "| Learner | Description | Flexibility |\n",
    "|---------|-------------|-------------|\n",
    "| **OLS** | Ordinary Least Squares | Parametric (low) |\n",
    "| **LASSO** | L1-regularised regression | Sparse/linear (moderate) |\n",
    "| **RF** | Random Forest | Nonparametric (high) |\n",
    "\n",
    "The key insight from the paper is that more flexible learners can *exacerbate* the conditioning problem in observational designs by more accurately predicting treatment status, leaving less residual variation $\\hat{U}_i = D_i - \\hat{m}(X_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc71336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learner configurations (simple keys used to select default learners)\n",
    "\n",
    "learner_configs = {\n",
    "\n",
    "    'LIN': {},\n",
    "\n",
    "    'LASSO': {},\n",
    "\n",
    "    'RF': {}\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Get outcome\n",
    "\n",
    "Y_exp = df_exp['re78'].values\n",
    "\n",
    "Y_obs = df_obs['re78'].values\n",
    "\n",
    "\n",
    "\n",
    "# DML settings\n",
    "\n",
    "n_folds = 5\n",
    "\n",
    "n_rep = 1  # Set to 10+ for publication\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Running DML with K={n_folds} folds, {n_rep} repetition(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bf07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DML for all designs and learners\n",
    "\n",
    "results_list = []\n",
    "\n",
    "for design_name, (X, D, Y) in [('Experimental', (X_exp, D_exp, Y_exp)), \n",
    "                                ('Observational', (X_obs, D_obs, Y_obs))]:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Design: {design_name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    for learner_name, config in learner_configs.items():\n",
    "        print(f\"\\n  Learner: {learner_name}\")\n",
    "        \n",
    "        # Get learners\n",
    "        ml_l = get_learner(learner_name)  # For E[Y|X]\n",
    "        ml_m = get_learner(learner_name)  # For E[D|X]\n",
    "        \n",
    "        # Create and fit DML model\n",
    "        dml = PLRDoubleMLKappa(\n",
    "            learner_m=ml_m,\n",
    "            learner_g=ml_l,\n",
    "            n_folds=n_folds,\n",
    "            random_state=42,\n",
    "        )\n",
    "        \n",
    "        dml.fit(Y, D, X)\n",
    "        \n",
    "        # Extract results\n",
    "        res = dml.summary_dict()\n",
    "        # Map to expected keys used in notebook\n",
    "        res['theta'] = res.pop('theta_hat')\n",
    "        res['se'] = res.pop('se_dml')\n",
    "        res['ci_lower'] = res.pop('ci_95_lower')\n",
    "        res['ci_upper'] = res.pop('ci_95_upper')\n",
    "        res['ci_length'] = res.pop('ci_length')\n",
    "        res['kappa_dml'] = res['kappa_dml']\n",
    "        res['n'] = res['n']\n",
    "        \n",
    "        res['Design'] = design_name\n",
    "        res['Learner'] = learner_name\n",
    "        results_list.append(res)\n",
    "        \n",
    "        print(f\"    Œ∏ÃÇ = {res['theta']:.2f} (SE = {res['se']:.2f})\")\n",
    "        print(f\"    Œ∫_DML = {res['kappa_dml']:.2f}\")\n",
    "        print(f\"    95% CI: [{res['ci_lower']:.2f}, {res['ci_upper']:.2f}]\")\n",
    "\n",
    "print(\"\\nDML estimation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473eb278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Reorder columns\n",
    "col_order = ['Design', 'Learner', 'n', 'theta', 'se', 'ci_lower', 'ci_upper', \n",
    "             'kappa_dml', 't_stat', 'p_value']\n",
    "results_df = results_df[[c for c in col_order if c in results_df.columns]]\n",
    "\n",
    "display(results_df.style.format({\n",
    "    'theta': '{:.2f}',\n",
    "    'se': '{:.2f}',\n",
    "    'ci_lower': '{:.2f}',\n",
    "    'ci_upper': '{:.2f}',\n",
    "    'kappa_dml': '{:.2f}',\n",
    "    't_stat': '{:.2f}',\n",
    "    'p_value': '{:.4f}'\n",
    "}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df165ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df.to_csv('../../results/empirical_dml_results.csv', index=False)\n",
    "print(\"Results saved to results/empirical_dml_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a814e15e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. The $\\kappa_{\\text{DML}}$ Diagnostic: Visualisation\n",
    "\n",
    "The condition number $\\kappa_{\\text{DML}}$ directly measures the flatness of the orthogonal score. From the paper's refined linearization:\n",
    "\n",
    "$$\\hat{\\theta} - \\theta_0 = \\kappa_{\\text{DML}} \\cdot (S_n + B_n) + R_n$$\n",
    "\n",
    "where $S_n$ is the score average and $B_n$ is the nuisance estimation bias. This implies:\n",
    "\n",
    "1. **Estimation error** scales as $\\kappa_{\\text{DML}}/\\sqrt{n}$ (variance) plus $\\kappa_{\\text{DML}} \\cdot r_n$ (bias)\n",
    "2. **CI length** scales as $\\kappa_{\\text{DML}}/\\sqrt{n}$\n",
    "3. **Three regimes**: Well-conditioned ($\\kappa \\approx 1$), moderately ill-conditioned ($\\kappa = O(n^\\beta)$), severely ill-conditioned ($\\kappa \\asymp \\sqrt{n}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0665b415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Figure 2: Condition Number by Design and Learner (Publication Quality)\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4))\n",
    "\n",
    "# Pivot for grouped bar chart\n",
    "pivot_kappa = results_df.pivot(index='Learner', columns='Design', values='kappa_dml')\n",
    "pivot_kappa = pivot_kappa.reindex(['LIN', 'LASSO', 'RF'])  # Ensure order\n",
    "pivot_kappa = pivot_kappa[['Experimental', 'Observational']]  # Order columns\n",
    "\n",
    "x = np.arange(len(pivot_kappa.index))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, pivot_kappa['Experimental'], width, \n",
    "               label='Experimental', color=COLORS['experimental'], \n",
    "               edgecolor='white', linewidth=0.8, alpha=0.9)\n",
    "bars2 = ax.bar(x + width/2, pivot_kappa['Observational'], width, \n",
    "               label='Observational', color=COLORS['observational'], \n",
    "               edgecolor='white', linewidth=0.8, alpha=0.9)\n",
    "\n",
    "# Add threshold line\n",
    "ax.axhline(y=10, color=COLORS['threshold'], linestyle='--', linewidth=1.5, \n",
    "           label=r'Threshold ($\\kappa = 10$)', zorder=1)\n",
    "\n",
    "ax.set_xlabel('Nuisance Learner')\n",
    "ax.set_ylabel(r'Condition Number $\\kappa_{\\mathrm{DML}}$')\n",
    "ax.set_title('DML Condition Number by Design and Learner', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['OLS', 'LASSO', 'Random Forest'])\n",
    "ax.legend(loc='upper left', framealpha=0.95)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height:.1f}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "\n",
    "# Set y-axis to start at 0\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../results/figure2_kappa_by_design.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig('../../results/figure2_kappa_by_design.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure 2 saved: results/figure2_kappa_by_design.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e530243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Figure 3: CI Length vs. Condition Number (Publication Quality)\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5.5, 4.5))\n",
    "\n",
    "results_df['ci_length'] = results_df['ci_upper'] - results_df['ci_lower']\n",
    "\n",
    "# Scatter plot with different markers for design\n",
    "markers = {'Experimental': 'o', 'Observational': 's'}\n",
    "learner_colors = {'LIN': COLORS['LIN'], 'LASSO': COLORS['LASSO'], 'RF': COLORS['RF']}\n",
    "\n",
    "for _, row in results_df.iterrows():\n",
    "    ax.scatter(row['kappa_dml'], row['ci_length'], \n",
    "               marker=markers[row['Design']], \n",
    "               c=learner_colors[row['Learner']], \n",
    "               s=120, alpha=0.9, edgecolors='black', linewidth=0.8, zorder=5)\n",
    "\n",
    "# Add reference line (theoretical relationship: CI length ‚àù Œ∫^{1/2})\n",
    "kappa_range = np.linspace(0.5, results_df['kappa_dml'].max() * 1.2, 100)\n",
    "# Baseline from well-conditioned experimental estimates\n",
    "exp_mask = results_df['Design'] == 'Experimental'\n",
    "baseline = results_df[exp_mask]['ci_length'].mean()\n",
    "baseline_kappa = results_df[exp_mask]['kappa_dml'].mean()\n",
    "ax.plot(kappa_range, baseline * np.sqrt(kappa_range / baseline_kappa), \n",
    "        color=COLORS['neutral'], linestyle='--', linewidth=1.5, alpha=0.7,\n",
    "        label=r'Theoretical: $\\propto \\sqrt{\\kappa}$', zorder=1)\n",
    "\n",
    "# Threshold line\n",
    "ax.axvline(x=10, color=COLORS['threshold'], linestyle=':', linewidth=1.5, \n",
    "           alpha=0.8, label=r'Threshold ($\\kappa = 10$)', zorder=2)\n",
    "\n",
    "ax.set_xlabel(r'Condition Number $\\kappa_{\\mathrm{DML}}$')\n",
    "ax.set_ylabel('95% CI Length (\\\\$)')\n",
    "ax.set_title('Confidence Interval Length vs. Condition Number', fontweight='bold')\n",
    "\n",
    "# Custom legend\n",
    "legend_elements = [\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor=COLORS['neutral'], \n",
    "           markersize=8, markeredgecolor='black', markeredgewidth=0.5, label='Experimental'),\n",
    "    Line2D([0], [0], marker='s', color='w', markerfacecolor=COLORS['neutral'], \n",
    "           markersize=8, markeredgecolor='black', markeredgewidth=0.5, label='Observational'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor=COLORS['LIN'], \n",
    "           markersize=8, markeredgecolor='black', markeredgewidth=0.5, label='OLS'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor=COLORS['LASSO'], \n",
    "           markersize=8, markeredgecolor='black', markeredgewidth=0.5, label='LASSO'),\n",
    "    Line2D([0], [0], marker='o', color='w', markerfacecolor=COLORS['RF'], \n",
    "           markersize=8, markeredgecolor='black', markeredgewidth=0.5, label='RF'),\n",
    "    Line2D([0], [0], linestyle='--', color=COLORS['neutral'], linewidth=1.5, \n",
    "           label=r'$\\propto \\sqrt{\\kappa}$'),\n",
    "]\n",
    "ax.legend(handles=legend_elements, loc='upper left', framealpha=0.95, ncol=2)\n",
    "\n",
    "ax.set_xlim(left=0)\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../results/figure3_ci_length_vs_kappa.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig('../../results/figure3_ci_length_vs_kappa.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure 3 saved: results/figure3_ci_length_vs_kappa.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59615e0e",
   "metadata": {},
   "source": [
    "**Key findings**:\n",
    "\n",
    "1. **Experimental design**: $\\kappa_{\\text{DML}} \\approx 4$ across all learners, reflecting the near-ideal overlap from randomisation.\n",
    "\n",
    "2. **Observational design**: $\\kappa_{\\text{DML}}$ is substantially larger (especially with flexible learners like RF), indicating poor overlap and inflated standard errors.\n",
    "\n",
    "3. **Learner effects**: More flexible learners (RF) tend to produce higher $\\kappa_{\\text{DML}}$ in the observational sample because they can more accurately predict treatment assignment, leaving less residual variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d0fc1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Robustness Analysis: Propensity Score Trimming\n",
    "\n",
    "A natural response to poor overlap is to **trim** observations with extreme propensity scores. This trades off sample size for improved conditioning:\n",
    "\n",
    "- **Benefits**: Reduces $\\kappa_{\\text{DML}}$ by removing observations where treatment is near-deterministic\n",
    "- **Costs**: Reduces sample size and may change the estimand (ATT on a restricted population)\n",
    "\n",
    "We examine symmetric trimming thresholds $\\alpha \\in \\{0, 0.01, 0.05, 0.10, 0.15\\}$, dropping observations with $\\hat{e}(X) < \\alpha$ or $\\hat{e}(X) > 1 - \\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361f3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trimming analysis for observational sample\n",
    "\n",
    "trim_thresholds = [0.0, 0.01, 0.05, 0.10, 0.15]\n",
    "\n",
    "trim_results = []\n",
    "\n",
    "\n",
    "\n",
    "for trim in trim_thresholds:\n",
    "\n",
    "    # Apply symmetric trimming\n",
    "\n",
    "    mask = (ps_obs > trim) & (ps_obs < 1 - trim)\n",
    "\n",
    "    n_trimmed = len(ps_obs) - mask.sum()\n",
    "\n",
    "    \n",
    "\n",
    "    if mask.sum() < 50:  # Skip if too few observations\n",
    "\n",
    "        continue\n",
    "\n",
    "    \n",
    "\n",
    "    X_trim = X_obs[mask]\n",
    "\n",
    "    D_trim = D_obs[mask]\n",
    "\n",
    "    Y_trim = Y_obs[mask]\n",
    "\n",
    "    \n",
    "\n",
    "    # Run DML with linear learner\n",
    "\n",
    "    ml_l = get_learner('LIN')\n",
    "\n",
    "    ml_m = get_learner('LIN')\n",
    "\n",
    "    \n",
    "\n",
    "    dml = PLRDoubleMLKappa(learner_m=ml_m, learner_g=ml_l, n_folds=5, random_state=42)\n",
    "\n",
    "    dml.fit(Y_trim, D_trim, X_trim)\n",
    "\n",
    "    \n",
    "\n",
    "    res = dml.summary_dict()\n",
    "\n",
    "    # Map to expected keys\n",
    "\n",
    "    res['theta'] = res.pop('theta_hat')\n",
    "\n",
    "    res['se'] = res.pop('se_dml')\n",
    "\n",
    "    res['ci_lower'] = res.pop('ci_95_lower')\n",
    "\n",
    "    res['ci_upper'] = res.pop('ci_95_upper')\n",
    "\n",
    "    res['ci_length'] = res.pop('ci_length')\n",
    "\n",
    "    res['kappa_dml'] = res['kappa_dml']\n",
    "\n",
    "    res['n'] = res['n']\n",
    "\n",
    "    \n",
    "\n",
    "    res['trim_threshold'] = trim\n",
    "\n",
    "    res['n_trimmed'] = n_trimmed\n",
    "\n",
    "    res['pct_trimmed'] = n_trimmed / len(ps_obs) * 100\n",
    "\n",
    "    trim_results.append(res)\n",
    "\n",
    "\n",
    "\n",
    "trim_df = pd.DataFrame(trim_results)\n",
    "\n",
    "display(trim_df[['trim_threshold', 'n', 'n_trimmed', 'pct_trimmed', 'theta', 'se', 'kappa_dml']].style.format({\n",
    "\n",
    "    'pct_trimmed': '{:.1f}%',\n",
    "\n",
    "    'theta': '{:.2f}',\n",
    "\n",
    "    'se': '{:.2f}',\n",
    "\n",
    "    'kappa_dml': '{:.2f}'\n",
    "\n",
    "}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193bd6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Figure 4: Trimming Analysis (Publication Quality)\n",
    "# =============================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(7, 3.5), layout='constrained')\n",
    "\n",
    "# Panel (a): Œ∫_DML vs trimming threshold\n",
    "ax = axes[0]\n",
    "ax.plot(trim_df['trim_threshold'] * 100, trim_df['kappa_dml'], \n",
    "        'o-', color=COLORS['experimental'], linewidth=2, markersize=8, \n",
    "        markeredgecolor='white', markeredgewidth=1)\n",
    "ax.axhline(y=10, color=COLORS['threshold'], linestyle='--', linewidth=1.5, \n",
    "           alpha=0.8, label=r'Threshold ($\\kappa = 10$)')\n",
    "ax.set_xlabel('Trimming Threshold (%)')\n",
    "ax.set_ylabel(r'Condition Number $\\kappa_{\\mathrm{DML}}$')\n",
    "ax.set_title(r'(a) Effect of Trimming on $\\kappa_{\\mathrm{DML}}$', fontweight='bold')\n",
    "ax.legend(loc='upper right', framealpha=0.95)\n",
    "ax.set_xlim(left=-0.5)\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "# Panel (b): Sample size vs Œ∫_DML trade-off\n",
    "ax = axes[1]\n",
    "scatter = ax.scatter(trim_df['n'], trim_df['kappa_dml'], \n",
    "                     c=trim_df['trim_threshold'] * 100, cmap='viridis', \n",
    "                     s=150, edgecolors='black', linewidth=1, zorder=5)\n",
    "for _, row in trim_df.iterrows():\n",
    "    ax.annotate(f\"{row['trim_threshold']*100:.0f}%\", \n",
    "                xy=(row['n'], row['kappa_dml']),\n",
    "                xytext=(8, 3), textcoords='offset points', fontsize=8)\n",
    "ax.axhline(y=10, color=COLORS['threshold'], linestyle='--', linewidth=1.5, alpha=0.8)\n",
    "ax.set_xlabel('Sample Size $n$')\n",
    "ax.set_ylabel(r'Condition Number $\\kappa_{\\mathrm{DML}}$')\n",
    "ax.set_title('(b) Sample Size vs. Conditioning Trade-off', fontweight='bold')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(scatter, ax=ax, shrink=0.8, pad=0.02)\n",
    "cbar.set_label('Trim (%)', fontsize=9)\n",
    "\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "plt.savefig('../../results/figure4_trimming_analysis.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig('../../results/figure4_trimming_analysis.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure 4 saved: results/figure4_trimming_analysis.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3b0fb5",
   "metadata": {},
   "source": [
    "### 5.2 Comparison Across All Learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ceb960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Figure 5: Forest Plot of Treatment Effect Estimates (Publication Quality)\n",
    "# =============================================================================\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6.5, 4.5))\n",
    "\n",
    "# Sort by Design then Learner\n",
    "results_df_sorted = results_df.sort_values(['Design', 'Learner'], ascending=[False, True])\n",
    "y_pos = np.arange(len(results_df_sorted))\n",
    "\n",
    "# Create labels\n",
    "labels = [f\"{row['Design']} ‚Äî {row['Learner']}\" for _, row in results_df_sorted.iterrows()]\n",
    "\n",
    "# Plot each point individually with appropriate colors\n",
    "for i, row in enumerate(results_df_sorted.itertuples(index=False)):\n",
    "    if 'Experimental' in labels[i]:\n",
    "        point_color = COLORS['experimental']\n",
    "    else:\n",
    "        point_color = COLORS['observational']\n",
    "    \n",
    "    ax.errorbar(row.theta, y_pos[i],\n",
    "                xerr=1.96 * row.se,\n",
    "                fmt='o', color=point_color, ecolor=point_color, \n",
    "                elinewidth=2, capsize=4, capthick=1.5, markersize=8,\n",
    "                markeredgecolor='white', markeredgewidth=0.8, zorder=5)\n",
    "\n",
    "# Reference line at 0\n",
    "ax.axvline(x=0, color=COLORS['neutral'], linestyle='-', linewidth=0.8, alpha=0.5, zorder=1)\n",
    "\n",
    "# Experimental benchmark (from LaLonde 1986 / Dehejia-Wahba 1999)\n",
    "EXPERIMENTAL_BENCHMARK = 1794\n",
    "ax.axvline(x=EXPERIMENTAL_BENCHMARK, color=COLORS['benchmark'], linestyle='-', \n",
    "           linewidth=2, alpha=0.7, label=f'Experimental benchmark (\\\\${EXPERIMENTAL_BENCHMARK:,})', zorder=2)\n",
    "\n",
    "# Add shaded region for benchmark uncertainty (illustrative)\n",
    "ax.axvspan(EXPERIMENTAL_BENCHMARK - 500, EXPERIMENTAL_BENCHMARK + 500, \n",
    "           alpha=0.1, color=COLORS['benchmark'], zorder=0)\n",
    "\n",
    "ax.set_yticks(y_pos)\n",
    "ax.set_yticklabels(labels)\n",
    "ax.set_xlabel('Treatment Effect on Earnings (\\\\$)')\n",
    "ax.set_title('DML Estimates with 95\\\\% Confidence Intervals', fontweight='bold')\n",
    "ax.legend(loc='lower right', framealpha=0.95)\n",
    "\n",
    "# Add grid only on x-axis\n",
    "ax.xaxis.grid(True, alpha=0.3)\n",
    "ax.yaxis.grid(False)\n",
    "\n",
    "# Adjust spines\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../results/figure5_forest_plot.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.savefig('../../results/figure5_forest_plot.png', bbox_inches='tight', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(\"Figure 5 saved: results/figure5_forest_plot.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c8c3d0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Summary and Conclusions\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "This empirical application demonstrates the practical relevance of the DML condition number diagnostic using the canonical LaLonde job training data:\n",
    "\n",
    "1. **Experimental vs. Observational**: The condition number $\\kappa_{\\text{DML}}$ is **substantially higher** in the observational sample (PSID comparison group) than in the experimental sample, reflecting the poor overlap between NSW participants and non-experimental controls.\n",
    "\n",
    "2. **Learner Effects**: More flexible nuisance learners (Random Forest) can **exacerbate** the conditioning problem by more accurately predicting treatment assignment. This leaves less residual treatment variation, inflating $\\kappa_{\\text{DML}}$.\n",
    "\n",
    "3. **CI Length Scaling**: Confidence interval length scales approximately as $\\sqrt{\\kappa_{\\text{DML}}}$, consistent with the paper's theoretical predictions.\n",
    "\n",
    "4. **Trimming Trade-offs**: Propensity score trimming improves conditioning but at the cost of sample size. There is a **bias-variance trade-off** in choosing the trimming threshold.\n",
    "\n",
    "### Practical Guidance\n",
    "\n",
    "When $\\kappa_{\\text{DML}} > 10$, practitioners should:\n",
    "- ‚ö†Ô∏è Investigate propensity score overlap diagnostics\n",
    "- ‚úÇÔ∏è Consider trimming extreme propensity scores\n",
    "- üìâ Use simpler nuisance learners if flexible ones produce high $\\kappa$\n",
    "- üìä Report $\\kappa_{\\text{DML}}$ alongside DML estimates as a routine diagnostic\n",
    "\n",
    "### Analogy to Instrumental Variables\n",
    "\n",
    "Just as the first-stage $F$-statistic and minimum eigenvalue diagnostics flag weak instruments in IV regression, $\\kappa_{\\text{DML}}$ flags weak conditioning in DML. Both diagnostics help practitioners assess when standard asymptotic inference may fail in finite samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table\n",
    "summary_table = results_df.pivot_table(\n",
    "    index='Design',\n",
    "    columns='Learner',\n",
    "    values=['theta', 'se', 'kappa_dml'],\n",
    "    aggfunc='first'\n",
    ")\n",
    "\n",
    "# Flatten column names\n",
    "summary_table.columns = [f\"{col[0]}_{col[1]}\" for col in summary_table.columns]\n",
    "\n",
    "print(\"=== Summary Table ===\")\n",
    "display(summary_table.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1106dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to LaTeX\n",
    "latex_table = results_to_latex(\n",
    "    results_df, \n",
    "    caption=\"DML estimates with condition number diagnostic\",\n",
    "    label=\"tab:empirical\"\n",
    ")\n",
    "\n",
    "with open('../../results/empirical_results.tex', 'w') as f:\n",
    "    f.write(latex_table)\n",
    "\n",
    "print(\"LaTeX table saved to results/empirical_results.tex\")\n",
    "print(\"\\n\" + latex_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afba76ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Reproducibility Notes\n",
    "\n",
    "### Session Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e61df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "print(\"=== Session Information ===\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"SciPy: {scipy.__version__}\")\n",
    "print(f\"Matplotlib: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output files\n",
    "import os\n",
    "\n",
    "results_dir = '../../results'\n",
    "print(\"\\n=== Output Files ===\")\n",
    "for f in sorted(os.listdir(results_dir)):\n",
    "    if f.startswith('empirical') or f.endswith('.pdf'):\n",
    "        path = os.path.join(results_dir, f)\n",
    "        size = os.path.getsize(path)\n",
    "        print(f\"  {f}: {size:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06157108",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Data Sources and Replication\n",
    "\n",
    "### Data Sources\n",
    "\n",
    "| Dataset | Description | Reference |\n",
    "|---------|-------------|-----------|\n",
    "| **NSW Experimental** | Randomised job training experiment | LaLonde (1986), *AER* |\n",
    "| **PSID Comparison** | Non-experimental control group | Dehejia & Wahba (1999), *JASA* |\n",
    "\n",
    "### References\n",
    "\n",
    "- LaLonde, R.J. (1986). \"Evaluating the Econometric Evaluations of Training Programs with Experimental Data.\" *American Economic Review*, 76(4), 604‚Äì620.\n",
    "\n",
    "- Dehejia, R.H. and Wahba, S. (1999). \"Causal Effects in Nonexperimental Studies: Reevaluating the Evaluation of Training Programs.\" *Journal of the American Statistical Association*, 94(448), 1053‚Äì1062.\n",
    "\n",
    "- Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C., Newey, W., and Robins, J. (2018). \"Double/Debiased Machine Learning for Treatment and Structural Parameters.\" *The Econometrics Journal*, 21(1), C1‚ÄìC68.\n",
    "\n",
    "### Replication\n",
    "\n",
    "```bash\n",
    "# Clone repository and install dependencies\n",
    "git clone https://github.com/gsaco/dml_paper.git\n",
    "cd dml_paper/src/empirical\n",
    "pip install -r requirements.txt\n",
    "\n",
    "# Run this notebook\n",
    "jupyter notebook empirical_application.ipynb\n",
    "```\n",
    "\n",
    "### Output Files\n",
    "\n",
    "All figures are saved to `results/` in both PDF (for LaTeX) and PNG (for preview) formats."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "economy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
