\documentclass{ectj}

\usepackage{amsfonts,amssymb,graphics,epsfig,verbatim,bm,latexsym,amsmath,url,amsbsy}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multirow}

% --------------------------------------------------------------------------
% Theorem Environments (following ECTJ template pattern)
% --------------------------------------------------------------------------
\newtheorem{theorem}{Theorem}
\newtheorem{assumption}{Assumption}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{lemma}{Lemma}
\newtheorem{example}{Example}
\newtheorem{remark}{Remark}

% --------------------------------------------------------------------------
% Put S in front of counters for online supplement
% --------------------------------------------------------------------------
\renewcommand{\theequation}{S.\arabic{equation}}
\renewcommand{\thesection}{S\arabic{section}}
\renewcommand{\thepage}{S\arabic{page}}
\renewcommand{\thetheorem}{S.\arabic{theorem}}
\renewcommand{\thelemma}{S.\arabic{lemma}}
\renewcommand{\thetable}{S.\arabic{table}}
\renewcommand{\thefigure}{S.\arabic{figure}}

\setcounter{equation}{0}
\setcounter{page}{1}

% --------------------------------------------------------------------------
% Custom Commands (same as main paper)
% --------------------------------------------------------------------------
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\norm}[1]{\lVert #1\rVert}
\newcommand{\abs}[1]{\lvert #1\rvert}
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\op}{o_P}
\newcommand{\Op}{O_P}

% --------------------------------------------------------------------------
% Front matter
% --------------------------------------------------------------------------
\year 2025
\received{December 2025}
\accepted{(to be completed by the journal)}
\volume{28}

\title[Finite-sample failures in DML: Online Supplement]{Finite-Sample Failures and Condition-Number Diagnostics
in Double Machine Learning: Online Supplement}

\author[Saco]{Gabriel Saco$^{\dagger}$}
\address{$^{\dagger}$(Affiliation to be added)}
\email{gsacoalvarado@gmail.com}


\begin{document}

\begin{abstract}

This online supplement contains additional theoretical details, extended simulation results, and supplementary proofs for ``Finite-Sample Failures and Condition-Number Diagnostics in Double Machine Learning.''

\keywords{Double Machine Learning, Online Supplement.}

\end{abstract}


% ==========================================================================
\section{Additional Theoretical Details}
% ==========================================================================

This section provides additional technical details supporting the main theoretical results.

\subsection{Detailed Proof of Lemma~3.2 (Refined Linearisation)}

\textbf{Proof:}
The DML estimator $\hat{\theta}$ solves $\Psi_n(\hat{\theta}, \hat{\eta}) = 0$. Using the PLR score (3.5), this becomes
\begin{equation}
\frac{1}{n}\sum_{i=1}^n \hat{U}_i(\hat{V}_i - \hat{\theta}\hat{U}_i) = 0,
\end{equation}
which yields the closed-form solution (3.9). To derive the linearisation, write
\begin{equation}
\hat{\theta} - \theta_0 
= \frac{\sum_i \hat{U}_i \hat{V}_i}{\sum_i \hat{U}_i^2} - \theta_0 
= \frac{\sum_i \hat{U}_i(\hat{V}_i - \theta_0 \hat{U}_i)}{\sum_i \hat{U}_i^2}.
\end{equation}

Define population residuals $U_i := D_i - m_0(X_i)$ and $V_i := Y_i - \ell_0(X_i)$. By the model (3.2), $V_i = \theta_0 U_i + \varepsilon_i$ where $\E[\varepsilon_i \mid X_i, D_i] = 0$. The numerator decomposes as
\begin{equation}
\sum_i \hat{U}_i(\hat{V}_i - \theta_0 \hat{U}_i)
= \underbrace{\sum_i U_i \varepsilon_i}_{=: n S_n} 
+ \underbrace{\sum_i \hat{U}_i(\hat{V}_i - V_i) - \theta_0 \sum_i \hat{U}_i(\hat{U}_i - U_i)}_{=: n B_n}
+ R_n',
\end{equation}
where $R_n'$ collects higher-order cross-terms. 

\emph{Analysis of $S_n$:} By construction, $S_n = n^{-1}\sum_i U_i \varepsilon_i$ is a sample average of mean-zero random variables with variance $\sigma_\psi^2/n$. Under Assumption~3.1(iii), the CLT gives $\sqrt{n} S_n \dto N(0, \sigma_\psi^2)$, so $S_n = \Op(n^{-1/2})$.

\emph{Analysis of $B_n$:} The bias term $B_n$ arises from nuisance estimation error. Expanding using $\hat{V}_i - V_i = (\ell_0(X_i) - \hat{\ell}(X_i))$ and $\hat{U}_i - U_i = (\hat{m}(X_i) - m_0(X_i))$, and applying orthogonality (Assumption~3.1(ii)) together with the product-rate condition (Assumption~3.3(i)), we obtain $B_n = \Op(r_n) = \op(n^{-1/2})$.

\emph{Denominator:} By Assumption~3.3(ii)--(iii), $n^{-1}\sum_i \hat{U}_i^2 \pto \sigma_U^2$, so
\begin{equation}
\kappa_{\mathrm{DML}} = \frac{n}{\sum_i \hat{U}_i^2} = \frac{1}{n^{-1}\sum_i \hat{U}_i^2} \pto \sigma_U^{-2}.
\end{equation}

Combining these elements:
\begin{equation}
\hat{\theta} - \theta_0 = \frac{n(S_n + B_n) + R_n'}{\sum_i \hat{U}_i^2}
= \kappa_{\mathrm{DML}}(S_n + B_n) + R_n,
\end{equation}
where $R_n = \op(n^{-1/2})$ absorbs remainder terms. The rate $R_n = \op(n^{-1/2})$ follows from the product-rate condition ensuring $\kappa_{\mathrm{DML}} \cdot r_n = \op(n^{-1/2})$ when $\kappa_{\mathrm{DML}} = \Op(1)$. \hfill$\square$


\subsection{Proof of Proposition~3.3 (Efficiency Bound Connection)}

\textbf{Proof:}
The efficiency bound follows from Hahn (1998) and Hirano, Imbens, and Ridder (2003) applied to the PLR model. In this model, the influence function is $\psi(W;\theta_0,\eta_0) = U\varepsilon$, which has variance $\E[U^2\varepsilon^2]$. The Jacobian is $J_\theta = -\E[U^2]$, so
\begin{equation}
V_{\mathrm{eff}} = \frac{\E[\psi^2]}{J_\theta^2} = \frac{\E[U^2\varepsilon^2]}{(\E[U^2])^2}.
\end{equation}

From Lemma~3.2, $\sqrt{n}(\hat{\theta} - \theta_0) = \sqrt{n}\kappa_{\mathrm{DML}} S_n + o_P(1)$, where $\sqrt{n}S_n \dto N(0, \E[U^2\varepsilon^2])$. Since $\kappa_{\mathrm{DML}} \pto 1/\E[U^2]$, the asymptotic variance is $\E[U^2\varepsilon^2]/(\E[U^2])^2 = V_{\mathrm{eff}}$, confirming that DML achieves the efficiency bound under good conditioning. The standard error expression follows from the plug-in estimator for $V_{\mathrm{eff}}$. \hfill$\square$


% ==========================================================================
\section{Extended Simulation Results}
% ==========================================================================

This section provides additional Monte Carlo results beyond those reported in the main text.

\subsection{Detailed Design Description}

We work in the PLR model with $n \in \{500, 2000\}$ observations, $p=10$ covariates, and the following data-generating process:

\begin{enumerate}
\item[(a)] Covariates: $X \sim N(0, \Sigma)$ where $\Sigma_{jk} = \rho^{|j-k|}$ with $\rho = 0.5$.

\item[(b)] Treatment: $D = X'\beta_D + \sigma_U \cdot \nu$ where $\nu \sim N(0,1)$ is independent of $X$, $\beta_D = (1, 0.5, 0.25, \ldots)$, and $\sigma_U^2$ is calibrated to achieve target $R^2(D \mid X) \in \{0.75, 0.90, 0.97\}$.

\item[(c)] Outcome: $Y = D\theta_0 + g_0(X) + \varepsilon$ where $\theta_0 = 1$, $g_0(X) = X'\beta_Y$ with $\beta_Y = (0.5, 0.3, 0.2, \ldots)$, and $\varepsilon \sim N(0, 1)$.
\end{enumerate}

\subsection{Full Results Tables}

Table~S.1 provides the complete simulation results for all combinations of overlap level, sample size, and nuisance learner.

\begin{table}[!htbp]
\centering
\caption{Complete Monte Carlo Results: PLR DML Simulations}
\label{tab:full_results}
\small
\begin{tabular}{@{}llccccccc@{}}
\toprule
$R^2$ & Learner & $n$ & $\bar{\kappa}$ & Coverage & CI Len & Bias & RMSE & $\bar{R}^2$ \\
\midrule
\multirow{6}{*}{0.75} 
  & LIN   & 500  & 0.67 & 95.2 & 0.18 & 0.001 & 0.046 & 0.75 \\
  & LIN   & 2000 & 0.66 & 95.0 & 0.09 & 0.000 & 0.023 & 0.75 \\
  & LAS   & 500  & 0.67 & 94.6 & 0.18 & 0.001 & 0.047 & 0.75 \\
  & LAS   & 2000 & 0.66 & 94.8 & 0.09 & 0.000 & 0.024 & 0.75 \\
  & RF    & 500  & 0.68 & 88.2 & 0.16 & $-$0.025 & 0.048 & 0.75 \\
  & RF    & 2000 & 0.66 & 89.8 & 0.08 & $-$0.012 & 0.025 & 0.75 \\
\midrule
\multirow{6}{*}{0.90} 
  & LIN   & 500  & 1.70 & 95.4 & 0.31 & 0.001 & 0.080 & 0.90 \\
  & LIN   & 2000 & 1.66 & 95.0 & 0.16 & 0.001 & 0.040 & 0.90 \\
  & LAS   & 500  & 1.71 & 94.4 & 0.31 & 0.002 & 0.082 & 0.90 \\
  & LAS   & 2000 & 1.66 & 95.2 & 0.16 & 0.000 & 0.041 & 0.90 \\
  & RF    & 500  & 1.73 & 76.4 & 0.25 & $-$0.058 & 0.086 & 0.90 \\
  & RF    & 2000 & 1.67 & 79.6 & 0.13 & $-$0.032 & 0.046 & 0.90 \\
\midrule
\multirow{6}{*}{0.97} 
  & LIN   & 500  & 5.12 & 94.6 & 0.59 & $-$0.003 & 0.152 & 0.97 \\
  & LIN   & 2000 & 4.53 & 95.0 & 0.30 & $-$0.001 & 0.077 & 0.97 \\
  & LAS   & 500  & 5.15 & 93.8 & 0.59 & $-$0.002 & 0.156 & 0.97 \\
  & LAS   & 2000 & 4.54 & 94.4 & 0.30 & $-$0.001 & 0.079 & 0.97 \\
  & RF    & 500  & 5.28 & 66.2 & 0.35 & $-$0.098 & 0.134 & 0.97 \\
  & RF    & 2000 & 4.58 & 70.0 & 0.18 & $-$0.062 & 0.078 & 0.97 \\
\bottomrule
\end{tabular}

\medskip
\raggedright
\footnotesize
\textit{Notes:} $B=500$ replications per cell. LIN = linear regression, LAS = Lasso, RF = random forest. $\bar{\kappa}$ = mean $\kappa_{\mathrm{DML}}$, $\bar{R}^2$ = mean sample $R^2(D \mid X)$. Coverage is the proportion of nominal 95\% CIs containing $\theta_0=1$. CI Len = average confidence interval length.
\end{table}


% ==========================================================================
\section{Data Sources and Replication}
% ==========================================================================

\subsection{LaLonde (1986) Data}

The empirical application uses data from the National Supported Work (NSW) demonstration, as analysed by LaLonde (1986). We use two samples:

\begin{enumerate}
\item[(a)] \textbf{Experimental sample} ($n=445$): The original randomised experiment combining NSW treated units ($n=185$) with NSW control units ($n=260$).

\item[(b)] \textbf{Observational sample} ($n=2,675$): NSW treated units ($n=185$) combined with the PSID-1 comparison group ($n=2,490$).
\end{enumerate}

The outcome variable is real earnings in 1978 (in 1982 dollars). Covariates include age, years of education, indicators for Black and Hispanic ethnicity, marital status, high school diploma, and lagged earnings in 1974 and 1975.

\subsection{Replication Code}

All simulations and empirical analyses were conducted in Python using the \texttt{dml\_diagnostic} package. Replication code is available at \url{https://github.com/gsaco/dml-diagnostic}.


% ==========================================================================
\section{The \texttt{dml\_diagnostic} Python Package}
\label{sec:package}
% ==========================================================================

This section documents the \texttt{dml\_diagnostic} Python package, which implements the DML condition number diagnostic $\kappa_{\mathrm{DML}}$ for practitioners.

\subsection{Installation}

The package can be installed via pip:

\begin{verbatim}
pip install dml-diagnostic
\end{verbatim}

For the latest development version:

\begin{verbatim}
pip install git+https://github.com/gsaco/dml-diagnostic.git
\end{verbatim}

Dependencies: numpy $\ge$ 1.20, pandas $\ge$ 1.3, scikit-learn $\ge$ 1.0. Optional: matplotlib for plotting.

\subsection{Quick Start}

The package provides a simple API for DML estimation with condition number diagnostics:

\begin{verbatim}
from dml_diagnostic import DMLDiagnostic, load_lalonde

# Load LaLonde experimental data
Y, D, X = load_lalonde(sample='experimental')

# Fit DML estimator with diagnostics
dml = DMLDiagnostic(learner='lasso')
result = dml.fit(Y, D, X)

# Print results with interpretation
print(result)
\end{verbatim}

Output:
\begin{verbatim}
DML Diagnostic Results
----------------------
  theta = 1793.42 (SE = 672.45)
  95% CI: [475.41, 3111.43]
  
  Condition Number: kappa_DML = 4.10
  
  n = 445, R^2(D|X) = -0.003, learner = lasso
\end{verbatim}

\subsection{API Reference}

\paragraph{DMLDiagnostic class.} Main estimator with condition number diagnostics.

\begin{verbatim}
DMLDiagnostic(
    learner='lasso',    # 'lin', 'lasso', 'ridge', 'rf', 'gbm'
    learner_m=None,     # Separate learner for E[D|X]
    learner_g=None,     # Separate learner for E[Y|X]
    n_folds=5,          # Cross-fitting folds
    random_state=42     # For reproducibility
)
\end{verbatim}

Methods:
\begin{itemize}
\item \texttt{fit(Y, D, X)}: Fit DML and compute $\kappa_{\mathrm{DML}}$, returns \texttt{DMLResult}.
\item \texttt{summary()}: Print detailed results with interpretation.
\end{itemize}

\paragraph{DMLResult attributes.}
\begin{itemize}
\item \texttt{theta}: Point estimate $\hat{\theta}$.
\item \texttt{se}: Standard error.
\item \texttt{ci\_lower}, \texttt{ci\_upper}: 95\% CI bounds.
\item \texttt{kappa}: Condition number $\kappa_{\mathrm{DML}}$.
\item \texttt{jacobian}: Empirical Jacobian $\hat{J}_\theta$.
\item \texttt{r\_squared\_d}: $R^2(D \mid X)$ from treatment regression.
\item \texttt{U\_hat}, \texttt{V\_hat}: Cross-fitted residuals.
\end{itemize}

\paragraph{Data loading.}

\begin{verbatim}
load_lalonde(
    sample='experimental',  # or 'observational'
    return_dataframe=False, # True returns DataFrame
    verbose=False
)
\end{verbatim}

Returns \texttt{(Y, D, X)} arrays for the LaLonde (1986) dataset.

\paragraph{Diagnostic functions.}

\begin{verbatim}
# Compute kappa from treatment residuals
compute_kappa(U_hat, n=None)

# Contextual interpretation 
kappa_interpretation(kappa, n, r_squared_d=None)

# Overlap diagnostics via propensity scores
overlap_check(D, X, method='logistic')
\end{verbatim}

\subsection{Comparing Experimental and Observational Samples}

The following example illustrates how $\kappa_{\mathrm{DML}}$ captures the difference in conditioning between the experimental and observational LaLonde samples:

\begin{verbatim}
from dml_diagnostic import DMLDiagnostic, load_lalonde

# Experimental sample (good overlap)
Y_exp, D_exp, X_exp = load_lalonde('experimental')
result_exp = DMLDiagnostic(learner='lasso').fit(Y_exp, D_exp, X_exp)

# Observational sample (poor overlap)
Y_obs, D_obs, X_obs = load_lalonde('observational')
result_obs = DMLDiagnostic(learner='lasso').fit(Y_obs, D_obs, X_obs)

print(f"Experimental: theta = {result_exp.theta:.0f}, kappa = {result_exp.kappa:.2f}")
print(f"Observational: theta = {result_obs.theta:.0f}, kappa = {result_obs.kappa:.2f}")
\end{verbatim}

Output:
\begin{verbatim}
Experimental: theta = 1793, kappa = 4.10
Observational: theta = 56, kappa = 15.71
\end{verbatim}

The higher $\kappa_{\mathrm{DML}}$ in the observational sample reflects the poor overlap between NSW treated units and PSID controls: treatment is more predictable from covariates, leaving less residual variation for identification. The experimental benchmark estimate of approximately \$1,800 is recovered when conditioning is good, but the observational estimate is unreliable due to the flat score.


\newpage

% ==========================================================================
% References for online supplement
% ==========================================================================

\begin{thebibliography}{}

\bibitem[\protect\citeauthoryear{Bach et al.}{Bach et~al.}{2022}]{bach2022doubleml}
Bach, P., V. Chernozhukov, M.~S. Kurz and M. Spindler (2022).
\newblock DoubleML -- An object-oriented implementation of double machine learning in Python.
\newblock {\em Journal of Machine Learning Research\/} {\em 23}(53), 1--6.

\bibitem[\protect\citeauthoryear{Hahn}{Hahn}{1998}]{hahn1998}
Hahn, J. (1998).
\newblock On the role of the propensity score in efficient semiparametric estimation of average treatment effects.
\newblock {\em Econometrica\/} {\em 66}(2), 315--331.

\bibitem[\protect\citeauthoryear{Hirano, Imbens, and Ridder}{Hirano et~al.}{2003}]{hiranoimbensridder2003}
Hirano, K., G.~W. Imbens and G. Ridder (2003).
\newblock Efficient estimation of average treatment effects using the estimated propensity score.
\newblock {\em Econometrica\/} {\em 71}(4), 1161--1189.

\bibitem[\protect\citeauthoryear{LaLonde}{LaLonde}{1986}]{lalonde1986}
LaLonde, R.~J. (1986).
\newblock Evaluating the econometric evaluations of training programs with experimental data.
\newblock {\em American Economic Review\/} {\em 76}(4), 604--620.

\end{thebibliography}


\end{document}
