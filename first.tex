\documentclass[10pt]{article}

% --------------------------------------------------------------------------
% Packages
% --------------------------------------------------------------------------
\usepackage[margin=1in]{geometry}
\usepackage{amsmath,amssymb,amsthm,bm,mathtools}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{url} % for \url command
%\usepackage[dvipsnames]{xcolor} % for SkyBlue citation color
\usepackage[round,authoryear]{natbib}
%\usepackage[colorlinks=true,
%            linkcolor=RoyalBlue,
%            citecolor=RoyalBlue,
%            urlcolor=RoyalBlue]{hyperref}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue, urlcolor=blue]{hyperref}

\usepackage{enumitem}
\usepackage{bbm}
\usepackage{setspace}

% --------------------------------------------------------------------------
% Spacing
% --------------------------------------------------------------------------
\onehalfspacing
\setlength{\parskip}{0.5em}
\setlength{\parindent}{0pt}

% --------------------------------------------------------------------------
% Theorem Environments
% --------------------------------------------------------------------------
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}{Assumption}

\theoremstyle{remark}
\newtheorem{remark}{Remark}


% --------------------------------------------------------------------------
% Custom Commands
% --------------------------------------------------------------------------
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\ind}{\mathbbm{1}}
\newcommand{\norm}[1]{\lVert #1\rVert}
\newcommand{\abs}[1]{\lvert #1\rvert}

% Probability and convergence
\newcommand{\dto}{\xrightarrow{d}}
\newcommand{\pto}{\xrightarrow{p}}
\newcommand{\op}{o_P}
\newcommand{\Op}{O_P}

% --------------------------------------------------------------------------
% Title
% --------------------------------------------------------------------------
\title{Finite-Sample Failures and Condition-Number Diagnostics\\
in Double Machine Learning}
\author{%
  Gabriel Saco\thanks{Universidad del PacÃ­fico, Lima, Peru. 
  Email: \texttt{ga.sacoa@up.edu.pe}. 
  ORCID: \href{https://orcid.org/0009-0009-8751-4154}{0009-0009-8751-4154}.}
}
\date{}

\begin{document}

\maketitle

\begin{abstract}
Standard Double Machine Learning (DML; \citealp{chernozhukov2018dml}) confidence intervals can exhibit substantial finite-sample coverage distortions when the underlying score equations are ill-conditioned, even if nuisance functions are estimated with state-of-the-art methods. Focusing on the partially linear regression (PLR) model, we show that a simple, easily computed condition number for the orthogonal score ($\kappa_{\mathrm{DML}} := 1/|\hat{J}_\theta|$) largely determines when DML inference is reliable. Our first result derives a nonasymptotic, Berry--Esseen-type bound showing that the coverage error of the usual DML $t$-statistic is of order $n^{-1/2} + \sqrt{n}\,r_n$, where $r_n$ is the standard DML remainder term summarizing nuisance estimation error. Our second result provides a refined linearization in which both estimation error and confidence interval length scale as $\kappa_{\mathrm{DML}}/\sqrt{n} + \kappa_{\mathrm{DML}} r_n$, so that ill-conditioning directly inflates both variance and bias. These expansions yield three conditioning regimes---well-conditioned, moderately ill-conditioned, and severely ill-conditioned---and imply that informative, shrinking confidence sets require $\kappa_{\mathrm{DML}} = o_{\mathrm{p}}(\sqrt{n})$ and $\kappa_{\mathrm{DML}} r_n \to 0$. We conduct Monte Carlo experiments across overlap levels, nuisance learners (OLS, Lasso, random forests) and both low- and high-dimensional ($p>n$) designs. Across these designs, $\kappa_{\mathrm{DML}}$ is highly predictive of finite-sample performance: well-conditioned designs with $\kappa_{\mathrm{DML}}<1$ deliver near-nominal coverage with short intervals, whereas severely ill-conditioned designs can exhibit large bias and coverage around $40\%$ for nominal $95\%$ intervals, despite flexible nuisance fitting. We propose reporting $\kappa_{\mathrm{DML}}$ alongside DML estimates as a routine diagnostic of score conditioning, in direct analogy to condition-number checks and weak-instrument diagnostics in instrumental variables settings.
\end{abstract}

\noindent\textbf{Keywords:} Double Machine Learning, Nonasymptotic Inference, High-Dimensional Inference, Partially Linear Regression

\newpage

% ==========================================================================
\section{Introduction}
\label{sec:intro}
% ==========================================================================

Double Machine Learning (DML)---introduced by \citet{chernozhukov2018dml}---constructs estimators that combine high-dimensional or otherwise complex nuisance fits with valid asymptotic inference for low-dimensional target parameters. The method is built around Neyman-orthogonal scores---which make the moment conditions locally insensitive to first-order perturbations in the nuisance functions---and cross-fitting schemes that accommodate flexible machine-learning methods.\footnote{See \citet{chernozhukov2022locally} for a general treatment of locally robust / orthogonal moments in semiparametric GMM models, and \citet{neweyrobins2017cross} for cross-fit remainder-rate theory.} Under suitable regularity conditions, the resulting DML estimators are $\sqrt{n}$-consistent and asymptotically normal, extending classical partially linear regression (PLR) results \citep{robinson1988root} and post-selection inference in high-dimensional PLR settings \citep{belloni2014inference}. As a result, DML is now standard in applications with rich covariate information.

Beyond first-order asymptotics, recent work has derived nonasymptotic guarantees for debiased and orthogonal-score estimators, including finite-sample Gaussian and Berry--Esseen-type bounds in high-dimensional settings \citep{chernozhukov2023simple,quintas2022finite,jung2023shortnote}. Complementary simulation studies document substantial coverage distortions of nominal DML confidence intervals in empirically relevant designs \citep{naghi2021finite,zimmert2018causal}. In parallel, literature on ill-posed and weakly identified problems shows that near-singularity of moment matrices can severely constrain inference, even when estimators are asymptotically efficient.\footnote{See, for example, \citet{poetscher2002illposed} on lower risk bounds in ill-posed problems and \citet{stockwright2000weakgmm,stockyogo2005weakiv} on weak-identification asymptotics and diagnostics in GMM and IV models.} Taken together, these strands point to the conditioning of the DML orthogonal score as the analogue of first-stage diagnostics in instrumental variables regression. Just as first-stage $F$-statistics and minimum-eigenvalue measures govern the reliability of IV inference, the conditioning of the DML score should govern the finite-sample reliability of DML-based confidence intervals.

What is currently missing is an explicit analysis that links this conditioning to parameter-scale accuracy and the behavior of confidence sets. Existing finite-sample DML results are formulated in $t$-statistic scale and treat the score Jacobian as bounded away from zero. They control the approximation error of Wald tests. However, they do not quantify how a nearly singular Jacobian slows convergence in parameter scale, amplifies bias or prevents confidence intervals from shrinking at the nominal $n^{-1/2}$ rate. Likewise, simulation evidence highlights weak-identification-type pathologies---skewed sampling distributions, unstable standard errors, and undercoverage---without providing a simple diagnostic for when DML operates in such regimes.

This paper makes that conditioning channel explicit in the PLR setting. We introduce the condition number
\[
  \kappa_{\mathrm{DML}} := \frac{1}{\lvert \hat{J}_\theta\rvert},
\]
where $\hat{J}_\theta$ is the empirical derivative of the orthogonal score with respect to the target parameter. This shows via nonasymptotic coverage bounds and simulation evidence, that $\kappa_{\mathrm{DML}}$ plays a role analogous to first-stage $F$-statistics and minimum-eigenvalue diagnostics in instrumental variables regression \citep{staigerstock1997weakiv,stockyogo2005weakiv}. In particular, $\kappa_{\mathrm{DML}}$ partitions designs into well, moderately, and severely ill-conditioned regimes and serves as a practical diagnostic for when standard DML confidence intervals are informative, and when they are effectively uninformative due to ill-conditioning.

\subsection*{Contributions}

This paper makes three contributions.

\paragraph{1. Finite-sample linearization and coverage bound.}
We introduce the DML condition number $\kappa_{\mathrm{DML}}$ for the PLR orthogonal score and derive a refined Z-estimation expansion of the form
\[
\hat{\theta} - \theta_0
= \kappa_{\mathrm{DML}}\{S_n + B_n\} + R_n,
\]
where $S_n$ is the centered score average, $B_n$ collects nuisance-induced distortions, and $R_n$ is a higher-order remainder. Under standard cross-fitted DML conditions, this yields the parameter-scale rate
\[
\abs{\hat{\theta} - \theta_0}
= O_P\!\Bigl(\frac{\kappa_{\mathrm{DML}}}{\sqrt{n}} + \kappa_{\mathrm{DML}} r_n\Bigr),
\]
so that the condition number multiplies both variance and bias. In $t$-statistic scale, we obtain a nonasymptotic Berry--Esseen-type coverage error bound for the usual DML confidence interval,
\[
\bigl|\Prob(\theta_0 \in \mathrm{CI}_{\mathrm{std}}) - (1-\alpha)\bigr|
\lesssim n^{-1/2} + \sqrt{n}\,r_n + \delta,
\]
for a remainder sequence $r_n$ and concentration parameter $\delta$ summarizing nuisance and higher-order terms. Relative to existing finite-sample DML results \citep[]{chernozhukov2023simple,quintas2022finite,jung2023shortnote}, these bounds make the dependence on the score Jacobian explicit in parameter scale and connect finite-sample accuracy directly to $\kappa_{\mathrm{DML}}$.

\paragraph{2. Conditioning regimes and effective convergence.}
The linearization implies that informative DML inference requires control over the growth of $\kappa_n := \kappa_{\mathrm{DML}}$. In particular, confidence set diameters tend to zero only if $\kappa_n = o_P(\sqrt{n})$ and the bias term $\kappa_n r_n \to 0$. This leads to a classification into well-conditioned ($\kappa_n = O_P(1)$), moderately ill-conditioned ($\kappa_n = O_P(n^\beta)$ with $0<\beta<1/2$), and severely ill-conditioned ($\kappa_n \asymp c\sqrt{n}$) regimes, paralleling strong versus weak identification in IV and GMM \citep{staigerstock1997weakiv,stockwright2000weakgmm,stockyogo2005weakiv}. In the severely ill-conditioned regime, the effective convergence rate of $\hat{\theta}$ can deteriorate to $O_P(1)$ even when the DML $t$-statistic is approximately normal, so Wald-type procedures can look well behaved in $t$-statistic scale while performing poorly in parameter scale.

\paragraph{3. Simulation evidence in low and high dimensions.}
We conduct Monte Carlo experiments in both low-dimensional PLR designs and high-dimensional designs, varying overlap, sample size, and nuisance learners (OLS, Lasso, random forests). Across designs, $\kappa_{\mathrm{DML}}$ is closely aligned with overlap strength: high-overlap configurations yield $\kappa_{\mathrm{DML}}<1$ with near-nominal coverage and relatively short intervals, whereas low-overlap configurations generate large condition numbers and either very wide intervals (for linear and Lasso nuisance fits) or pronounced undercoverage with non-negligible bias (for flexible random forests). In complementary high-dimensional designs, increases in $R^2(D\mid X)$ are accompanied by higher $\kappa_{\mathrm{DML}}$ and systematic declines in coverage. These results support the use of $\kappa_{\mathrm{DML}}$ as a diagnostic for DML conditioning and indicate that reporting $\kappa_{\mathrm{DML}}$ alongside DML estimates helps detect designs in which nominal coverage is unlikely to hold.

% ==========================================================================
\section{Setup: PLR Model and DML Estimator}
\label{sec:setup}
% ==========================================================================

We consider the canonical Partially Linear Regression (PLR) model. Observations $W_i = (Y_i, D_i, X_i)$, $i = 1, \ldots, n$, are drawn i.i.d.\ from a distribution $P$, where $Y_i \in \R$ is the outcome, $D_i \in \R$ is a scalar treatment or policy variable, and $X_i \in \R^p$ is a vector of controls or confounders. The structural model is
\begin{equation}
\label{eq:plr_model}
Y = D\theta_0 + g_0(X) + \varepsilon,
\qquad
\E[\varepsilon \mid D, X] = 0,
\end{equation}
where $\theta_0 \in \R$ is the scalar parameter of interest and $g_0: \R^p \to \R$ is an unknown nuisance function. This model goes back to the semiparametric PLR formulation of \citet{robinson1988root} and has been extensively studied in high-dimensional settings \citep[]{belloni2014inference}.

Define the nuisance regression functions
\[
m_0(X) := \E[D \mid X],
\qquad
\ell_0(X) := \E[Y \mid X].
\]
Using \eqref{eq:plr_model},
\[
\ell_0(X)
= \theta_0 m_0(X) + g_0(X),
\]
so that $\ell_0$ encodes both the structural and reduced-form components.

\paragraph{Orthogonal score.}
For PLR, the standard Neyman-orthogonal score is
\begin{equation}
\label{eq:score}
\psi(W; \theta, \eta)
:= (D - m(X))\bigl(Y - g(X) - \theta(D - m(X))\bigr),
\end{equation}
where $\eta = (g, m)$ collects nuisance functions. At the reference point we take
\[
\eta_0 := (g_0^\star, m_0),
\qquad
g_0^\star(X) := \ell_0(X) = \E[Y\mid X].
\]
With this choice, the score satisfies
\[
\Psi(\theta_0, \eta_0) := \E[\psi(W;\theta_0,\eta_0)] = 0,
\]
and Neyman orthogonality holds:
\[
\partial_\eta \Psi(\theta_0,\eta)\big|_{\eta=\eta_0} = 0,
\]
so that the moment condition is insensitive to first-order perturbations in $\eta$.

\paragraph{Cross-fitted DML estimator.}
The DML estimator uses $K$-fold cross-fitting. Let $\hat{m}$ and $\hat{g}$ denote generic cross-fitted estimators of $m_0$ and $g_0^\star$. For each $i$, they are trained on folds not containing observation $i$. Define residualized variables
\begin{equation}
\label{eq:residuals}
\hat{U}_i := D_i - \hat{m}(X_i),
\qquad
\hat{V}_i := Y_i - \hat{g}(X_i).
\end{equation}
The empirical score average is
\[
\Psi_n(\theta, \hat{\eta})
:= \frac{1}{n}\sum_{i=1}^n \psi(W_i; \theta, \hat{\eta})
= \frac{1}{n}\sum_{i=1}^n \hat{U}_i(\hat{V}_i - \theta \hat{U}_i),
\]
and the DML estimator $\hat{\theta}$ is defined by the empirical moment equation
\[
\Psi_n(\hat{\theta}, \hat{\eta}) = 0.
\]
Solving gives the familiar partialling-out formula
\begin{equation}
\label{eq:theta_hat}
\hat{\theta}
= \frac{\sum_{i=1}^n \hat{U}_i \hat{V}_i}{\sum_{i=1}^n \hat{U}_i^2},
\end{equation}
which coincides with post-double-selection PLR estimators in \citet{belloni2014inference} when $\hat{m}$ and $\hat{g}$ are obtained via sparse linear methods.

\paragraph{Jacobian and condition number.}
The empirical Jacobian is
\begin{equation}
\label{eq:J_hat}
\hat{J}_\theta
:= \partial_\theta \Psi_n(\theta, \hat{\eta})
= -\frac{1}{n}\sum_{i=1}^n \hat{U}_i^2,
\end{equation}
which does not depend on $\theta$ and is nonpositive. We define the DML condition number
\begin{equation}
\label{eq:kappa}
\kappa_{\mathrm{DML}}
:= -\frac{1}{\hat{J}_\theta}
= \frac{1}{|\hat{J}_\theta|}
= \frac{n}{\sum_{i=1}^n \hat{U}_i^2},
\end{equation}
which is finite whenever $\sum_{i=1}^n \hat{U}_i^2>0$. Small residual treatment variation (small $\sum \hat{U}_i^2$) implies a large $\kappa_{\mathrm{DML}}$, corresponding to a nearly flat score and a numerically unstable estimator, exactly mirroring the weak-instrument problem in IV \citep{staigerstock1997weakiv,stockyogo2005weakiv}.\footnote{See also \citet{belloni2012sparse} for Lasso-based IV and the role of strong first stages in high-dimensional optimal instrument construction.}

% ==========================================================================
\section{Linearization and Coverage Error Bound}
\label{sec:theory}
% ==========================================================================

We now establish a refined linearization of the DML estimator and derive a coverage error bound for the standard DML confidence interval. Our arguments follow the orthogonal-score expansion principles underlying debiased / desparsified estimators, but specialized to cross-fitted PLR DML.

\subsection{Linearization}

Define the empirical score average
\[
\Psi_n(\theta, \eta)
:= \frac{1}{n}\sum_{i=1}^n \psi(W_i; \theta, \eta).
\]
At $(\theta_0,\eta_0)$ and $(\theta_0,\hat{\eta})$ set
\begin{align}
S_n &:= \Psi_n(\theta_0, \eta_0)
     = \frac{1}{n}\sum_{i=1}^n \psi(W_i;\theta_0,\eta_0),
     \label{eq:Sn}\\[0.25em]
B_n &:= \Psi_n(\theta_0, \hat{\eta}) - \Psi_n(\theta_0, \eta_0),
     \label{eq:Bn}
\end{align}
and let $R_n$ denote a Taylor remainder.

\begin{assumption}[Regularity Conditions]
\label{ass:regularity}
The following conditions hold.
\begin{enumerate}[label=(\roman*)]
    \item \textbf{(Score regularity)} For some $\tilde{\theta}$ between $\hat{\theta}$ and $\theta_0$,
    \[
    \partial_\theta \Psi_n(\tilde{\theta}, \hat{\eta})
    = \hat{J}_\theta + \op(1),
    \]
    where $\hat{J}_\theta$ is defined in \eqref{eq:J_hat}. In the PLR score, this equality holds exactly.
    \item \textbf{(Invertibility)} There exists a deterministic sequence $c_{J,n}>0$ and a constant $\delta_J\in(0,1)$ such that
    \[
    \Prob\big(|\hat{J}_\theta|\ge c_{J,n}\big) \ge 1-\delta_J.
    \]
    We write $\kappa_n := 1/c_{J,n}$ for the implied upper envelope of $\kappa_{\mathrm{DML}}$.
    \item \textbf{(Nuisance rate)} The nuisance estimators satisfy
    \[
    \norm{\hat{m} - m_0}_{L^2}
    \cdot
    \norm{\hat{g} - g_0^\star}_{L^2}
    = \op(n^{-1/2}),
    \]
    as in standard DML conditions \citep{dukes2024doubly}.
    \item \textbf{(Moment bounds)} For the score at the truth,
    \[
    \E\big[\psi(W;\theta_0,\eta_0)^2\big] =: \sigma_\psi^2 \in (0,\infty),
    \qquad
    \E\big[\abs{\psi(W;\theta_0,\eta_0)}^3\big] \le M_3 < \infty.
    \]
\end{enumerate}
\end{assumption}

\begin{lemma}[Refined Linearization of the DML Estimator]
\label{lem:linearization}
Under Assumption~\ref{ass:regularity},
\begin{equation}
\label{eq:linearization}
\hat{\theta} - \theta_0
= \kappa_{\mathrm{DML}}\{S_n + B_n\} + R_n,
\end{equation}
where $\kappa_{\mathrm{DML}}$ is defined in \eqref{eq:kappa}, $S_n$ and $B_n$ are given in \eqref{eq:Sn}--\eqref{eq:Bn}, and $R_n$ satisfies $R_n = \op(n^{-1/2})$. Moreover,
\[
S_n = \Op(n^{-1/2}),
\qquad
B_n = \op(n^{-1/2}).
\]
\end{lemma}

\begin{proof}
Since $\hat{\theta}$ solves $\Psi_n(\hat{\theta}, \hat{\eta}) = 0$, a first-order Taylor expansion around $\theta_0$ yields
\[
\Psi_n(\hat{\theta}, \hat{\eta})
= \Psi_n(\theta_0, \hat{\eta})
 + \partial_\theta \Psi_n(\tilde{\theta}, \hat{\eta})(\hat{\theta}-\theta_0),
\]
for some $\tilde{\theta}$ between $\hat{\theta}$ and $\theta_0$. Rearranging,
\begin{equation}
\label{eq:lin_proof_1}
\hat{J}_\theta(\hat{\theta}-\theta_0)
= -\Psi_n(\theta_0,\hat{\eta})
   - r_{n,\theta}(\hat{\theta}-\theta_0),
\end{equation}
where $r_{n,\theta} = \partial_\theta \Psi_n(\tilde{\theta}, \hat{\eta}) - \hat{J}_\theta = \op(1)$ by Assumption~\ref{ass:regularity}(i). On the event $\{|\hat{J}_\theta|\ge c_{J,n}\}$ we can divide by $\hat{J}_\theta$ and obtain
\[
\hat{\theta}-\theta_0
= -\hat{J}_\theta^{-1}\Psi_n(\theta_0,\hat{\eta})
   - \hat{J}_\theta^{-1} r_{n,\theta}(\hat{\theta}-\theta_0).
\]
Define
\[
R_n := -\hat{J}_\theta^{-1} r_{n,\theta}(\hat{\theta}-\theta_0),
\]
and decompose
\[
\Psi_n(\theta_0,\hat{\eta})
= \Psi_n(\theta_0,\eta_0)
 + \big(\Psi_n(\theta_0,\hat{\eta}) - \Psi_n(\theta_0,\eta_0)\big)
= S_n + B_n.
\]
Using $\kappa_{\mathrm{DML}} = -\hat{J}_\theta^{-1}$, we obtain \eqref{eq:linearization}. The orders $S_n=\Op(n^{-1/2})$ and $B_n=\op(n^{-1/2})$ follow from the central limit theorem for the orthogonal score and the product-rate condition in Assumption~\ref{ass:regularity}(iii).

Under standard DML conditions, $\hat{\theta}-\theta_0=\Op(n^{-1/2})$, so $R_n = \Op\big((\hat{\theta}-\theta_0)^2\big) = \Op(n^{-1}) = \op(n^{-1/2})$.
\end{proof}

\begin{remark}[Interpretation]
\label{rem:linearization_interp}
The decomposition \eqref{eq:linearization} separates three sources of error:
\begin{itemize}[leftmargin=*]
\item $\kappa_{\mathrm{DML}} S_n$ is the sampling fluctuation component, of order $\Op(\kappa_{\mathrm{DML}}/\sqrt{n})$.
\item $\kappa_{\mathrm{DML}} B_n$ is the nuisance-induced component; orthogonality ensures $B_n=\op(n^{-1/2})$, but large $\kappa_{\mathrm{DML}}$ can magnify it.
\item $R_n$ is a higher-order remainder, negligible at $n^{-1/2}$ scale.
\end{itemize}
When $\kappa_{\mathrm{DML}} = \Op(1)$, both variance and bias are $\Op(n^{-1/2})$. When $\kappa_{\mathrm{DML}}$ grows, the effective convergence rate deteriorates and the confidence interval length grows proportionally to $\kappa_{\mathrm{DML}}$, as in desparsified high-dimensional procedures \citep{vandegeer2014optimal,javanmard2014confidence}.
\end{remark}

\subsection{Coverage Error Bound}

We now establish a coverage error bound for the standard DML confidence interval, in the spirit of the finite-sample Gaussian approximation results of \citet{chernozhukov2023simple} and the high-dimensional bounds in \citet{quintas2022finite}.

Let
\begin{equation}
\label{eq:CI_std}
\mathrm{CI}_{\mathrm{std}}
:= \Bigl[\hat{\theta} \pm z_{1-\alpha/2}\,\widehat{\mathrm{SE}}_{\mathrm{DML}}\Bigr],
\end{equation}
where $z_{1-\alpha/2}$ is the $(1-\alpha/2)$ quantile of $N(0,1)$ and
\begin{equation}
\label{eq:se_dml_correct}
\widehat{\mathrm{SE}}_{\mathrm{DML}}
:= \frac{\kappa_{\mathrm{DML}}}{\sqrt{n}}\,
   \sqrt{\frac{1}{n}\sum_{i=1}^n \hat{U}_i^2 \hat{\varepsilon}_i^2},
\end{equation}
with
\[
\hat{\varepsilon}_i
:= Y_i - \hat{g}(X_i) - \hat{\theta}(D_i - \hat{m}(X_i)).
\]
This is the usual plug-in estimator of the asymptotic standard deviation of $\hat{\theta}$. Let
\[
s_n := \frac{\kappa_{\mathrm{DML}}\sigma_\psi}{\sqrt{n}}
\]
denote the corresponding nonrandom target scale.

\begin{assumption}[Concentration Bounds]
\label{ass:concentration}
For some $\delta\in(0,1)$, there exist deterministic sequences $a_n(\delta)$ and $r_n(\delta)$ and a constant $c_\xi\in(0,1/2)$ such that, with probability at least $1-\delta$,
\begin{enumerate}[label=(\roman*)]
    \item \textbf{(Sampling fluctuation)} $\abs{S_n} \le a_n(\delta)$ with $a_n(\delta) = O(\sigma_\psi/\sqrt{n})$.
    \item \textbf{(Nuisance and remainder)} $\abs{B_n} + \abs{R_n} \le r_n(\delta)$ with $r_n(\delta) = O(n^{-1/2-\gamma})$ for some $\gamma>0$.
    \item \textbf{(SE consistency)} $\abs{\widehat{\mathrm{SE}}_{\mathrm{DML}} - s_n} \le c_\xi s_n$.
\end{enumerate}
\end{assumption}

On this event,
\[
(1-c_\xi)s_n
\le \widehat{\mathrm{SE}}_{\mathrm{DML}}
\le (1+c_\xi)s_n,
\]
so $\widehat{\mathrm{SE}}_{\mathrm{DML}}$ is bounded away from zero and of order $s_n$.

Define the $t$-statistic
\[
T_n := \frac{\hat{\theta}-\theta_0}{\widehat{\mathrm{SE}}_{\mathrm{DML}}}.
\]
Then
\[
\theta_0 \in \mathrm{CI}_{\mathrm{std}}
\quad\Longleftrightarrow\quad
\abs{T_n} \le z_{1-\alpha/2}.
\]

\begin{theorem}[Coverage Error Bound]
\label{thm:coverage_error}
Under Assumptions~\ref{ass:regularity} and~\ref{ass:concentration}, there exist constants $C_1, C_2, C_3, C_4 > 0$ such that
\begin{equation}
\label{eq:coverage_bound}
\left| \Prob\bigl(\theta_0 \in \mathrm{CI}_{\mathrm{std}}\bigr) - (1-\alpha) \right|
\le \frac{C_1}{\sqrt{n}}
 + C_2 \sqrt{n}\,r_n(\delta)
 + C_3 \delta
 + C_4 c_\xi,
\end{equation}
where $r_n(\delta)$ is as in Assumption~\ref{ass:concentration}(ii). If, in addition, $c_\xi = O(n^{-1/2})$, the last term can be absorbed into the first, yielding
\begin{equation}
\label{eq:coverage_bound_simplified}
\left| \Prob\bigl(\theta_0 \in \mathrm{CI}_{\mathrm{std}}\bigr) - (1-\alpha) \right|
\le \frac{\tilde{C}_1}{\sqrt{n}}
 + C_2 \sqrt{n}\,r_n(\delta)
 + C_3 \delta
\end{equation}
for some $\tilde{C}_1>0$.
\end{theorem}

\begin{proof}[Proof]
Write
\[
T_n
= \frac{\kappa_{\mathrm{DML}}(S_n + B_n) + R_n}{\widehat{\mathrm{SE}}_{\mathrm{DML}}}
= T_{n,0} + \Delta_n,
\]
where
\[
T_{n,0}
:= \frac{\kappa_{\mathrm{DML}}S_n}{\widehat{\mathrm{SE}}_{\mathrm{DML}}},
\qquad
\Delta_n
:= \frac{\kappa_{\mathrm{DML}}B_n + R_n}{\widehat{\mathrm{SE}}_{\mathrm{DML}}}.
\]
Define the ``ideal'' statistic
\[
\tilde{T}_{n,0}
:= \frac{\kappa_{\mathrm{DML}}S_n}{s_n}
= \frac{\sqrt{n}}{\sigma_\psi} S_n
= \frac{1}{\sqrt{n}\,\sigma_\psi}\sum_{i=1}^n \psi(W_i;\theta_0,\eta_0),
\]
which is a standardized average of mean-zero i.i.d.\ scores. By the classical Berry--Esseen theorem,
\begin{equation}
\label{eq:BE}
\sup_{z\in\R}
\Bigl|
\Prob\bigl(\tilde{T}_{n,0}\le z\bigr) - \Phi(z)
\Bigr|
\le \frac{C_1}{\sqrt{n}},
\end{equation}
for some $C_1>0$ depending only on $M_3$ and $\sigma_\psi$.

On the concentration event in Assumption~\ref{ass:concentration}, the difference between $T_{n,0}$ and $\tilde{T}_{n,0}$ is controlled by the SE consistency:
\[
\bigl|T_{n,0} - \tilde{T}_{n,0}\bigr|
= \kappa_{\mathrm{DML}}\abs{S_n}
\left|
\frac{1}{\widehat{\mathrm{SE}}_{\mathrm{DML}}}
- \frac{1}{s_n}
\right|
\le C'_2 c_\xi,
\]
for some $C'_2>0$, using $a_n(\delta) = O(\sigma_\psi/\sqrt{n})$ and $s_n = \kappa_{\mathrm{DML}}\sigma_\psi/\sqrt{n}$. Moreover, the nuisance and remainder terms satisfy
\[
\abs{\Delta_n}
\le \frac{\kappa_{\mathrm{DML}}\abs{B_n}+\abs{R_n}}{(1-c_\xi)s_n}
\le C''_2 \sqrt{n}\,r_n(\delta),
\]
for some constant $C''_2>0$, since $s_n \asymp \kappa_{\mathrm{DML}}/\sqrt{n}$. Thus, on the concentration event, $T_n$ differs from $\tilde{T}_{n,0}$ by at most $C_2\sqrt{n}\,r_n(\delta) + C_4 c_\xi$, and anti-concentration inequalities for the normal distribution imply that this shift perturbs $\Prob(\abs{T_n}\le z_{1-\alpha/2})$ by at most a constant multiple of the shift magnitude. Combining this with \eqref{eq:BE} and adding the probability of the complement event (at most $\delta$) yields \eqref{eq:coverage_bound}. The structure of the argument parallels the Gaussian approximation bounds in \citet{chernozhukov2023simple}, but is specialized to scalar PLR DML and made explicit in terms of $\kappa_{\mathrm{DML}}$.
\end{proof}

\begin{remark}[From $t$-scale to parameter scale]
\label{rem:t_to_theta}
The bound \eqref{eq:coverage_bound} is expressed in $t$-statistic scale and does not display $\kappa_{\mathrm{DML}}$ explicitly, because the leading term normalizes by $s_n \propto \kappa_{\mathrm{DML}}/\sqrt{n}$. Combining Theorem~\ref{thm:coverage_error} with Lemma~\ref{lem:linearization}, however, shows that
\[
\hat{\theta} - \theta_0
= O_P\!\Bigl(\frac{\kappa_{\mathrm{DML}}}{\sqrt{n}} + \kappa_{\mathrm{DML}} r_n(\delta)\Bigr),
\]
so poor conditioning directly inflates both variance and bias of the DML estimator and the length of $\mathrm{CI}_{\mathrm{std}}$. This is fully analogous to how weak instruments inflate IV variance and bias \citep{staigerstock1997weakiv,stockyogo2005weakiv}.
\end{remark}

% ==========================================================================
\section{Conditioning Regimes}
\label{sec:regimes}
% ==========================================================================

The combination of the linearization \eqref{eq:linearization} and Assumption~\ref{ass:concentration} induces a natural classification of DML inference into regimes based on the growth rate of $\kappa_n := \kappa_{\mathrm{DML}}$.

\begin{corollary}[Conditioning Regimes]
\label{cor:regimes}
Suppose Assumptions~\ref{ass:regularity}--\ref{ass:concentration} hold and $r_n(\delta) = O(n^{-1/2-\gamma})$ for some $\gamma>0$. Then:
\begin{itemize}[leftmargin=*]
\item[(i)] \textbf{Well-conditioned} ($\kappa_n = \Op(1)$). The estimator satisfies
\[
\hat{\theta} - \theta_0 = O_P(n^{-1/2}),
\]
and the confidence interval length is $O_P(n^{-1/2})$. Standard DML inference is reliable and informative.

\item[(ii)] \textbf{Moderately ill-conditioned} ($\kappa_n = \Op(n^\beta)$, $0 < \beta < 1/2$). The estimator satisfies
\[
\hat{\theta} - \theta_0 = O_P(n^{\beta - 1/2}),
\]
and the confidence interval length is $O_P(n^{\beta - 1/2})$. Consistency is preserved, but convergence is slower and finite-sample coverage can be poor, echoing the behavior observed in high-dimensional desparsified estimators.

\item[(iii)] \textbf{Severely ill-conditioned} ($\kappa_n \asymp c\sqrt{n}$). The estimator satisfies
\[
\hat{\theta} - \theta_0 = O_P(1),
\]
and the confidence interval length is $O_P(1)$: the intervals fail to shrink as $n$ grows, even if the $t$-statistic remains asymptotically normal.
\end{itemize}
\end{corollary}

\paragraph{Interpretation.}
The coverage bound in Theorem~\ref{thm:coverage_error} depends on $\sqrt{n}\,r_n(\delta)$, but not explicitly on $\kappa_n$, because the latter cancels in $t$-scale. For the confidence sets to be informative, however, their length must vanish, which forces $\kappa_n = \op(\sqrt{n})$ and $\kappa_n r_n(\delta) \to 0$. Large condition numbers thus undermine finite-sample precision and slow the rate at which DML intervals concentrate, just as weak instruments undermine IV inference \citep{staigerstock1997weakiv,stockyogo2005weakiv}. In this sense, our analysis is complementary to the nonasymptotic DML theorems of \citet{chernozhukov2023simple} and \citet{quintas2022finite}, which control coverage in $t$-scale but are agnostic about parameter-scale conditioning.

% ==========================================================================
\section{Simulation Evidence}
\label{sec:simulations}
% ==========================================================================

We now present Monte Carlo evidence on how $\kappa_{\mathrm{DML}}$ interacts with overlap, covariate correlation, and nuisance learners to determine the finite-sample behavior of standard DML inference.\footnote{Replication code for all simulations is available at \url{https://github.com/gsaco/dml-condition}.} The design keeps the PLR setup in Section~\ref{sec:setup} deliberately simple, so that the link between $\kappa_{\mathrm{DML}}$ and coverage can be seen as transparently as possible.

All simulations are conducted under the partially linear regression (PLR) model in \eqref{eq:plr_model} with scalar treatment $D$, scalar outcome $Y$, and $p=10$ covariates $X \in \mathbb{R}^{10}$. The true treatment effect is fixed at $\theta_0 = 1$ throughout.

\subsection{Data-Generating Process}

The data-generating process (DGP) follows the PLR structure:
\[
Y = D \theta_0 + g_0(X) + \varepsilon,
\qquad
\varepsilon \sim N(0,1), \quad \varepsilon \perp (D,X).
\]

\paragraph{Covariates.}
For each design, covariates $X \in \R^p$ are drawn from a multivariate Gaussian distribution with Toeplitz covariance:
\begin{equation}
\label{eq:dgp_covariates_sim}
X \sim N(0, \Sigma(\rho)),
\qquad
\Sigma_{jk} = \rho^{|j-k|}, \quad j,k = 1,\ldots,p,
\end{equation}
where we fix $\rho = 0.5$ throughout. This AR(1)-type structure is standard in PLR and DML simulations and induces a moderate, persistent correlation among regressors. In the baseline design we set $p=10$, while in subsequent high-dimensional designs we consider $p>n$ under the same covariance structure.


\paragraph{Treatment equation and overlap.}
The treatment is generated from a linear model
\begin{equation}
\label{eq:dgp_treatment_sim}
D = X^\top \beta_D + U,
\qquad
U \sim N(0, \sigma_U^2), \quad U \perp X,
\end{equation}
with a decaying coefficient pattern
\[
\beta_D
= (1, 0.8, 0.6, 0.4, 0.2, 0, \ldots, 0)^\top \in \mathbb{R}^{10}.
\]
This avoids having a single dominant regressor and roughly mimics designs used in high-dimensional PLR work.

We control the degree of overlap through the population $R^2$ of the treatment regression:
\[
R^2(D \mid X)
= \frac{\Var(X^\top \beta_D)}{\Var(D)}
= \frac{\beta_D^\top \Sigma(\rho) \beta_D}{\beta_D^\top \Sigma(\rho) \beta_D + \sigma_U^2}.
\]
For a target value $R^2_{\mathrm{target}} \in (0,1)$, solving for $\sigma_U^2$ yields
\begin{equation}
\label{eq:overlap_calibration_sim}
\sigma_U^2
= \beta_D^\top \Sigma(\rho)\beta_D
  \Biggl(\frac{1 - R^2_{\mathrm{target}}}{R^2_{\mathrm{target}}}\Biggr).
\end{equation}
We consider three overlap regimes:
\begin{itemize}[leftmargin=*]
\item \textbf{High overlap}: $R^2_{\mathrm{target}}(D \mid X) = 0.75$;
\item \textbf{Moderate overlap}: $R^2_{\mathrm{target}}(D \mid X) = 0.90$;
\item \textbf{Low overlap}: $R^2_{\mathrm{target}}(D \mid X) = 0.97$.
\end{itemize}
In the simulated samples, the realized $R^2(D \mid X)$ is very close to its target in each cell. 

\paragraph{Outcome equation.}
The outcome combines the treatment effect with a nonlinear nuisance:
\begin{equation}
\label{eq:dgp_outcome_sim}
Y = D \theta_0 + g_0(X) + \varepsilon,
\qquad
\theta_0 = 1,
\quad
g_0(X) = \gamma^\top \sin(X),
\end{equation}
where
\[
\gamma
= (1, 0.5, 0.25, 0.125, 0.0625, 0, \ldots, 0)^\top.
\]
The function $g_0$ is smooth and nonlinear, so that flexible learners have an advantage, but the structure remains simple enough to keep the simulation design interpretable.

\subsection{DML Implementation and Learners}

For each configuration, we implement the cross-fitted PLR DML estimator with $K=5$ folds, following Section~\ref{sec:setup}. Given a sample $\{(Y_i, D_i, X_i)\}_{i=1}^n$, the algorithm is:

\begin{enumerate}[label=(\roman*),leftmargin=*]
\item Split the data into $K=5$ folds $I_1, \ldots, I_5$.
\item For each fold $k$ and each nuisance learner, fit
\[
\hat{m}^{(-k)}(x) \approx \mathbb{E}[D \mid X = x],
\qquad
\hat{\ell}^{(-k)}(x) \approx \mathbb{E}[Y \mid X = x],
\]
using only observations $i \notin I_k$, and obtain cross-fitted predictions for $i \in I_k$.
\item Form residuals
\[
\hat{U}_i = D_i - \hat{m}^{(-k)}(X_i),
\qquad
\hat{V}_i = Y_i - \hat{\ell}^{(-k)}(X_i),
\]
and compute the DML estimator via the partialling-out formula
\[
\hat{\theta}
= \frac{\sum_{i=1}^n \hat{U}_i \hat{V}_i}{\sum_{i=1}^n \hat{U}_i^2}.
\]
\item Compute the empirical Jacobian $\hat{J}_\theta = -n^{-1}\sum_i \hat{U}_i^2$, the condition number
\[
\kappa_{\mathrm{DML}}
= \frac{1}{|\hat{J}_\theta|}
= \frac{n}{\sum_{i=1}^n \hat{U}_i^2},
\]
and the plug-in standard error $\widehat{\mathrm{SE}}_{\mathrm{DML}}$ as in \eqref{eq:se_dml_correct}.
\item Form a nominal 95\% confidence interval
\[
\mathrm{CI}_{\mathrm{std}}
= \bigl[\hat{\theta} \pm 1.96\, \widehat{\mathrm{SE}}_{\mathrm{DML}}\bigr],
\]
and record whether it covers $\theta_0 = 1$, together with interval length, bias $\hat{\theta} - \theta_0$, squared error $(\hat{\theta} - \theta_0)^2$, and the realized sample $R^2(D\mid X)$.
\end{enumerate}

We consider three widely used nuisance-learner specifications:

\begin{itemize}[leftmargin=*]
\item \textbf{Linear regression (LIN)}: Ordinary least squares for both $m_0$ and $\ell_0$;
\item \textbf{$\ell_1$-penalized regression (LAS)}: Lasso, with the penalty selected by cross-validation;
\item \textbf{Random forests (RF)}: Random forest regressors for both $m_0$ and $\ell_0$, with 200 trees and maximum depth 5 (a conservative configuration to reduce overfitting).
\end{itemize}

We simulate two sample sizes, $n \in \{500, 2000\}$, all three overlap levels $R^2_{\mathrm{target}} \in \{0.75, 0.90, 0.97\}$ and all three learners. For each $(n, R^2_{\mathrm{target}}, \text{learner})$ configuration we run $B=500$ Monte Carlo replications. The resulting dataset has $9000$ rows (500 replications $\times$ 2 sample sizes $\times$ 3 overlap levels $\times$ 3 learners) and 15 columns per row (including $\hat{\theta}$, $\kappa_{\mathrm{DML}}$, $\widehat{\mathrm{SE}}_{\mathrm{DML}}$, CI end points, coverage, bias, squared error, and sample $R^2$).

Across all designs, the empirical $\kappa_{\mathrm{DML}}$ ranges from about $0.459$ to $9.475$. Pooling over all cells and learners, the overall coverage rate for nominal 95\% intervals is $89.3\%$, but this masks substantial heterogeneity across designs.

\subsection{Design-Level Behavior of $\kappa_{\mathrm{DML}}$}

To summarize how overlap translates into conditioning, Table~\ref{tab:design_summary} reports median, mean, and standard deviation of $\kappa_{\mathrm{DML}}$ by overlap regime, pooling over $n$ and learners.

\begin{table}[!htbp]
\centering
\caption{Design Summary: Median $\kappa_{\mathrm{DML}}$ by Overlap Level}
\label{tab:design_summary}
\small
\begin{tabular}{llrrrrll}
\toprule
 & Overlap & $R^2(D\mid X)$ & Median $\kappa_{\mathrm{DML}}$ & Mean $\kappa_{\mathrm{DML}}$ & SD $\kappa_{\mathrm{DML}}$ & $n$ values & Learners \\
\midrule
0 & High (R$^2=0.75$)     & 0.750 & 0.710 & 0.680 & 0.080 & 500, 2000 & LIN, LAS, RF \\
1 & Moderate (R$^2=0.90$) & 0.900 & 2.110 & 1.840 & 0.500 & 500, 2000 & LIN, LAS, RF \\
2 & Low (R$^2=0.97$)      & 0.970 & 7.600 & 5.820 & 2.860 & 500, 2000 & LIN, LAS, RF \\
\bottomrule
\end{tabular}
\end{table}

High-overlap designs produce well-conditioned scores with $\kappa_{\mathrm{DML}}$ comfortably below 1. As the treatment becomes nearly deterministic in $X$ (low overlap), $\kappa_{\mathrm{DML}}$ increases sharply. The median is around $7.6$, and the distribution spreads out substantially (standard deviation around $2.9$). This pattern is exactly what one would expect from the definition $\kappa_{\mathrm{DML}} = n / \sum_i \hat{U}_i^2$. When residual treatment variation shrinks, $\sum_i \hat{U}_i^2$ becomes small and the score becomes nearly flat.

The table intentionally pools over learners, but the cell-level summary reveals systematic learner-specific differences. At low overlap, for example, linear and Lasso nuisances produce median $\kappa_{\mathrm{DML}}$ around $7.7$--$7.9$, while random forests keep $\kappa_{\mathrm{DML}}$ closer to $1.6$--$2.0$, reflecting better residualization of $D$ on $X$.

\subsection{Cell-Level Summary: $(n, R^2, \text{learner})$}

Table~\ref{tab:cell_summary} reports cell-level statistics for each combination of sample size, overlap level, and learner (for $\rho=0.5$). For each cell we report median and mean $\kappa_{\mathrm{DML}}$, coverage, average CI length, mean bias, and RMSE.

\begin{table}[!htbp]
\centering
\caption{Cell-Level Monte Carlo Summary by $n$, Overlap, and Learner ($\rho=0.5$, $B=500$)}
\label{tab:cell_summary}
\footnotesize
\begin{tabular}{@{}cccccccccc@{}}
\toprule
$n$ & $R^2_{\mathrm{target}}$ & Learner & Overlap & Median $\kappa_{\mathrm{DML}}$ & Mean $\kappa_{\mathrm{DML}}$ & Coverage & Avg CI Length & Mean Bias & RMSE \\
\midrule
\multicolumn{10}{@{}l}{\textit{High overlap: $R^2_{\mathrm{target}}(D\mid X) = 0.75$}} \\
500  & 0.75 & LAS & High     & 0.7245 & 0.7239 & 0.940 & 0.195 &  \phantom{-}0.001 & 0.0508 \\
500  & 0.75 & LIN & High     & 0.7242 & 0.7267 & 0.956 & 0.194 &  \phantom{-}0.001 & 0.0486 \\
500  & 0.75 & RF  & High     & 0.5452 & 0.5481 & 0.956 & 0.177 & -0.020           & 0.0454 \\
2000 & 0.75 & LAS & High     & 0.7325 & 0.7339 & 0.946 & 0.097 & -0.000           & 0.0241 \\
2000 & 0.75 & LIN & High     & 0.7333 & 0.7326 & 0.946 & 0.096 &  \phantom{-}0.002 & 0.0245 \\
2000 & 0.75 & RF  & High     & 0.5892 & 0.5898 & 0.824 & 0.084 & -0.025           & 0.0314 \\
\midrule
\multicolumn{10}{@{}l}{\textit{Moderate overlap: $R^2_{\mathrm{target}}(D\mid X) = 0.90$}} \\
500  & 0.90 & LAS & Moderate & 2.1483 & 2.1645 & 0.934 & 0.334 & -0.000           & 0.0894 \\
500  & 0.90 & LIN & Moderate & 2.1497 & 2.1575 & 0.950 & 0.335 &  \phantom{-}0.001 & 0.0848 \\
500  & 0.90 & RF  & Moderate & 1.0584 & 1.0595 & 0.962 & 0.279 & -0.043           & 0.0707 \\
2000 & 0.90 & LAS & Moderate & 2.1952 & 2.1975 & 0.962 & 0.167 &  \phantom{-}0.002 & 0.0421 \\
2000 & 0.90 & LIN & Moderate & 2.2022 & 2.2039 & 0.954 & 0.168 &  \phantom{-}0.000 & 0.0437 \\
2000 & 0.90 & RF  & Moderate & 1.2287 & 1.2293 & 0.598 & 0.135 & -0.061           & 0.0665 \\
\midrule
\multicolumn{10}{@{}l}{\textit{Low overlap: $R^2_{\mathrm{target}}(D\mid X) = 0.97$}} \\
500  & 0.97 & LAS & Low      & 7.7628 & 7.7802 & 0.932 & 0.636 &  \phantom{-}0.000 & 0.1708 \\
500  & 0.97 & LIN & Low      & 7.7144 & 7.7329 & 0.946 & 0.635 & -0.007           & 0.1651 \\
500  & 0.97 & RF  & Low      & 1.6075 & 1.6087 & 0.964 & 0.381 & -0.074           & 0.0986 \\
2000 & 0.97 & LAS & Low      & 7.8996 & 7.8961 & 0.950 & 0.317 & -0.002           & 0.0817 \\
2000 & 0.97 & LIN & Low      & 7.8944 & 7.9059 & 0.950 & 0.317 &  \phantom{-}0.004 & 0.0816 \\
2000 & 0.97 & RF  & Low      & 2.0148 & 2.0156 & 0.398 & 0.191 & -0.104           & 0.1092 \\
\bottomrule
\end{tabular}

\bigskip
\raggedright
\footnotesize
\textit{Notes:} Coverage is the proportion of nominal 95\% confidence intervals containing $\theta_0=1$. ``Avg CI Length'' is the empirical mean of $2\times 1.96\,\widehat{\mathrm{SE}}_{\mathrm{DML}}$. Mean bias and RMSE are computed from the empirical distribution of $\hat{\theta}$ over $B=500$ replications.
\end{table}

Several features of Table~\ref{tab:cell_summary} are worth highlighting:

\begin{itemize}[leftmargin=*]
\item In high-overlap designs ($R^2_{\mathrm{target}} = 0.75$), all learners produce small condition numbers, with median $\kappa_{\mathrm{DML}} \approx 0.73$ for LAS/LIN and $0.55$ for RF. For $n=500$, coverage hovers around 94--96\% with short intervals. For $n=2000$, Lasso and linear regression maintain coverage at 94.6\%, while random forests drop to 82.4\% coverage, driven by a negative bias around $-0.025$ that becomes non-negligible relative to the shrinking standard error.

\item In moderate-overlap designs ($R^2_{\mathrm{target}} = 0.90$), Lasso and linear regression exhibit condition numbers around $2.15$--$2.20$ but still maintain coverage between 93.4\% and 96.2\%. The price is interval length: at $n=2000$ the average CI length is about $0.167$, almost twice as long as in the high-overlap designs. Random forests, by contrast, reduce the median $\kappa_{\mathrm{DML}}$ to about $1.06$ (for $n=500$) and $1.23$ (for $n=2000$) but suffer a sharp coverage drop at $n=2000$ (to 59.8\%), with a substantial negative bias of $-0.061$.

\item In low-overlap designs ($R^2_{\mathrm{target}} = 0.97$), Lasso and linear regression produce very large condition numbers, around $7.7$--$7.9$ at both sample sizes. Coverage remains near nominal (93.2\%--95.0\%), but intervals are wide: length drops from about $0.63$ at $n=500$ to $0.32$ at $n=2000$, still much larger than in high-overlap designs. Random forests reduce $\kappa_{\mathrm{DML}}$ to about $1.61$ at $n=500$ and $2.01$ at $n=2000$, but coverage collapses to 39.8\% at $n=2000$, with a large negative bias of about $-0.10$.
\end{itemize}

Overall, Table~\ref{tab:cell_summary} shows that the same overlap level can produce very different $\kappa_{\mathrm{DML}}$ across learners, and that the same range of $\kappa_{\mathrm{DML}}$ can coexist with good coverage (LAS/LIN) or severe undercoverage (RF), depending on how quickly the nuisance error $r_n$ shrinks.

\subsection{Regime-Level Aggregation by $\kappa_{\mathrm{DML}}$}

To connect more directly with the theoretical regimes in Corollary~\ref{cor:regimes}, we classify each design cell by its median $\kappa_{\mathrm{DML}}$ as well-conditioned when $\kappa_{\mathrm{DML}} < 1$, moderately ill-conditioned when $1 \le \kappa_{\mathrm{DML}} < 2$, and severely ill-conditioned when $\kappa_{\mathrm{DML}} \ge 2$. Using this classification, we compute regime-level averages of coverage, CI length, bias, and RMSE for each learner, and report these summaries in Table~\ref{tab:coverage_by_regime}.

\begin{table}[!htbp]
\centering
\caption{Coverage and CI Length by $\kappa_{\mathrm{DML}}$ Regime and Learner}
\label{tab:coverage_by_regime}
\small
\begin{tabular}{lllrrrr}
\toprule
 & $\kappa$-Regime & Learner & Coverage (\%) & Avg CI Length & Bias & RMSE \\
\midrule
1 & $< 1$   & LAS & 94.3 & 0.146 &  \phantom{-}0.000 & 0.037 \\
2 & $< 1$   & LIN & 95.1 & 0.145 &  \phantom{-}0.001 & 0.036 \\
3 & $< 1$   & RF  & 89.0 & 0.131 & -0.023            & 0.038 \\
\addlinespace
0 & $[1,2)$ & RF  & 84.1 & 0.265 & -0.059            & 0.079 \\
\addlinespace
4 & $\ge 2$ & LAS & 94.4 & 0.364 & -0.000            & 0.096 \\
5 & $\ge 2$ & LIN & 95.0 & 0.363 & -0.000            & 0.094 \\
6 & $\ge 2$ & RF  & 39.8 & 0.191 & -0.103            & 0.109 \\
\bottomrule
\end{tabular}

\bigskip
\raggedright
\footnotesize
\textit{Notes:} Regimes are defined using median $\kappa_{\mathrm{DML}}$ in each design cell (by $n$, overlap level, and learner). Coverage is the empirical percentage of nominal 95\% CIs that contain $\theta_0 = 1$. ``Avg CI Length'' is the mean length of $\mathrm{CI}_{\mathrm{std}}$ across replications; ``Bias'' and ``RMSE'' are the Monte Carlo mean and root mean-squared error of $\hat{\theta}$.
\end{table}

Three main patterns emerge.

\paragraph{(i) Well-conditioned designs ($\kappa_{\mathrm{DML}} < 1$).}
When the score is well-conditioned, standard DML behaves essentially as asymptotic theory suggests. For Lasso and linear regression, coverage is close to nominal (94.3\% and 95.1\%, respectively) with short intervals (average length around $0.145$) and small RMSE (about $0.036$--$0.037$). Random forests also perform reasonably well in this regime, but coverage is somewhat lower (89.0\%) and the intervals are slightly shorter. This illustrates that $\kappa_{\mathrm{DML}} < 1$ is a very favorable regime, though it does not completely eliminate the risk of learner-induced bias.

\paragraph{(ii) Moderately ill-conditioned designs ($1 \le \kappa_{\mathrm{DML}} < 2$).}
The moderately ill-conditioned regime is populated by random-forest designs at higher sample sizes and tighter overlap. Here coverage drops to 84.1\% despite relatively long intervals (average length $0.265$). The main driver is a more substantial negative bias ($-0.059$ on average), consistent with the $\kappa_{\mathrm{DML}} r_n$ term in the linearization: $\kappa_{\mathrm{DML}}$ is only moderately large, but the residual nuisance error of the flexible learner is large enough that $\kappa_{\mathrm{DML}} r_n$ does not vanish quickly.

\paragraph{(iii) Severely ill-conditioned designs ($\kappa_{\mathrm{DML}} \ge 2$).}
In the severely ill-conditioned regime, Lasso and linear regression maintain coverage around 94--95\%, but only by producing very wide intervals. Average length is about $0.36$, more than twice as large as in well-conditioned designs. In other words, standard DML inference can still be ``correct'' in the sense of coverage, but the resulting confidence sets shrink slowly and remain imprecise even at $n=2000$.

Random forests behave differently. For RF designs with $\kappa_{\mathrm{DML}} \ge 2$, coverage collapses to 39.8\%, with only moderate interval lengths (average $0.191$) but sizeable negative bias ($-0.103$) and RMSE around $0.11$. Here, the combination of poor conditioning and non-negligible nuisance bias leads to a dramatic failure of standard DML intervals.

\subsection{Connecting Back to the Linearization}

Taken together, the simulation output supports the theoretical decomposition
\[
\hat{\theta} - \theta_0
= \kappa_{\mathrm{DML}} \{S_n + B_n\} + R_n,
\qquad
\hat{\theta} - \theta_0
= O_P\!\Bigl(\frac{\kappa_{\mathrm{DML}}}{\sqrt{n}} + \kappa_{\mathrm{DML}} r_n\Bigr),
\]
in three distinct ways:

\begin{itemize}[leftmargin=*]
\item When $\kappa_{\mathrm{DML}} < 1$ and $r_n$ is small (LAS/LIN in high-overlap designs), both variance and bias are of order $n^{-1/2}$, and standard intervals work well. This is exactly the regime where the asymptotic normal approximation is most trustworthy.

\item When $\kappa_{\mathrm{DML}}$ is moderate or large but nuisance bias remains small (LAS/LIN in moderate- and low-overlap designs), coverage can be maintained only by inflating the variance term: $\kappa_{\mathrm{DML}}/\sqrt{n}$ becomes large, and the confidence intervals become visibly wide. The parameter-scale error still shrinks, but at a slower rate.

\item When $\kappa_{\mathrm{DML}}$ is moderate or large \emph{and} $r_n$ is not negligible (RF in moderate- and low-overlap designs at $n=2000$), the $\kappa_{\mathrm{DML}} r_n$ term dominates, leading to substantial bias and severe undercoverage. In these cells, the $t$-statistic can look reasonably well-behaved, but the parameter-scale error does not vanish at the desired rate.
\end{itemize}

In this sense, $\kappa_{\mathrm{DML}}$ is best viewed as a fragility gauge for DML inference. Large values do not automatically imply coverage failure, but they indicate that inference will either be very noisy (wide intervals) or very sensitive to nuisance bias.

\subsection{High-Dimensional Study: $p > n$}
\label{sec:high_dim}

A natural question is whether $\kappa_{\mathrm{DML}}$ remains informative when the covariate dimension exceeds the sample size. To investigate, we run a complementary Monte Carlo experiment with $n=200$ observations and $p=500$ covariates (ratio $p/n = 2.5$), using Lasso as the nuisance learner throughout. This setting is relevant for many empirical applications where DML is deployed precisely because of high-dimensional confounders.

\paragraph{Design.}
The DGP extends the low-dimensional specification in Section~\ref{sec:simulations}. Covariates follow $X \sim N(0, \Sigma(\rho))$ with $\rho=0.5$ and $p=500$. The treatment coefficient vector $\beta_D$ has decaying entries $\beta_{D,j} = 0.7^{j-1}$, $j = 1,\ldots,p$, yielding a sparse-like structure where the first few covariates dominate the first stage. The outcome nuisance $g_0(X)$ remains moderately nonlinear. We calibrate $\sigma_U^2$ to achieve $R^2(D \mid X) \in \{0.75, 0.90, 0.97\}$ as before and run $B=500$ Monte Carlo replications per overlap level.

\paragraph{Results.}
Table~\ref{tab:high_dim} summarizes the high-dimensional simulation.

\begin{table}[!htbp]
\centering
\caption{High-Dimensional PLR Study ($n=200$, $p=500$, Lasso, $B=500$)}
\label{tab:high_dim}
\small
\begin{tabular}{lrrrrl}
\toprule
$R^2(D\mid X)$ & Median $\kappa_{\mathrm{DML}}$ & Coverage (\%) & Avg CI Length & RMSE & Regime \\
\midrule
0.75 (High overlap)     & 0.62 & 92.2 & 0.302 & 0.085 & $< 1$ \\
0.90 (Moderate overlap) & 1.78 & 89.8 & 0.519 & 0.157 & $[1,2)$ \\
0.97 (Low overlap)      & 6.27 & 87.4 & 0.974 & 0.325 & $\ge 2$ \\
\bottomrule
\end{tabular}

\bigskip
\raggedright
\footnotesize
\textit{Notes:} Coverage is the proportion of nominal 95\% CIs containing $\theta_0=1$. Regime classification follows Section~\ref{sec:simulations}.
\end{table}

Several patterns emerge:

\begin{itemize}[leftmargin=*]
\item \textbf{$\kappa_{\mathrm{DML}}$ scales with overlap as expected.} Even with $p=500 > n=200$, the condition number increases sharply as overlap declines. Median $\kappa_{\mathrm{DML}}$ rises from $0.62$ at $R^2=0.75$ to $6.27$ at $R^2=0.97$, roughly a tenfold increase. This confirms that the overlap-to-conditioning relationship persists in high dimensions.

\item \textbf{Coverage degrades monotonically with $\kappa_{\mathrm{DML}}$.} In the well-conditioned regime ($\kappa < 1$), coverage is about $92\%$, close to nominal. As $\kappa_{\mathrm{DML}}$ enters the moderately ill-conditioned regime, coverage drops to roughly $90\%$. In the severely ill-conditioned regime, it falls further to about $87\%$. While less dramatic than the random-forest failures in the low-dimensional study, this monotonic decline illustrates that $\kappa_{\mathrm{DML}}$ remains a reliable diagnostic even when $p > n$.

\item \textbf{CI length and RMSE scale with $\kappa_{\mathrm{DML}}$.} Average CI length more than triples from $0.30$ ($R^2=0.75$) to $0.97$ ($R^2=0.97$), and RMSE increases from $0.085$ to $0.325$. This is consistent with the rate $\kappa_{\mathrm{DML}}/\sqrt{n}$: larger condition numbers amplify both variance and estimation error.

\item \textbf{Lasso maintains reasonable coverage even when $\kappa_{\mathrm{DML}}$ is large.} Unlike random forests in the low-dimensional study, Lasso does not exhibit catastrophic undercoverage in the severely ill-conditioned regime. This aligns with the theoretical insight that coverage failure requires both large $\kappa_{\mathrm{DML}}$ and non-negligible $r_n$. Lasso's regularization keeps nuisance error controlled even in high dimensions.
\end{itemize}

Overall, the high-dimensional study reinforces the main message: $\kappa_{\mathrm{DML}}$ is a portable diagnostic that predicts inference quality across both low- and high-dimensional settings. When $p > n$, practitioners should be especially attentive to $\kappa_{\mathrm{DML}}$, since limited overlap becomes harder to detect through conventional diagnostics.


% ==========================================================================
\section{Discussion}
\label{sec:conclusion}
% ==========================================================================

This paper has investigated how the conditioning of the DML score, summarized by the scalar condition number $\kappa_{\mathrm{DML}}$, shapes the finite-sample reliability of standard Double Machine Learning inference. The combination of the analytical results in Sections~\ref{sec:theory}--\ref{sec:regimes} and the detailed simulations in Section~\ref{sec:simulations} yields a coherent picture.

\paragraph{1. Parameter-scale linearization and coverage.}
On the theoretical side, we derived a refined linearization showing that the parameter-scale error satisfies
\[
\hat{\theta} - \theta_0
= \kappa_{\mathrm{DML}} \{S_n + B_n\} + R_n,
\qquad
\hat{\theta} - \theta_0
= O_P\!\Bigl(\frac{\kappa_{\mathrm{DML}}}{\sqrt{n}} + \kappa_{\mathrm{DML}} r_n(\delta)\Bigr),
\]
with $S_n$ capturing sampling fluctuations, $B_n$ capturing nuisance-induced distortions, and $R_n$ a higher-order remainder. Combined with a Berry--Esseen-type bound for the DML $t$-statistic, this yields a coverage error bound of order
\[
\frac{C_1}{\sqrt{n}} + C_2 \sqrt{n}\,r_n(\delta) + C_3\delta,
\]
consistent with recent finite-sample DML theory \citep{chernozhukov2023simple,quintas2022finite,jung2023shortnote}, but now explicitly linked to the conditioning regime through the parameter-scale rate.

\paragraph{2. Conditioning regimes and identification strength.}
The linearization naturally induces a classification into well-conditioned, moderately ill-conditioned, and severely ill-conditioned regimes. In well-conditioned designs ($\kappa_{\mathrm{DML}}=O_P(1)$), standard DML inference behaves as expected: confidence sets shrink at the usual $n^{-1/2}$ rate, and coverage is close to nominal. As $\kappa_{\mathrm{DML}}$ grows, both variance and bias are amplified in $\theta$-space, and confidence sets shrink only if $\kappa_{\mathrm{DML}} = o_P(\sqrt{n})$ and $\kappa_{\mathrm{DML}} r_n(\delta)\to 0$. When $\kappa_{\mathrm{DML}}\asymp c\sqrt{n}$, the effective convergence rate can deteriorate to $O_P(1)$, paralleling strong vs.\ weak identification regimes in instrumental variables \citep{staigerstock1997weakiv,stockyogo2005weakiv}. The simulations make this classification concrete: high-overlap designs cluster in the well-conditioned regime; low-overlap designs populate the severely ill-conditioned regime.

\paragraph{3. Empirical behavior across overlap, learners, and dimensions.}
The Monte Carlo experiments confirm that $\kappa_{\mathrm{DML}}$ is a useful diagnostic across both low-dimensional ($p=10$) and high-dimensional ($p=500 > n=200$) settings:

\begin{itemize}[leftmargin=*]
\item In well-conditioned designs ($\kappa_{\mathrm{DML}}<1$), Lasso and linear regression achieve near-nominal coverage (94--95\%) with short intervals, while random forests perform reasonably but can exhibit mild undercoverage when residual bias is non-negligible.

\item In moderately ill-conditioned designs ($1 \le \kappa_{\mathrm{DML}} < 2$), the interaction between conditioning and nuisance complexity becomes visible: the same range of $\kappa_{\mathrm{DML}}$ can produce either good coverage (RF at smaller $n$) or severe undercoverage (RF at larger $n$), depending on the decay of $r_n$.

\item In severely ill-conditioned designs ($\kappa_{\mathrm{DML}}\ge 2$), Lasso and linear regression maintain coverage mainly by inflating standard errors and producing wide intervals, whereas random forests can suffer dramatic undercoverage (below 40\%) and sizeable bias. This is precisely the pattern predicted by the rate $\kappa_{\mathrm{DML}}/\sqrt{n} + \kappa_{\mathrm{DML}}r_n$: poor conditioning either forces large variance (inflated intervals) or amplifies residual bias into substantial coverage distortions.

\item In the high-dimensional setting ($p=500$, $n=200$), $\kappa_{\mathrm{DML}}$ continues to predict inference quality: coverage declines monotonically from 93\% when $\kappa \approx 0.6$ to 87\% when $\kappa \approx 6.2$. Lasso's regularization keeps nuisance error controlled, preventing catastrophic failures, but the condition number still signals fragility.
\end{itemize}

\subsection{Practical Recommendations}

The theoretical and empirical results suggest several practical guidelines for applied DML analyses:

\begin{itemize}[leftmargin=*]
\item \textbf{Routinely report $\kappa_{\mathrm{DML}}$.}
We recommend that practitioners compute and report $\kappa_{\mathrm{DML}}$ alongside DML point estimates and standard errors, much as first-stage $F$-statistics are routinely reported in IV applications \citep{staigerstock1997weakiv,stockyogo2005weakiv}. Designs with $\kappa_{\mathrm{DML}} < 1$ are generally trustworthy; designs with $\kappa_{\mathrm{DML}} \ge 2$ warrant particular scrutiny.

\item \textbf{Interpret $\kappa_{\mathrm{DML}}$ jointly with learner complexity.}
The simulations show that $\kappa_{\mathrm{DML}}$ is not sufficient on its own: the same condition number can correspond to different coverage behavior under different nuisance learners. In practice, $\kappa_{\mathrm{DML}}$ should be interpreted jointly with diagnostics on nuisance fit and learner complexity.

\item \textbf{Use $\kappa_{\mathrm{DML}}$ to guide robustness checks.}
When $\kappa_{\mathrm{DML}}$ lies in the moderately or severely ill-conditioned regimes, practitioners should perform systematic robustness checks: alternative learners, alternative specifications that improve overlap, or alternative identification strategies (e.g., IV, panel designs). In such settings, standard DML intervals should be viewed as diagnostic summaries rather than definitive measures of uncertainty.
\end{itemize}

\subsection{Limitations and Directions for Future Work}

Our analysis focuses on the scalar PLR model with cross-fitted nuisance estimators and standard plug-in standard errors. Extending $\kappa$-based diagnostics to multi-parameter targets, IV-DML, panel and clustered designs. Moreover, while we treat $\kappa_{\mathrm{DML}}$ as a diagnostic, it is natural to ask whether one can construct conditioning-aware inference procedures that explicitly account for $\kappa_{\mathrm{DML}}$ in interval construction, for example via $\kappa$-inflated standard errors or bias-aware adjustments \citep{armstrong2020bias}. Finally, understanding how different classes of machine learners (e.g., boosted trees, neural networks) interact with $\kappa_{\mathrm{DML}}$ and the remainder term $r_n$ remains an open question, particularly in highly nonparametric settings.

Asymptotic DML theory remains valid, but its practical reliability hinges on the conditioning summarized by $\kappa_{\mathrm{DML}}$ and its interaction with nuisance estimation. Large condition numbers can degrade DML inference even in large samples, much as weak instruments degrade IV inference regardless of sample size. Making $\kappa_{\mathrm{DML}}$ a routine part of DML reporting practices is a simple step toward more transparent and reliable empirical work in high-dimensional and semiparametric settings.


\bibliographystyle{apalike}

\begin{thebibliography}{99}

\bibitem[Armstrong et al., 2020]{armstrong2020bias}
\newblock Armstrong, T. B., KolesÃ¡r, M., and Kwon, S. (2020). Bias-aware inference in regularized regression models. 
\newblock arXiv preprint arXiv:2012.14823.

\bibitem[Belloni et~al., 2012]{belloni2012sparse}
Belloni, A., Chen, D., Chernozhukov, V., and Hansen, C. (2012).
\newblock Sparse models and methods for optimal instruments with an application
  to eminent domain.
\newblock {\em Econometrica}, 80(6):2369--2429.

\bibitem[Belloni et~al., 2014]{belloni2014inference}
Belloni, A., Chernozhukov, V., and Hansen, C. (2014).
\newblock Inference on treatment effects after selection among
  high-dimensional controls.
\newblock {\em The Review of Economic Studies}, 81(2):608--650.

\bibitem[Chernozhukov et~al., 2018]{chernozhukov2018dml}
Chernozhukov, V., Chetverikov, D., Demirer, M., Duflo, E., Hansen, C.,
  Newey, W., and Robins, J. (2018).
\newblock Double/debiased machine learning for treatment and structural
  parameters.
\newblock {\em The Econometrics Journal}, 21(1):C1--C68.

\bibitem[Chernozhukov et~al., 2022]{chernozhukov2022locally}
Chernozhukov, V., Escanciano, J.~C., Ichimura, H., Newey, W.~K., and Robins,
  J.~M. (2022).
\newblock Locally robust semiparametric estimation.
\newblock {\em Econometrica}, 90(4):1501--1535.

\bibitem[Chernozhukov et~al., 2023]{chernozhukov2023simple}
Chernozhukov, V., Newey, W.~K., and Singh, R. (2023).
\newblock A simple and general debiased machine learning theorem with
  finite-sample guarantees.
\newblock {\em Biometrika}, 110(1):257--264.

\bibitem[Dukes et al., 2024]{dukes2024doubly}
Dukes, O., Vansteelandt, S., and Whitney, D.\ (2024).
On doubly robust inference for double machine learning in semiparametric regression.
\emph{Journal of Machine Learning Research}, 25(279), 1--46.

\bibitem[Javanmard and Montanari, 2014]{javanmard2014confidence}
Javanmard, A. and Montanari, A. (2014).
\newblock Confidence intervals and hypothesis testing for high-dimensional
  regression.
\newblock {\em Journal of Machine Learning Research}, 15(1):2869--2909.

\bibitem[Jung, 2023]{jung2023shortnote}
Jung, Y. (2023).
\newblock A short note on finite sample analysis on double/debiased machine
  learning.
\newblock Manuscript, Purdue University.

\bibitem[Naghi and Wirths, 2021]{naghi2021finite}
Naghi, A. A., and Wirths, C. P. (2021).
\newblock Finite sample evaluation of causal machine learning methods: Guidelines for the applied researcher (No. TI 2021-090/III).
\newblock {\em Tinbergen Institute Discussion Paper} 2021-090.

\bibitem[Newey and Robins, 2017]{neweyrobins2017cross}
Newey, W. K. and Robins, J. M. (2017). \newblock Cross-fitting and fast remainder rates for semiparametric estimation.
\newblock CeMMAP Working Paper CWP41/17.

\bibitem[P\"otscher, 2002]{poetscher2002illposed}
P\"otscher, B.~M. (2002).
\newblock Lower risk bounds and properties of confidence sets for ill-posed estimation problems.
\newblock {\em Econometrica}, 70(3):1035--1065.

\bibitem[Quintas-Martinez, 2022]{quintas2022finite}
Quintas-Martinez, V.~M. (2022).
\newblock Finite-sample guarantees for high-dimensional DML.
\newblock arXiv preprint arXiv:2206.07386.

\bibitem[Robinson, 1988]{robinson1988root}
Robinson, P.~M. (1988).
\newblock Root-{N}-consistent semiparametric regression.
\newblock {\em Econometrica}, 56(4):931--954.

\bibitem[Staiger and Stock, 1997]{staigerstock1997weakiv}
Staiger, D. and Stock, J.~H. (1997).
\newblock Instrumental variables regression with weak instruments.
\newblock {\em Econometrica}, 65(3):557--586.

\bibitem[Stock and Wright, 2000]{stockwright2000weakgmm}
Stock, J.~H. and Wright, J.~H. (2000).
\newblock GMM with weak identification.
\newblock {\em Econometrica}, 68(5):1055--1096.

\bibitem[Stock and Yogo, 2005]{stockyogo2005weakiv}
Stock, J.~H. and Yogo, M. (2005).
\newblock Testing for weak instruments in linear {IV} regression.
\newblock In Andrews, D.~W.~K. and Stock, J.~H., editors,
\newblock {\em Identification and Inference for Econometric Models: Essays in Honor of Thomas Rothenberg}, pages 80--108.
\newblock Cambridge University Press.

\bibitem[van~de Geer et~al., 2014]{vandegeer2014optimal}
van~de Geer, S., B{\"u}hlmann, P., Ritov, Y., and Dezeure, R. (2014).
\newblock On asymptotically optimal confidence regions and tests for
  high-dimensional models.
\newblock {\em The Annals of Statistics}, 42(3):1166--1202.

\bibitem[Zimmert, 2018]{zimmert2018causal}
Zimmert, M. (2018).
\newblock The finite sample performance of treatment effect estimators in high-dimensional settings.
\newblock {\em arXiv preprint arXiv:1805.05067}.


\end{thebibliography}



\newpage

% ==========================================================================
% Appendix: Detailed Derivations
% ==========================================================================
\appendix

\section{Additional Derivations for the PLR Model and Orthogonal Score}
\label{app:derivations}

\subsection{PLR Model, Residualization, and Population Moment}

We consider the partially linear regression (PLR) model. For $i=1,\dots,n$,
we observe
\[
W_i = (Y_i,D_i,X_i),
\]
with $Y_i \in \R$, $D_i \in \R$, and $X_i \in \R^p$. The structural model is
\begin{equation}
\label{app:eq:plr_model}
Y = \theta_0 D + g_0(X) + \varepsilon,
\qquad
\E[\varepsilon \mid D,X] = 0,
\end{equation}
where $\theta_0 \in \R$ is the parameter of interest and $g_0:\R^p\to\R$
is an unknown nuisance function.

Define the nuisance regression functions
\begin{align}
m_0(X) &:= \E[D \mid X], \label{app:eq:m0}\\
\ell_0(X) &:= \E[Y \mid X]. \label{app:eq:l0}
\end{align}
Using \eqref{app:eq:plr_model}, we express $\ell_0(X)$ explicitly. For each $x$,
\begin{align}
\ell_0(x)
&= \E[Y \mid X=x] \nonumber\\
&= \E\big[\theta_0 D + g_0(X) + \varepsilon \,\big|\, X=x\big] \nonumber\\
&= \theta_0 \E[D \mid X=x] + \E[g_0(X)\mid X=x] + \E[\varepsilon \mid X=x].
\label{app:eq:l0_expand_step1}
\end{align}
We have
\[
\E[D \mid X=x] = m_0(x),
\qquad
\E[g_0(X)\mid X=x] = g_0(x),
\]
and, using iterated expectations,
\begin{align}
\E[\varepsilon \mid X]
&= \E\big[\E[\varepsilon\mid D,X]\mid X\big] = \E[0\mid X] = 0.
\label{app:eq:eps_cond_X_zero}
\end{align}
Substituting into \eqref{app:eq:l0_expand_step1},
\begin{equation}
\label{app:eq:l0_expand}
\ell_0(x) = \theta_0 m_0(x) + g_0(x).
\end{equation}

\subsubsection*{Residualization}

Define the residualized variables
\begin{equation}
\label{app:eq:resid_def}
\tilde D := D - m_0(X),
\qquad
\tilde Y := Y - \ell_0(X).
\end{equation}
Substitute \eqref{app:eq:plr_model} and \eqref{app:eq:l0_expand} into $\tilde Y$:
\begin{align}
\tilde Y
&= Y - \ell_0(X) \nonumber\\
&= \big(\theta_0 D + g_0(X) + \varepsilon\big)
   - \big(\theta_0 m_0(X) + g_0(X)\big) \nonumber\\
&= \theta_0 \big(D - m_0(X)\big) + \varepsilon \nonumber\\
&= \theta_0 \tilde D + \varepsilon.
\label{app:eq:resid_regression}
\end{align}
Thus, after residualization,
\begin{equation}
\label{app:eq:resid_eq}
\tilde Y - \theta_0 \tilde D = \varepsilon.
\end{equation}

\subsubsection*{Population Moment Condition}

Multiply both sides of \eqref{app:eq:resid_eq} by $\tilde D$:
\begin{equation}
\label{app:eq:resid_eq_mult}
(\tilde Y - \theta_0 \tilde D)\tilde D = \varepsilon \tilde D.
\end{equation}
Take expectations:
\begin{equation}
\label{app:eq:moment_before_zero}
\E\big[(\tilde Y - \theta_0 \tilde D)\tilde D\big]
= \E[\varepsilon \tilde D].
\end{equation}
We now show $\E[\varepsilon \tilde D]=0$. Using iterated expectations,
\begin{align}
\E[\varepsilon \tilde D]
&= \E\big[\E[\varepsilon \tilde D \mid X]\big] \nonumber\\
&= \E\big[\E[\varepsilon (D - m_0(X)) \mid X]\big] \nonumber\\
&= \E\Big(
     \E[\varepsilon D \mid X]
     - m_0(X)\E[\varepsilon \mid X]
   \Big).
\label{app:eq:EepsDtilde_expand}
\end{align}
Consider each term separately.

First term:
\begin{align}
\E[\varepsilon D \mid X]
&= \E\big[D \varepsilon \mid X\big] \nonumber\\
&= \E\big[\E[D \varepsilon \mid D,X] \mid X\big] \nonumber\\
&= \E\big[D \E[\varepsilon \mid D,X] \mid X\big] \nonumber\\
&= \E[D \cdot 0 \mid X] \nonumber\\
&= 0.
\label{app:eq:EepsD_condX}
\end{align}
Second term: 
\begin{align}
m_0(X)\E[\varepsilon \mid X]
&= m_0(X)\cdot 0 = 0.
\label{app:eq:m0EepsX}
\end{align}
Substituting \eqref{app:eq:EepsD_condX} and \eqref{app:eq:m0EepsX} into
\eqref{app:eq:EepsDtilde_expand},
\begin{equation}
\E[\varepsilon \tilde D] = \E[0 - 0] = 0.
\label{app:eq:EepsDtilde_zero}
\end{equation}
Therefore, from \eqref{app:eq:moment_before_zero} and
\eqref{app:eq:EepsDtilde_zero},
\begin{equation}
\label{app:eq:population_moment}
\E\big[(\tilde Y - \theta_0 \tilde D)\tilde D\big] = 0.
\end{equation}

\begin{remark}
Equation \eqref{app:eq:population_moment} is the population normal equation
for the PLR parameter $\theta_0$ after residualizing both $Y$ and $D$ on $X$.
\end{remark}

\subsection{Orthogonal Score and Neyman Orthogonality}

\subsubsection*{Score Definition}

We introduce generic nuisance functions
\[
g:\R^p \to \R,
\qquad
m:\R^p \to \R,
\]
and set $\eta := (g,m)$. The PLR score is defined as
\begin{equation}
\label{app:eq:score}
\psi(W;\theta,\eta)
:= (D - m(X))\bigl(Y - g(X) - \theta(D - m(X))\bigr).
\end{equation}
Denote the associated population moment by
\begin{equation}
\label{app:eq:Psi_population}
\Psi(\theta,\eta)
:= \E\big[\psi(W;\theta,\eta)\big].
\end{equation}

At the truth, we set
\[
\eta_0 := (g_0^\star,m_0),
\qquad
g_0^\star(X) := \ell_0(X) = \E[Y\mid X],
\qquad
m_0(X) := \E[D\mid X].
\]
(Here $g_0^\star$ is the conditional mean of $Y$ given $X$; it is not the
structural $g_0$ from \eqref{app:eq:plr_model} but satisfies
$\ell_0(X) = \theta_0 m_0(X) + g_0(X)$.)

Substitute $(\theta_0,\eta_0)$ into \eqref{app:eq:score}:
\begin{align}
\psi(W;\theta_0,\eta_0)
&= (D - m_0(X))
   \Big(Y - g_0^\star(X) - \theta_0\big(D - m_0(X)\big)\Big).
\label{app:eq:score_truth_1}
\end{align}
Use $g_0^\star(X) = \ell_0(X)$ and \eqref{app:eq:resid_regression},
\[
Y - \ell_0(X)
= \theta_0(D - m_0(X)) + \varepsilon.
\]
Therefore,
\begin{align}
Y - g_0^\star(X) - \theta_0(D - m_0(X))
&= \big(Y - \ell_0(X)\big) - \theta_0(D - m_0(X)) \nonumber\\
&= \varepsilon.
\label{app:eq:inner_truth}
\end{align}
Substituting \eqref{app:eq:inner_truth} into \eqref{app:eq:score_truth_1},
\begin{equation}
\label{app:eq:score_truth_2}
\psi(W;\theta_0,\eta_0)
= (D - m_0(X))\varepsilon.
\end{equation}
Thus the population moment at $(\theta_0,\eta_0)$ is
\begin{equation}
\label{app:eq:Psi_truth}
\Psi(\theta_0,\eta_0)
= \E[(D - m_0(X))\varepsilon]
= 0,
\end{equation}
using exactly the same argument as in
\eqref{app:eq:EepsDtilde_zero}.

\subsubsection*{Neyman Orthogonality}

Let $\eta = (g,m)$ belong to a function space. For a direction $h=(h_g,h_m)$,
consider the path
\begin{equation}
\eta_t := \eta_0 + t h = \big(g_0^\star + t h_g,\; m_0 + t h_m\big),
\qquad t\in\R,
\end{equation}
and define
\begin{equation}
\Phi(t) := \Psi(\theta_0,\eta_t).
\end{equation}

\begin{definition}[Neyman orthogonality]
The score $\psi$ is Neyman-orthogonal at $(\theta_0,\eta_0)$ if
\[
\left.\frac{d}{dt}\Psi(\theta_0,\eta_t)\right|_{t=0} = 0
\quad\text{for all directions }h=(h_g,h_m).
\]
\end{definition}

We now compute the derivative explicitly.

For brevity, write
\begin{align}
U_t &:= D - m_t(X)
     = D - m_0(X) - t h_m(X), \label{app:eq:Ut_def}\\[0.4em]
V_t &:= Y - g_t(X) - \theta_0(D - m_t(X)). \label{app:eq:Vt_def}
\end{align}
Then
\begin{equation}
\label{app:eq:psi_UtVt}
\psi(W;\theta_0,\eta_t) = U_t V_t.
\end{equation}
We first expand $V_t$:
\begin{align}
V_t 
&= Y - \big(g_0^\star(X) + t h_g(X)\big)
   - \theta_0\Big(D - \big(m_0(X) + t h_m(X)\big)\Big) \nonumber\\
&= \big(Y - g_0^\star(X)\big) - t h_g(X)
   - \theta_0\big(D - m_0(X) - t h_m(X)\big) \nonumber\\
&= \big(Y - g_0^\star(X) - \theta_0(D - m_0(X))\big)
   - t h_g(X) + \theta_0 t h_m(X).
\label{app:eq:Vt_expand_1}
\end{align}
Using \eqref{app:eq:inner_truth}, the term in parentheses is $\varepsilon$, so
\begin{equation}
\label{app:eq:Vt_expand_2}
V_t = \varepsilon - t h_g(X) + \theta_0 t h_m(X).
\end{equation}
Differentiate \eqref{app:eq:Ut_def} and \eqref{app:eq:Vt_expand_2} with respect to $t$:
\begin{align}
\frac{d}{dt}U_t &= -h_m(X), \label{app:eq:dUt}\\
\frac{d}{dt}V_t &= -h_g(X) + \theta_0 h_m(X). \label{app:eq:dVt}
\end{align}
Using the product rule on \eqref{app:eq:psi_UtVt},
\begin{equation}
\frac{d}{dt}\psi(W;\theta_0,\eta_t)
= \frac{dU_t}{dt} V_t + U_t \frac{dV_t}{dt}.
\label{app:eq:dpsi_dt}
\end{equation}
Evaluate at $t=0$:
\begin{align}
\left.\frac{d}{dt}\psi(W;\theta_0,\eta_t)\right|_{t=0}
&= \left(-h_m(X)\right) V_0
   + U_0 \big(-h_g(X) + \theta_0 h_m(X)\big) \nonumber\\[0.4em]
&= -h_m(X)\varepsilon
   + (D - m_0(X))\big(-h_g(X) + \theta_0 h_m(X)\big),
\label{app:eq:dpsi_dt_at0}
\end{align}
since $U_0 = D - m_0(X)$ and $V_0=\varepsilon$ by \eqref{app:eq:Vt_expand_2}.

Taking expectations,
\begin{align}
\left.\frac{d}{dt}\Psi(\theta_0,\eta_t)\right|_{t=0}
&= -\E\big[h_m(X)\varepsilon\big]
   - \E\big[(D - m_0(X))h_g(X)\big]
   + \theta_0 \E\big[(D - m_0(X))h_m(X)\big].
\label{app:eq:dPsi_dt_components}
\end{align}
We now show that each expectation is zero.

\emph{First term:}
\begin{align}
\E[h_m(X)\varepsilon]
&= \E\big[\E[h_m(X)\varepsilon \mid X]\big] \nonumber\\
&= \E\big[h_m(X)\E[\varepsilon\mid X]\big] \nonumber\\
&= \E[h_m(X)\cdot 0] = 0,
\label{app:eq:EhmXe}
\end{align}
using \eqref{app:eq:eps_cond_X_zero}.

\emph{Second and third terms:}
for any measurable function $a(X)$,
\begin{align}
\E\big[(D - m_0(X)) a(X)\big]
&= \E\big[\E[(D - m_0(X))a(X)\mid X]\big] \nonumber\\
&= \E\big[a(X)\E[D - m_0(X)\mid X]\big] \nonumber\\
&= \E\big[a(X)(\E[D\mid X] - m_0(X))\big] \nonumber\\
&= \E\big[a(X)(m_0(X) - m_0(X))\big] \nonumber\\
&= 0.
\label{app:eq:EDminusm0a}
\end{align}
Hence both $\E[(D - m_0(X))h_g(X)]$ and
$\E[(D - m_0(X))h_m(X)]$ are zero.

Substituting \eqref{app:eq:EhmXe} and \eqref{app:eq:EDminusm0a} into
\eqref{app:eq:dPsi_dt_components},
\begin{equation}
\label{app:eq:dPsi_dt_zero}
\left.\frac{d}{dt}\Psi(\theta_0,\eta_t)\right|_{t=0} = 0.
\end{equation}

\begin{lemma}[Neyman orthogonality]
The score \eqref{app:eq:score} is Neyman-orthogonal at $(\theta_0,\eta_0)$,
i.e.\ \eqref{app:eq:dPsi_dt_zero} holds for all directions $h=(h_g,h_m)$.
\end{lemma}

\subsection{Empirical Score, DML Estimator, Jacobian, and Condition Number}

Let $\hat g$ and $\hat m$ be \emph{cross-fitted} estimators of
$g_0^\star$ and $m_0$ (trained on folds not containing observation $i$).
Define the empirical residuals
\begin{equation}
\label{app:eq:UhVh}
\hat U_i := D_i - \hat m(X_i),
\qquad
\hat V_i := Y_i - \hat g(X_i).
\end{equation}
The empirical score average is
\begin{equation}
\label{app:eq:Psi_n_def}
\Psi_n(\theta,\eta)
:= \frac{1}{n}\sum_{i=1}^n \psi(W_i;\theta,\eta).
\end{equation}
For $\hat\eta := (\hat g,\hat m)$, use \eqref{app:eq:score} and \eqref{app:eq:UhVh}:
\begin{align}
\psi(W_i;\theta,\hat\eta)
&= (D_i - \hat m(X_i))
   \bigl(Y_i - \hat g(X_i) - \theta(D_i - \hat m(X_i))\bigr) \nonumber\\
&= \hat U_i (\hat V_i - \theta \hat U_i).
\label{app:eq:psi_emp}
\end{align}
Therefore,
\begin{align}
\Psi_n(\theta,\hat\eta)
&= \frac{1}{n}\sum_{i=1}^n \hat U_i(\hat V_i - \theta \hat U_i) \nonumber\\
&= \frac{1}{n}\sum_{i=1}^n \hat U_i \hat V_i
   - \theta \frac{1}{n}\sum_{i=1}^n \hat U_i^2.
\label{app:eq:Psi_n_expand}
\end{align}

\subsubsection*{DML Estimator as a Z-Estimator}

The DML estimator $\hat\theta$ solves the empirical moment condition
\begin{equation}
\label{app:eq:Z_eq}
\Psi_n(\hat\theta,\hat\eta) = 0.
\end{equation}
Substitute \eqref{app:eq:Psi_n_expand} with $\theta = \hat\theta$:
\begin{align}
0
&= \frac{1}{n}\sum_{i=1}^n \hat U_i \hat V_i
   - \hat\theta \frac{1}{n}\sum_{i=1}^n \hat U_i^2. 
\label{app:eq:Z_eq_expand}
\end{align}
Multiply both sides of \eqref{app:eq:Z_eq_expand} by $n$:
\begin{equation}
0 = \sum_{i=1}^n \hat U_i \hat V_i
    - \hat\theta \sum_{i=1}^n \hat U_i^2.
\end{equation}
Rearrange to solve for $\hat\theta$:
\begin{equation}
\label{app:eq:theta_hat}
\hat\theta
= \frac{\sum_{i=1}^n \hat U_i \hat V_i}{\sum_{i=1}^n \hat U_i^2}.
\end{equation}

\subsubsection*{Empirical Jacobian}

Differentiate \eqref{app:eq:psi_emp} with respect to $\theta$:
\begin{align}
\partial_\theta \psi(W_i;\theta,\hat\eta)
&= \partial_\theta\Big(\hat U_i(\hat V_i - \theta \hat U_i)\Big) \nonumber\\
&= \hat U_i(-\hat U_i) \nonumber\\
&= -\hat U_i^2.
\label{app:eq:dpsi_dtheta}
\end{align}
Then
\begin{align}
\partial_\theta \Psi_n(\theta,\hat\eta)
&= \frac{1}{n}\sum_{i=1}^n \partial_\theta \psi(W_i;\theta,\hat\eta) 
\nonumber\\
&= \frac{1}{n}\sum_{i=1}^n (-\hat U_i^2) \nonumber\\
&= -\frac{1}{n}\sum_{i=1}^n \hat U_i^2.
\label{app:eq:Jacobian_calc}
\end{align}
This derivative does not depend on $\theta$, so we define the \emph{empirical
Jacobian}
\begin{equation}
\label{app:eq:J_hat}
\hat J_\theta := \partial_\theta \Psi_n(\theta,\hat\eta)
= -\frac{1}{n}\sum_{i=1}^n \hat U_i^2.
\end{equation}
Since $\hat U_i^2\ge 0$, we have $\hat J_\theta \le 0$.

\subsubsection*{DML Condition Number}

We define the DML condition number as
\begin{equation}
\label{app:eq:kappa_def}
\kappa_{\mathrm{DML}}
:= -\frac{1}{\hat J_\theta}
= \frac{1}{\abs{\hat J_\theta}}
= \frac{n}{\sum_{i=1}^n \hat U_i^2}.
\end{equation}
By construction, $\kappa_{\mathrm{DML}}>0$ whenever $\sum \hat U_i^2>0$.

\begin{remark}
Small $\sum \hat U_i^2$ (weak residual variation in $D$) implies large
$\kappa_{\mathrm{DML}}$, which corresponds to a nearly flat score and a
sensitive estimator.
\end{remark}

\subsection{Refined Linearization of the DML Estimator}

Define the empirical moment
\[
\Psi_n(\theta,\eta) := \frac{1}{n}\sum_{i=1}^n \psi(W_i;\theta,\eta).
\]
At the truth and the estimated nuisances, define
\begin{align}
S_n &:= \Psi_n(\theta_0,\eta_0)
     = \frac{1}{n}\sum_{i=1}^n \psi(W_i;\theta_0,\eta_0), \label{app:eq:Sn}\\[0.3em]
B_n &:= \Psi_n(\theta_0,\hat\eta) - \Psi_n(\theta_0,\eta_0), \label{app:eq:Bn}
\end{align}
and let $R_n$ denote the higher-order remainder to be specified.

\subsubsection*{Regularity Conditions}

\begin{assumption}[Regularity]
\label{app:ass:regularity}
\mbox{}
\begin{enumerate}[label=(\roman*)]
    \item \textbf{(Score regularity)} For $\tilde\theta$ between $\hat\theta$
    and $\theta_0$,
    \[
    \partial_\theta \Psi_n(\tilde\theta,\hat\eta)
    = \hat J_\theta + \op(1).
    \]
    In PLR with \eqref{app:eq:score}, this holds exactly with
    $\partial_\theta \Psi_n(\tilde\theta,\hat\eta)=\hat J_\theta$.
    \item \textbf{(Non-degeneracy)} There exist $c_J>0$ and $\delta_J\in(0,1)$
    such that
    \[
    \Prob\big(|\hat J_\theta|\ge c_J\big) \ge 1-\delta_J.
    \]
    \item \textbf{(Nuisance rate)} The nuisance estimators satisfy
    \[
    \norm{\hat m - m_0}_{L^2} \cdot \norm{\hat g - g_0^\star}_{L^2}
    = \op(n^{-1/2}).
    \]
    \item \textbf{(Moment bounds)} For the score at the truth,
    \[
    \E[\psi(W;\theta_0,\eta_0)^2] =: \sigma_\psi^2 < \infty,
    \qquad
    \E[\abs{\psi(W;\theta_0,\eta_0)}^3] \le M_3 < \infty.
    \]
\end{enumerate}
\end{assumption}

\subsubsection*{Linearization}

Since $\hat\theta$ solves $\Psi_n(\hat\theta,\hat\eta)=0$, perform a
first-order Taylor expansion around $\theta_0$:
\begin{equation}
\label{app:eq:Taylor_Psi}
\Psi_n(\hat\theta,\hat\eta)
= \Psi_n(\theta_0,\hat\eta)
 + \partial_\theta \Psi_n(\tilde\theta,\hat\eta) (\hat\theta - \theta_0),
\end{equation}
for some random $\tilde\theta$ between $\hat\theta$ and $\theta_0$. Using
Assumption~\ref{app:ass:regularity}(i), write
\begin{equation}
\partial_\theta \Psi_n(\tilde\theta,\hat\eta)
= \hat J_\theta + r_{n,\theta},
\qquad
r_{n,\theta} = \op(1).
\end{equation}
Thus \eqref{app:eq:Taylor_Psi} becomes
\begin{align}
0
&= \Psi_n(\theta_0,\hat\eta)
   + (\hat J_\theta + r_{n,\theta})(\hat\theta - \theta_0) \nonumber\\
&= \Psi_n(\theta_0,\hat\eta)
   + \hat J_\theta(\hat\theta - \theta_0)
   + r_{n,\theta}(\hat\theta - \theta_0).
\label{app:eq:Taylor_Psi_expand}
\end{align}
Rearrange \eqref{app:eq:Taylor_Psi_expand}:
\begin{equation}
\hat J_\theta(\hat\theta - \theta_0)
= -\Psi_n(\theta_0,\hat\eta) - r_{n,\theta}(\hat\theta - \theta_0).
\label{app:eq:Jhat_theta_diff}
\end{equation}
Assumption~\ref{app:ass:regularity}(ii) ensures that $\hat J_\theta$ is bounded
away from zero with high probability, so we can divide by $\hat J_\theta$:
\begin{align}
\hat\theta - \theta_0
&= -\hat J_\theta^{-1}\Psi_n(\theta_0,\hat\eta)
   - \hat J_\theta^{-1} r_{n,\theta}(\hat\theta - \theta_0).
\label{app:eq:theta_diff_raw}
\end{align}
Define the remainder
\begin{equation}
\label{app:eq:Rn_def}
R_n := - \hat J_\theta^{-1} r_{n,\theta}(\hat\theta - \theta_0).
\end{equation}
Then \eqref{app:eq:theta_diff_raw} is
\begin{equation}
\label{app:eq:theta_diff_SN_BN}
\hat\theta - \theta_0
= -\hat J_\theta^{-1}\Psi_n(\theta_0,\hat\eta) + R_n.
\end{equation}
Next, decompose the score at $\theta_0$:
\begin{align}
\Psi_n(\theta_0,\hat\eta)
&= \Psi_n(\theta_0,\eta_0)
   + \big(\Psi_n(\theta_0,\hat\eta) - \Psi_n(\theta_0,\eta_0)\big) \nonumber\\
&= S_n + B_n,
\label{app:eq:Psi_n_SN_BN}
\end{align}
where $S_n$ and $B_n$ were defined in \eqref{app:eq:Sn}--\eqref{app:eq:Bn}.
Substitute \eqref{app:eq:Psi_n_SN_BN} into \eqref{app:eq:theta_diff_SN_BN}:
\begin{equation}
\hat\theta - \theta_0
= -\hat J_\theta^{-1}(S_n + B_n) + R_n.
\label{app:eq:theta_diff_SN_BN2}
\end{equation}
Using the definition \eqref{app:eq:kappa_def},
\[
-\hat J_\theta^{-1} = \kappa_{\mathrm{DML}},
\]
so \eqref{app:eq:theta_diff_SN_BN2} becomes
\begin{equation}
\label{app:eq:linearization_final}
\hat\theta - \theta_0
= \kappa_{\mathrm{DML}}(S_n + B_n) + R_n.
\end{equation}

We now describe orders of magnitude. From \eqref{app:eq:score_truth_2},
\[
S_n = \frac{1}{n}\sum_{i=1}^n (D_i - m_0(X_i))\varepsilon_i,
\]
so by the central limit theorem and Assumption~\ref{app:ass:regularity}(iv),
\[
S_n = \Op(n^{-1/2}).
\]
By Neyman orthogonality and Assumption~\ref{app:ass:regularity}(iii), the nuisance
perturbation satisfies
\[
B_n = \op(n^{-1/2}).
\]
Under standard DML results, $\hat\theta - \theta_0 = \Op(n^{-1/2})$, so from
\eqref{app:eq:Rn_def},
\[
R_n = \Op\bigl((\hat\theta - \theta_0)^2\bigr) = \Op(n^{-1}) = \op(n^{-1/2}).
\]

\begin{lemma}[Linearization]
Under Assumption~\ref{app:ass:regularity},
\[
\hat\theta - \theta_0
= \kappa_{\mathrm{DML}}(S_n + B_n) + R_n,
\]
with $S_n = \Op(n^{-1/2})$, $B_n = \op(n^{-1/2})$, and $R_n = \op(n^{-1/2})$.
\end{lemma}

\subsection{Coverage Error Bound for the Standard DML Confidence Interval}

Define the estimated standard error
\begin{equation}
\label{app:eq:SE_DML_hat}
\widehat{\mathrm{SE}}_{\mathrm{DML}}
:= \frac{\kappa_{\mathrm{DML}}}{\sqrt{n}}\,
   \sqrt{\frac{1}{n}\sum_{i=1}^n \hat U_i^2 \hat\varepsilon_i^2},
\end{equation}
where
\begin{equation}
\label{app:eq:eps_hat_def}
\hat\varepsilon_i := Y_i - \hat g(X_i) - \hat\theta(D_i - \hat m(X_i)).
\end{equation}
The standard $(1-\alpha)$ confidence interval is
\begin{equation}
\label{app:eq:CI_std}
\mathrm{CI}_{\mathrm{std}}
:= \bigl[\hat\theta \pm z_{1-\alpha/2}\,\widehat{\mathrm{SE}}_{\mathrm{DML}}\bigr],
\end{equation}
where $z_{1-\alpha/2}$ is the $(1-\alpha/2)$ quantile of $N(0,1)$.
Define the t-statistic
\begin{equation}
\label{app:eq:Tn_def}
T_n := \frac{\hat\theta - \theta_0}{\widehat{\mathrm{SE}}_{\mathrm{DML}}}.
\end{equation}
Then
\[
\theta_0 \in \mathrm{CI}_{\mathrm{std}}
\quad\Longleftrightarrow\quad
\abs{T_n} \le z_{1-\alpha/2},
\]
so
\begin{equation}
\Prob(\theta_0 \in \mathrm{CI}_{\mathrm{std}})
= \Prob\big(\abs{T_n}\le z_{1-\alpha/2}\big).
\end{equation}

\subsubsection*{Concentration Assumptions}

\begin{assumption}[Concentration]
\label{app:ass:concentration}
For some $\delta\in(0,1)$, with probability at least $1-\delta$,
\begin{enumerate}[label=(\roman*)]
    \item \textbf{(Sampling fluctuation)} There exists $a_n(\delta)$ with
    $a_n(\delta) = O(\sigma_\psi/\sqrt{n})$ such that
    \[
    \abs{S_n} \le a_n(\delta).
    \]
    \item \textbf{(Nuisance and remainder)} There exists $r_n(\delta)$ with
    $r_n(\delta) = O(n^{-1/2-\gamma})$ for some $\gamma>0$ such that
    \[
    \abs{B_n} + \abs{R_n} \le r_n(\delta).
    \]
    \item \textbf{(SE consistency)} There exist $s_n>0$ and $c_\xi<1/2$ such
    that
    \[
    \abs{\widehat{\mathrm{SE}}_{\mathrm{DML}} - s_n} \le c_\xi s_n.
    \]
\end{enumerate}
\end{assumption}

On the event of Assumption~\ref{app:ass:concentration}(iii),
\[
(1-c_\xi) s_n \le \widehat{\mathrm{SE}}_{\mathrm{DML}}
\le (1+c_\xi)s_n,
\]
so $\widehat{\mathrm{SE}}_{\mathrm{DML}}$ is bounded away from zero and of
order $s_n$. As we will formalize below, a natural choice is
\[
s_n := \frac{\kappa_{\mathrm{DML}}\sigma_\psi}{\sqrt{n}},
\]
since $S_n$ has variance $\sigma_\psi^2/n$ and
$\hat\theta - \theta_0 \approx \kappa_{\mathrm{DML}} S_n$.

\subsubsection*{Coverage Error Bound}

We compare the distribution of $T_n$ with that of a standard normal $Z$.
The key point is that the \emph{parameter-scale} error
$\hat\theta - \theta_0$ is amplified by the condition number
$\kappa_{\mathrm{DML}}$, while the $t$-statistic is normalized to remove
this scaling. The coverage bound is therefore ``$\kappa$-free'' in
$t$-scale but implicitly \emph{driven} by $\kappa_{\mathrm{DML}}$ in
$\theta$-scale.

\emph{Step 1 (Ideal statistic and Berry--Esseen).}
Recall the decomposition
\[
T_n
= T_{n,0} + \Delta_n,
\qquad
T_{n,0}
:= \frac{\kappa_{\mathrm{DML}} S_n}{\widehat{\mathrm{SE}}_{\mathrm{DML}}},
\qquad
\Delta_n
:= \frac{\kappa_{\mathrm{DML}} B_n + R_n}{\widehat{\mathrm{SE}}_{\mathrm{DML}}}.
\]
At $(\theta_0,\eta_0)$ we have $\Psi(\theta_0,\eta_0)=0$ by
\eqref{app:eq:Psi_truth}, so $\E[\psi(W_i;\theta_0,\eta_0)] = 0$. Together with
Assumption~\ref{app:ass:regularity}(iv), the score
$\psi_i := \psi(W_i;\theta_0,\eta_0)$ satisfies
\[
\E[\psi_i] = 0,
\qquad
\Var(\psi_i) = \sigma_\psi^2 \in (0,\infty),
\qquad
\E\big[|\psi_i|^3\big] \le M_3 < \infty.
\]

Define the nonrandom target scale
\[
s_n := \frac{\kappa_{\mathrm{DML}}\sigma_\psi}{\sqrt{n}}.
\]
Then
\[
\tilde T_{n,0}
:= \frac{\kappa_{\mathrm{DML}} S_n}{s_n}
= \frac{\sqrt{n}}{\sigma_\psi}\, S_n
= \frac{1}{\sqrt{n}\,\sigma_\psi} \sum_{i=1}^n \psi_i
\]
is the usual standardized average of the score. In particular,
$s_n$ is precisely the \emph{asymptotic standard deviation of
$\hat\theta$}, so the typical $\theta$-scale fluctuation is of order
\[
\hat\theta - \theta_0
\approx \kappa_{\mathrm{DML}} S_n
= O_P\!\left(\frac{\kappa_{\mathrm{DML}}}{\sqrt{n}}\right).
\]
Thus $\kappa_{\mathrm{DML}}$ directly inflates the parameter-scale error
and the confidence interval length, even though we will normalize it away
in $t$-statistic scale.

The classical Berry--Esseen theorem applied to the sum
$\sum_{i=1}^n \psi_i$ yields
\begin{equation}
\label{app:eq:BE_bound}
\sup_{z\in\R}
\Big|
\Prob\big(\tilde T_{n,0}\le z\big) - \Phi(z)
\Big|
\le \frac{C_1}{\sqrt{n}},
\end{equation}
for some constant $C_1>0$ depending only on $M_3$ and $\sigma_\psi$.
Since the concentration event $\mathcal{G}$ in
Assumption~\ref{app:ass:concentration} has probability at least $1-\delta$,
the same bound holds conditional on $\mathcal{G}$ up to a change in
constants, and we continue to denote the constant by $C_1$:
\[
\sup_{z\in\R}
\Big|
\Prob\big(\tilde T_{n,0}\le z \mid \mathcal{G}\big) - \Phi(z)
\Big|
\le \frac{C_1}{\sqrt{n}}.
\]

\medskip

\emph{Step 2 (Replacing $\widehat{\mathrm{SE}}_{\mathrm{DML}}$ by $s_n$).}
On the event $\mathcal{G}$ where Assumption~\ref{app:ass:concentration}(i) and
(iii) hold, we have
\[
T_{n,0} - \tilde T_{n,0}
= \kappa_{\mathrm{DML}} S_n
\Big(
\frac{1}{\widehat{\mathrm{SE}}_{\mathrm{DML}}}
- \frac{1}{s_n}
\Big)
= \kappa_{\mathrm{DML}} S_n
\frac{s_n - \widehat{\mathrm{SE}}_{\mathrm{DML}}}
     {\widehat{\mathrm{SE}}_{\mathrm{DML}}\, s_n}.
\]
Using the concentration bounds on $S_n$ and on the standard error, we
obtain
\begin{align*}
\big|T_{n,0} - \tilde T_{n,0}\big|
&\le
\kappa_{\mathrm{DML}}\, |S_n|\,
\frac{\big|\widehat{\mathrm{SE}}_{\mathrm{DML}} - s_n\big|}
     {\widehat{\mathrm{SE}}_{\mathrm{DML}}\, s_n}
\\
&\le
\kappa_{\mathrm{DML}}\, a_n(\delta)\,
\frac{c_\xi s_n}{(1-c_\xi) s_n^2}
\quad\text{(by Assumption~\ref{app:ass:concentration}(i),(iii))}\\[0.3em]
&=
\frac{c_\xi}{1-c_\xi}\,
\frac{\kappa_{\mathrm{DML}} a_n(\delta)}{s_n}.
\end{align*}
By construction of $s_n$ and Assumption~\ref{app:ass:concentration}(i),
$a_n(\delta) = O(\sigma_\psi/\sqrt{n})$ and
$s_n = \kappa_{\mathrm{DML}}\sigma_\psi/\sqrt{n}$, so
\[
\frac{\kappa_{\mathrm{DML}} a_n(\delta)}{s_n}
= O(1),
\]
and therefore, on $\mathcal{G}$,
\begin{equation}
\label{app:eq:Tn0_vs_Tn0tilde_correct}
\big|T_{n,0} - \tilde T_{n,0}\big|
\le C_2' c_\xi,
\end{equation}
for some $C_2'>0$ independent of $n$.

\medskip

\emph{Step 3 (Bounding the bias and remainder term).}
On $\mathcal{G}$, Assumption~\ref{app:ass:concentration}(ii) gives
\[
\abs{B_n} + \abs{R_n} \le r_n(\delta),
\qquad
r_n(\delta) = O\bigl(n^{-1/2-\gamma}\bigr), \ \gamma>0.
\]
Using the lower bound on $\widehat{\mathrm{SE}}_{\mathrm{DML}}$ and the
definition of $s_n$,
\begin{align*}
\abs{\Delta_n}
&=
\left|
\frac{\kappa_{\mathrm{DML}} B_n + R_n}{\widehat{\mathrm{SE}}_{\mathrm{DML}}}
\right|
\le
\frac{\kappa_{\mathrm{DML}} |B_n| + |R_n|}
     {(1-c_\xi) s_n}
\\
&\le
\frac{(\kappa_{\mathrm{DML}}+1)\, r_n(\delta)}
     {(1-c_\xi) s_n}
=
\frac{(\kappa_{\mathrm{DML}}+1)\, r_n(\delta)\,\sqrt{n}}
     {(1-c_\xi)\,\kappa_{\mathrm{DML}}\sigma_\psi}.
\end{align*}
In the interesting regime where $\kappa_{\mathrm{DML}}\ge 1$, we have
$(\kappa_{\mathrm{DML}}+1)/\kappa_{\mathrm{DML}}\le 2$, and therefore
\begin{equation}
\label{app:eq:Deltan_bound}
\abs{\Delta_n}
\le C_2 \,\sqrt{n}\, r_n(\delta),
\end{equation}
for some $C_2>0$ depending only on $(1-c_\xi)$ and $\sigma_\psi$.

\medskip

\emph{Step 4 (From shifts in $T_n$ to coverage error).}
We have
\[
\theta_0 \in \mathrm{CI}_{\mathrm{std}}
\quad\Longleftrightarrow\quad
\abs{T_n} \le z_{1-\alpha/2},
\]
so
\[
\Prob(\theta_0 \in \mathrm{CI}_{\mathrm{std}})
= \Prob\big(\abs{T_n}\le z_{1-\alpha/2}\big).
\]
On the event $\mathcal{G}$,
\[
T_n = T_{n,0} + \Delta_n,
\qquad
T_{n,0} = \tilde T_{n,0} + (T_{n,0} - \tilde T_{n,0}).
\]
Anti-concentration inequalities for the normal distribution imply that
shifting a statistic by an amount $\delta$ changes
$\Prob(\abs{\cdot}\le z_{1-\alpha/2})$ by at most a constant multiple of
$|\delta|$. Combining this with \eqref{app:eq:BE_bound},
\eqref{app:eq:Tn0_vs_Tn0tilde_correct}, and \eqref{app:eq:Deltan_bound}, and
adding the probability of the complement event
$\mathcal{G}^c$ (which is at most $\delta$), yields a bound of the form
\begin{equation}
\label{app:eq:coverage_bound_final_corrected}
\Big|\Prob(\theta_0 \in \mathrm{CI}_{\mathrm{std}}) - (1-\alpha)\Big|
\le \frac{C_1}{\sqrt{n}}
 + C_2 \sqrt{n}\, r_n(\delta)
 + C_3 \delta
 + C_4 c_\xi,
\end{equation}
for suitable constants $C_1,C_2,C_3,C_4>0$.

If, in addition, the standard error estimator is $\sqrt{n}$-consistent so
that $c_\xi = O(n^{-1/2})$, the last term in
\eqref{app:eq:coverage_bound_final_corrected} is also $O(n^{-1/2})$ and can be
absorbed into the first term by adjusting $C_1$. In that case, the coverage
error bound simplifies to
\[
\Big|\Prob(\theta_0 \in \mathrm{CI}_{\mathrm{std}}) - (1-\alpha)\Big|
\le \frac{\tilde C_1}{\sqrt{n}}
 + C_2 \sqrt{n}\, r_n(\delta)
 + C_3 \delta,
\]
for a suitably modified constant $\tilde C_1>0$.

\begin{remark}[How $\kappa_{\mathrm{DML}}$ enters the picture]
\label{app:rem:kappa_role_coverage}
The bound \eqref{app:eq:coverage_bound_final_corrected} is expressed in
$t$-statistic scale, where the leading Berry--Esseen term is of order
$1/\sqrt{n}$ and does not display $\kappa_{\mathrm{DML}}$ explicitly
because we have normalized by the correct scale $s_n \propto
\kappa_{\mathrm{DML}}/\sqrt{n}$. However, combining this bound with the
linearization
\[
\hat\theta - \theta_0
= \kappa_{\mathrm{DML}}(S_n + B_n) + R_n,
\]
shows that the \emph{parameter-scale} error and the confidence interval
length are both of order $\kappa_{\mathrm{DML}}/\sqrt{n}$, while the bias
term is of order $\kappa_{\mathrm{DML}} r_n(\delta)$. Thus, even if the
$t$-statistic is well approximated by a normal distribution, a large
$\kappa_{\mathrm{DML}}$ magnifies nuisance error and leads to regimes where
$\kappa_{\mathrm{DML}} r_n(\delta)$ is non-negligible compared to
$\kappa_{\mathrm{DML}}/\sqrt{n}$, causing systematic coverage failures in
$\theta$-space. This is the sense in which finite-sample reliability of
DML is governed by the condition number.
\end{remark}



\end{document}
